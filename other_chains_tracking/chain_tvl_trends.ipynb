{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('start tvl trends')\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import requests as r\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "sys.path.append(\"../helper_functions\")\n",
    "import defillama_utils as dfl\n",
    "import duneapi_utils as du\n",
    "import opstack_metadata_utils as ops\n",
    "import csv_utils as cu\n",
    "import google_bq_utils as bqu\n",
    "sys.path.pop()\n",
    "\n",
    "current_utc_date = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "# print(current_utc_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trailing_mos = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_categorized_chains = dfl.get_chains_config()\n",
    "all_chains = dfl.get_chain_list()\n",
    "all_chains = all_chains.rename(columns={'name':'defillama_slug'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorized_chains.to_csv('outputs/dfl_categorized_chains.csv', index=False)\n",
    "categorized_chains = all_chains.merge(raw_categorized_chains,\n",
    "                                          how='left',on='defillama_slug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_categorized_chains.columns)\n",
    "# all_categorized_chains.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert float values in 'parent' column to dictionaries\n",
    "categorized_chains['parent'] = categorized_chains['parent'].apply(lambda x: {} if pd.isna(x) else x)\n",
    "\n",
    "# Check if the chain belongs to EVM category\n",
    "categorized_chains['is_EVM'] = categorized_chains['categories'].apply(lambda x: isinstance(x, list) and 'EVM' in x)\n",
    "\n",
    "# Check if the chain belongs to Rollup category or L2 is in parent_types\n",
    "def is_rollup(row):\n",
    "    categories = row['categories']\n",
    "    parent = row.get('parent')\n",
    "    parent_types = parent.get('types') if parent else []\n",
    "    \n",
    "    if isinstance(categories, list) and 'Rollup' in categories:\n",
    "        return True\n",
    "    if 'L2' in parent_types:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def extract_layer(row):\n",
    "    parent = row.get('parent')\n",
    "    parent_types = parent.get('types') if parent else []\n",
    "    \n",
    "    if 'L2' in parent_types:\n",
    "        return 'L2'\n",
    "    elif 'L3' in parent_types:\n",
    "        return 'L3'\n",
    "    \n",
    "    return 'L1'\n",
    "\n",
    "categorized_chains['is_Rollup'] = categorized_chains.apply(is_rollup, axis=1)\n",
    "categorized_chains['layer'] = categorized_chains.apply(extract_layer, axis=1)\n",
    "\n",
    "#Replace opBNB\n",
    "categorized_chains['defillama_slug'] = categorized_chains['defillama_slug'].replace('Op_Bnb', 'opBNB')\n",
    "categorized_chains['defillama_slug'] = categorized_chains['defillama_slug'].replace('op_bnb', 'opBNB')\n",
    "\n",
    "# consider all chains\n",
    "considered_chains = categorized_chains.copy()#[categorized_chains['is_EVM']]\n",
    "considered_chains = considered_chains[['defillama_slug','layer','is_EVM','is_Rollup','chainId']]\n",
    "\n",
    "# considered_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_chains['chainId'] = categorized_chains['chainId'].astype(str).replace('nan', '')\n",
    "categorized_chains['parent'] = categorized_chains['parent'].astype(str)\n",
    "categorized_chains.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload config to bigquery\n",
    "table_id = 'defillama_chain_categorization'\n",
    "bqu.write_df_to_bq_table(categorized_chains, table_id = table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tvl_to_count_apps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPStack Metadata will auto pull, but we can also add curated protocols here (we handle for dupes)\n",
    "curated_chains = [\n",
    "    #L2s\n",
    "     ['Optimism', 'L2']\n",
    "    ,['Base', 'L2']\n",
    "    ,['Arbitrum', 'L2']\n",
    "    #non-dune L2s - Check L2Beat for comprehensiveness\n",
    "    ,['zkSync Era', 'L2']\n",
    "    ,['Polygon zkEVM', 'L2']\n",
    "    ,['Starknet', 'L2']\n",
    "    ,['Linea', 'L2']\n",
    "    ,['Mantle', 'L2']\n",
    "    ,['Scroll', 'L2']\n",
    "    ,['Boba', 'L2']\n",
    "    ,['Metis', 'L2']\n",
    "    ,['opBNB', 'L2']\n",
    "    ,['Rollux', 'L2']\n",
    "    ,['Manta', 'L2']\n",
    "    ,['Kroma','L2']\n",
    "    ,['Arbitrum Nova','L2']\n",
    "    #L1\n",
    "    ,['Ethereum', 'L1']\n",
    "    #Others\n",
    "    ,['Fantom', 'L1']\n",
    "    ,['Avalanche', 'L1']\n",
    "    ,['Gnosis' , 'L1']\n",
    "    ,['Celo', 'L1']\n",
    "    ,['Polygon', 'L1']\n",
    "    ,['BSC', 'L1']\n",
    "]\n",
    "\n",
    "curated_protocols = [\n",
    "         ['aevo', 'L2',['optimism','arbitrum','ethereum']]\n",
    "        ,['dydx', 'L2',['ethereum']]\n",
    "        ,['immutablex', 'L2',['ethereum']]\n",
    "        ,['apex-protocol', 'L2',['ethereum']]\n",
    "        # ,['brine.fi', 'L2',['ethereum']] #no longer listed\n",
    "        ,['loopring', 'L2',['ethereum']]\n",
    "        ,['aztec', 'L2',['ethereum']]\n",
    "        # ,['lyra-v2', 'L2', ['ethereum']] # renamed to derive-v2\n",
    "        ,['derive-v2', 'L2', ['ethereum']]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opstack_metadata = pd.read_csv('../op_chains_tracking/outputs/chain_metadata.csv')\n",
    "#filter to defillama chains\n",
    "tvl_opstack_metadata = opstack_metadata[~opstack_metadata['defillama_slug'].isna()].copy()\n",
    "# opstack_chains\n",
    "tvl_opstack_metadata = tvl_opstack_metadata[['defillama_slug','chain_layer','chain_type']]\n",
    "\n",
    "opstack_chains = tvl_opstack_metadata[~tvl_opstack_metadata['defillama_slug'].str.contains('protocols/')].copy()\n",
    "op_superchain_chains = tvl_opstack_metadata[tvl_opstack_metadata['chain_type'].notna()].copy()\n",
    "opstack_protocols = tvl_opstack_metadata[tvl_opstack_metadata['defillama_slug'].str.contains('protocols/')].copy()\n",
    "#clean column\n",
    "opstack_protocols['defillama_slug'] = opstack_protocols['defillama_slug'].str.replace('protocols/','')\n",
    "# Use apply to create an array with one element\n",
    "opstack_protocols['chain_list'] = opstack_protocols.apply(lambda x: ['ethereum'], axis=1) #guess, we can manually override if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(op_superchain_chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new lists\n",
    "# curated_chains_tmp = curated_chains.copy()\n",
    "raw_chains_df = categorized_chains[['defillama_slug','layer']].copy()\n",
    "protocols = curated_protocols.copy()\n",
    "# Iterate through the DataFrame and append data to 'chains'\n",
    "# for index, row in opstack_chains.iterrows():\n",
    "#     if all(row['defillama_slug'] != item[0] for item in curated_chains_tmp):\n",
    "#         curated_chains_tmp.append([row['defillama_slug'], row['chain_layer']])\n",
    "# Iterate through the DataFrame and append data to 'chains'\n",
    "for index, row in opstack_protocols.iterrows():\n",
    "    if all(row['defillama_slug'] != item[0] for item in protocols):\n",
    "        protocols.append([row['defillama_slug'], row['chain_layer'],row['chain_list']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_chains_df = pd.DataFrame(curated_chains_tmp, columns = ['defillama_slug','layer'])\n",
    "chains_df = raw_chains_df.merge(considered_chains, on = ['defillama_slug','layer'], how = 'left')\n",
    "\n",
    "chains_df.fillna({'is_EVM': True, 'is_Rollup': True}, inplace=True)\n",
    "chains_df = chains_df.infer_objects(copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = pd.Timestamp.now() - pd.DateOffset(months=12)\n",
    "cutoff_date = date_start.replace(day=1)\n",
    "# Filter the DataFrame to the last 12 mos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_agg = []\n",
    "# for c in raw_chains_df:\n",
    "#         try:\n",
    "#                 d = dfl.get_historical_chain_tvl(c[0])\n",
    "#                 d['chain'] = c[0]\n",
    "#                 d['layer'] = c[1]\n",
    "#                 d['defillama_slug'] = c[0]\n",
    "#                 d['source'] = 'chain'\n",
    "                \n",
    "#                 c_agg.append(d)\n",
    "#         except Exception as e:\n",
    "#                 print(f\"An unexpected error occurred for {c}: {e}\")\n",
    "#                 continue  # Continue to the next iteration of the loop\n",
    "\n",
    "\n",
    "\n",
    "total_rows = len(raw_chains_df)\n",
    "milestone = total_rows // 4  # This will be 25% of the total rows\n",
    "processed_rows = 0\n",
    "\n",
    "for index, row in raw_chains_df.iterrows():\n",
    "    try:\n",
    "        defillama_slug = row['defillama_slug']\n",
    "        layer = row['layer']\n",
    "\n",
    "        if dfl.matches_filter_pattern(defillama_slug):\n",
    "            # print(f\"Invalid chain name: {defillama_slug}\")\n",
    "            continue  # Skip to the next iteration\n",
    "\n",
    "        d = dfl.get_historical_chain_tvl(defillama_slug)\n",
    "        if d is not None and not d.empty:\n",
    "            d = d[d['date'] >= cutoff_date]\n",
    "\n",
    "            d['chain'] = defillama_slug\n",
    "            d['layer'] = layer\n",
    "            d['defillama_slug'] = defillama_slug\n",
    "            d['source'] = 'chain'\n",
    "            \n",
    "            c_agg.append(d)\n",
    "        \n",
    "        processed_rows += 1\n",
    "        \n",
    "        # Check if we've hit a 25% milestone\n",
    "        if processed_rows % milestone == 0:\n",
    "            percentage = (processed_rows / total_rows) * 100\n",
    "            print(f\"{processed_rows}/{total_rows} rows completed ({percentage:.0f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for {defillama_slug}: {e}\")\n",
    "        continue  # Continue to the next iteration of the loop\n",
    "\n",
    "\n",
    "df_ch = pd.concat(c_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the chains already present in chains_df\n",
    "existing_chains = set(chains_df['defillama_slug'])\n",
    "\n",
    "# Filter out the chains from considered_chains that are not in chains_df\n",
    "to_append = considered_chains[~considered_chains['defillama_slug'].isin(existing_chains)]\n",
    "\n",
    "# Append the selected rows to chains_df\n",
    "chains_df = pd.concat([chains_df, to_append], ignore_index=True)\n",
    "\n",
    "\n",
    "# merged_chains_df.sort_values(by='chain').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_name_list = chains_df['defillama_slug'].unique().tolist()\n",
    "get_app_list = op_superchain_chains['defillama_slug'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('get tvls')\n",
    "\n",
    "p = dfl.get_all_protocol_tvls_by_chain_and_token(\n",
    "    min_tvl=min_tvl_to_count_apps, \n",
    "    chains = get_app_list,\n",
    "    do_aggregate = 'Yes'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by date\n",
    "p = p.sort_values(by='date',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data\n",
    "\n",
    "app_level_dfl_list = p.groupby(['chain', 'date','parent_protocol','protocol']).agg(\n",
    "    sum_token_value_usd_flow=pd.NamedAgg(column='sum_token_value_usd_flow', aggfunc='sum'),\n",
    "    sum_token_value_usd_price_change=pd.NamedAgg(column='sum_token_value_usd_price_change', aggfunc='sum'),\n",
    "    avg_usd_tvl=pd.NamedAgg(column='sum_usd_value', aggfunc='mean')\n",
    "\n",
    ").reset_index()\n",
    "\n",
    "app_level_dfl_list = app_level_dfl_list.rename(columns={'date':'dt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bqu.write_df_to_bq_table(app_level_dfl_list, 'daily_defillama_op_chain_app_tvl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try to use config API to get Apps by Chain, since this only shows TVLs\n",
    "\n",
    "app_dfl_list = p.groupby(['chain', 'date']).agg(\n",
    "    distinct_protocol=pd.NamedAgg(column='protocol', aggfunc=pd.Series.nunique),\n",
    "    distinct_parent_protocol=pd.NamedAgg(column='parent_protocol', aggfunc=pd.Series.nunique),\n",
    "    sum_token_value_usd_flow=pd.NamedAgg(column='sum_token_value_usd_flow', aggfunc='sum'),\n",
    "    sum_token_value_usd_price_change=pd.NamedAgg(column='sum_token_value_usd_price_change', aggfunc='sum'),\n",
    "    avg_usd_tvl=pd.NamedAgg(column='sum_usd_value', aggfunc='mean')\n",
    ")\n",
    "\n",
    "p = None #Clear Memory\n",
    "\n",
    "app_dfl_list = app_dfl_list.reset_index()\n",
    "app_dfl_list = app_dfl_list.rename(columns={'chain':'defillama_slug'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_agg = []\n",
    "for p in protocols:\n",
    "    try:\n",
    "        d = dfl.get_single_tvl(p[0], p[2], print_api_str=False) #Set print_api_str=True for debugging\n",
    "        # print(f\"Raw content for {p}: {d}\")\n",
    "        d['chain_prot'] = p[0].title()\n",
    "        d['layer'] = p[1]\n",
    "        d['defillama_slug'] = 'protocols/' + p[0]\n",
    "        d['source'] = 'protocol'\n",
    "        # d['prot_chain'] = c\n",
    "        p_agg.append(d)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON for {p}: {e}\")\n",
    "        continue  # Continue to the next iteration of the loop\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for {p}: {e}\")\n",
    "        continue  # Continue to the next iteration of the loop\n",
    "\n",
    "df = pd.concat(p_agg)\n",
    "df = df.fillna(0)\n",
    "# prob use total_app_tvl since it has more history and we're not yet doing flows\n",
    "df_sum = df.groupby(['date', 'chain_prot', 'layer', 'defillama_slug', 'source']).agg({'usd_value':'sum'}).reset_index()\n",
    "# print(df_sum.columns)\n",
    "\n",
    "df_sum = df_sum[['date', 'usd_value', 'chain_prot', 'layer', 'defillama_slug', 'source']]\n",
    "df_sum = df_sum.rename(columns={'chain_prot': 'chain', 'usd_value': 'tvl'})\n",
    "\n",
    "# print(df_sum.tail(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_ch,df_sum])\n",
    "# Rename\n",
    "df['chain'] = df['chain'].str.replace('%20', ' ', regex=False)\n",
    "df['chain'] = df['chain'].str.replace('-', ' ', regex=False)\n",
    "df.loc[df['chain'] == 'Optimism', 'chain'] = 'OP Mainnet'\n",
    "df.loc[df['chain'] == 'BSC', 'chain'] = 'BNB'\n",
    "df.loc[df['chain'] == 'Brine.Fi', 'chain'] = 'Brine'\n",
    "df.loc[df['chain'] == 'Immutablex', 'chain'] = 'ImmutableX'\n",
    "df.loc[df['chain'] == 'dYdX v3', 'chain'] = 'dYdX'\n",
    "\n",
    "\n",
    "df = df[df['date'] <= current_utc_date ] #rm dupes at current datetime\n",
    "df['tvl'] = df['tvl'].fillna(0)\n",
    "df['tvl'] = df['tvl'].replace('', 0)\n",
    "\n",
    "# df['date'] = pd.to_datetime(df['date']) - timedelta(days=1) #map to the prior day, since dfl adds an extra day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_sum[df_sum['chain']=='Aevo'])\n",
    "# display(df_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(app_dfl_list, on =['defillama_slug','date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df[df['chain']=='Mode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod defillama slugs for joining\n",
    "opstack_metadata['clean_slug'] = opstack_metadata['defillama_slug'].str.replace('/', '')\n",
    "df['clean_slug'] = df['defillama_slug'].str.replace('/', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_st.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st = df.copy()\n",
    "# Add Metadata\n",
    "meta_cols = ['defillama_slug', 'is_op_chain','mainnet_chain_id','op_based_version', 'alignment','chain_name','display_name']\n",
    "\n",
    "df_st =df_st.merge(opstack_metadata[~opstack_metadata['defillama_slug'].isna()][meta_cols + ['clean_slug']], on='clean_slug', how = 'left')\n",
    "\n",
    "\n",
    "df_st['is_op_chain'] = df_st['is_op_chain'].fillna(False)\n",
    "df_st['date'] = pd.to_datetime(df['date'])\n",
    "df_st['tvl'].fillna(0, inplace=True)  # Or use an appropriate default value\n",
    "\n",
    "\n",
    "df_st['defillama_slug'] = df_st['defillama_slug_x'].combine_first(df_st['defillama_slug_y'])\n",
    "\n",
    "#DFL Metadata cols\n",
    "df_st = df_st.merge(chains_df, on = ['defillama_slug'], how = 'left')\n",
    "\n",
    "df_st['alignment'] = df_st['alignment'].fillna(df_st['is_EVM'].map({True: 'Other EVMs', False: 'Other Non-EVMs'}))\n",
    "\n",
    "# Coalesce layer_x and layer_y into a new column layer\n",
    "df_st['layer'] = df_st['layer_x'].combine_first(df_st['layer_y'])\n",
    "\n",
    "\n",
    "# Drop the original layer_x and layer_y columns\n",
    "df_st.drop(['layer_x', 'layer_y','defillama_slug_x','defillama_slug_y','clean_slug'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_st.copy()\n",
    "df_st = None #Clear memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mod Display Name\n",
    "df['display_name'] = df['display_name'].combine_first(df['chain'])\n",
    "# Convert the column to string\n",
    "df['is_EVM'] = df['is_EVM'].astype(str)\n",
    "df['is_Rollup'] = df['is_Rollup'].astype(str)\n",
    "df['chainId'] = df['chainId'].fillna(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.Timestamp.now() - pd.DateOffset(months=trailing_mos)\n",
    "cutoff_date = start_date.replace(day=1)\n",
    "# Filter the DataFrame to the last X mos\n",
    "df = df[df['date'] >= cutoff_date]\n",
    "\n",
    "\n",
    "print('len before filter: ' + str(len(df)))\n",
    "print('Unique display_name values before filter:')\n",
    "# print(df['display_name'].unique())\n",
    "\n",
    "# Filter to complete dates\n",
    "df = df[df['date'].dt.time == pd.Timestamp('00:00:00').time()]\n",
    "\n",
    "print('\\nlen after filter: ' + str(len(df)))\n",
    "print('Unique display_name values after filter:')\n",
    "# print(df['display_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define aggregation functions for each column\n",
    "aggregations = {\n",
    "    'tvl': ['min', 'last', 'mean'],\n",
    "    'distinct_protocol': ['min', 'last', 'mean'],\n",
    "    'distinct_parent_protocol': ['min', 'last', 'mean'],\n",
    "    'sum_token_value_usd_flow': 'sum',\n",
    "    'sum_token_value_usd_price_change': 'sum'\n",
    "}\n",
    "# Function to perform aggregation based on frequency\n",
    "def aggregate_data(df, freq, date_col='date', groupby_cols=None, aggs=None):\n",
    "    if groupby_cols is None:\n",
    "        groupby_cols = ['chain', 'layer', 'defillama_slug', 'source', 'is_op_chain', 'mainnet_chain_id', 'op_based_version', 'alignment', 'chain_name', 'display_name', 'chainId']\n",
    "    if aggs is None:\n",
    "        aggs = aggregations\n",
    "\n",
    "    # Ensure the date column is in datetime format\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    # Determine the start of the week after the first date in the DataFrame\n",
    "    first_date = df[date_col].min()\n",
    "    days_until_next_monday = (7 - first_date.weekday()) % 7\n",
    "    start_of_week = first_date + pd.Timedelta(days=days_until_next_monday)\n",
    "\n",
    "    # Filter the DataFrame to start from the beginning of the next week\n",
    "    df = df[df[date_col] >= start_of_week]\n",
    "\n",
    "    # Group by the specified frequency and other columns, then apply aggregations\n",
    "    df_agg = df.groupby([pd.Grouper(key=date_col, freq=freq, closed='left')] + groupby_cols, dropna=False).agg(aggs).reset_index()\n",
    "\n",
    "    # Flatten the hierarchical column index and concatenate aggregation function names with column names\n",
    "    df_agg.columns = [f'{col}_{func}' if func != '' else col for col, func in df_agg.columns]\n",
    "\n",
    "    # Rename the 'date' column based on the frequency\n",
    "    date_col_name = 'month' if freq == 'MS' else 'week'\n",
    "    df_agg.rename(columns={f'{date_col}_': date_col_name}, inplace=True)\n",
    "\n",
    "    return df_agg\n",
    "\n",
    "# Perform monthly aggregation\n",
    "df_monthly = aggregate_data(df, freq='MS')\n",
    "\n",
    "# Perform weekly aggregation\n",
    "df_weekly = aggregate_data(df, freq='W-MON')\n",
    "\n",
    "# Now df_monthly and df_weekly contain the aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['chain'] == 'Ethereum'].tail(5)\n",
    "# df_monthly.sample(10)\n",
    "# df_weekly.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BQ Upload\n",
    "unique_cols = ['date','chain']\n",
    "bqu.append_and_upsert_df_to_bq_table(df, 'daily_defillama_chain_tvl', unique_keys = unique_cols)\n",
    "time.sleep(1)\n",
    "bqu.append_and_upsert_df_to_bq_table(df_monthly, 'monthly_defillama_chain_tvl', unique_keys = unique_cols)\n",
    "time.sleep(1)\n",
    "bqu.append_and_upsert_df_to_bq_table(df_weekly, 'weekly_defillama_chain_tvl', unique_keys = unique_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# folder = 'outputs/'\n",
    "# df.to_csv(folder + 'dfl_chain_tvl.csv', index = False)\n",
    "# df_365.to_csv(folder + 'dfl_chain_tvl_t365d.csv', index = False)\n",
    "# df_monthly.to_csv(folder + 'dfl_chain_tvl_monthly.csv', index = False)\n",
    "# df_weekly.to_csv(folder + 'dfl_chain_tvl_weekly.csv', index = False)\n",
    "# Write to Dune\n",
    "du.write_dune_api_from_pandas(df, 'dfl_chain_tvl',\\\n",
    "                             'TVL for select chains from DefiLlama')\n",
    "du.write_dune_api_from_pandas(df_monthly, 'dfl_chain_tv_monthly',\\\n",
    "                             'Monthly TVL for select chains from DefiLlama')\n",
    "du.write_dune_api_from_pandas(df_weekly, 'dfl_chain_tv_weekly',\\\n",
    "                             'Weekly TVL for select chains from DefiLlama')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
