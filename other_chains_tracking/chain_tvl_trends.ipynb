{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('start tvl trends')\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import requests as r\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "sys.path.append(\"../helper_functions\")\n",
    "import defillama_utils as dfl\n",
    "import duneapi_utils as du\n",
    "import opstack_metadata_utils as ops\n",
    "import csv_utils as cu\n",
    "import google_bq_utils as bqu\n",
    "sys.path.pop()\n",
    "\n",
    "current_utc_date = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "# print(current_utc_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_chains = dfl.get_chains_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorized_chains.to_csv('outputs/dfl_categorized_chains.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert float values in 'parent' column to dictionaries\n",
    "categorized_chains['parent'] = categorized_chains['parent'].apply(lambda x: {} if pd.isna(x) else x)\n",
    "\n",
    "# Check if the chain belongs to EVM category\n",
    "categorized_chains['is_EVM'] = categorized_chains['categories'].apply(lambda x: isinstance(x, list) and 'EVM' in x)\n",
    "\n",
    "# Check if the chain belongs to Rollup category or L2 is in parent_types\n",
    "def is_rollup(row):\n",
    "    categories = row['categories']\n",
    "    parent = row.get('parent')\n",
    "    parent_types = parent.get('types') if parent else []\n",
    "    \n",
    "    if isinstance(categories, list) and 'Rollup' in categories:\n",
    "        return True\n",
    "    if 'L2' in parent_types:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def extract_layer(row):\n",
    "    parent = row.get('parent')\n",
    "    parent_types = parent.get('types') if parent else []\n",
    "    \n",
    "    if 'L2' in parent_types:\n",
    "        return 'L2'\n",
    "    elif 'L3' in parent_types:\n",
    "        return 'L3'\n",
    "    \n",
    "    return 'L1'\n",
    "\n",
    "categorized_chains['is_Rollup'] = categorized_chains.apply(is_rollup, axis=1)\n",
    "categorized_chains['layer'] = categorized_chains.apply(extract_layer, axis=1)\n",
    "\n",
    "#Replace opBNB\n",
    "categorized_chains['defillama_slug'] = categorized_chains['defillama_slug'].replace('Op_Bnb', 'opBNB')\n",
    "\n",
    "considered_chains = categorized_chains[categorized_chains['is_EVM']]\n",
    "considered_chains = considered_chains[['defillama_slug','layer','is_EVM','is_Rollup','chainId']]\n",
    "\n",
    "# considered_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_chains['chainId'] = categorized_chains['chainId'].astype(str).replace('nan', '')\n",
    "categorized_chains['parent'] = categorized_chains['parent'].astype(str)\n",
    "categorized_chains.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload config to bigquery\n",
    "table_id = 'defillama_chain_categorization'\n",
    "bqu.write_df_to_bq_table(categorized_chains, table_id = table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tvl_to_count_apps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPStack Metadata will auto pull, but we can also add curated protocols here (we handle for dupes)\n",
    "curated_chains = [\n",
    "    #L2s\n",
    "     ['Optimism', 'L2']\n",
    "    ,['Base', 'L2']\n",
    "    ,['Arbitrum', 'L2']\n",
    "    #non-dune L2s - Check L2Beat for comprehensiveness\n",
    "    ,['zkSync Era', 'L2']\n",
    "    ,['Polygon zkEVM', 'L2']\n",
    "    ,['Starknet', 'L2']\n",
    "    ,['Linea', 'L2']\n",
    "    ,['Mantle', 'L2']\n",
    "    ,['Scroll', 'L2']\n",
    "    ,['Boba', 'L2']\n",
    "    ,['Metis', 'L2']\n",
    "    ,['opBNB', 'L2']\n",
    "    ,['Rollux', 'L2']\n",
    "    ,['Manta', 'L2']\n",
    "    ,['Kroma','L2']\n",
    "    ,['Arbitrum Nova','L2']\n",
    "    #L1\n",
    "    ,['Ethereum', 'L1']\n",
    "    #Others\n",
    "    ,['Fantom', 'L1']\n",
    "    ,['Avalanche', 'L1']\n",
    "    ,['Gnosis' , 'L1']\n",
    "    ,['Celo', 'L1']\n",
    "    ,['Polygon', 'L1']\n",
    "    ,['BSC', 'L1']\n",
    "]\n",
    "\n",
    "curated_protocols = [\n",
    "         ['aevo', 'L2',['optimism','arbitrum','ethereum']]\n",
    "        ,['dydx', 'L2',['ethereum']]\n",
    "        ,['immutablex', 'L2',['ethereum']]\n",
    "        ,['apex-protocol', 'L2',['ethereum']]\n",
    "        # ,['brine.fi', 'L2',['ethereum']] #no longer listed\n",
    "        ,['loopring', 'L2',['ethereum']]\n",
    "        ,['aztec', 'L2',['ethereum']]\n",
    "        ,['lyra-v2', 'L2', ['ethereum']]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opstack_metadata = pd.read_csv('../op_chains_tracking/outputs/chain_metadata.csv')\n",
    "#filter to defillama chains\n",
    "tvl_opstack_metadata = opstack_metadata[~opstack_metadata['defillama_slug'].isna()].copy()\n",
    "# opstack_chains\n",
    "tvl_opstack_metadata = tvl_opstack_metadata[['defillama_slug','chain_layer','chain_type']]\n",
    "\n",
    "opstack_chains = tvl_opstack_metadata[~tvl_opstack_metadata['defillama_slug'].str.contains('protocols/')].copy()\n",
    "op_superchain_chains = tvl_opstack_metadata[tvl_opstack_metadata['chain_type'].notna()].copy()\n",
    "opstack_protocols = tvl_opstack_metadata[tvl_opstack_metadata['defillama_slug'].str.contains('protocols/')].copy()\n",
    "#clean column\n",
    "opstack_protocols['defillama_slug'] = opstack_protocols['defillama_slug'].str.replace('protocols/','')\n",
    "# Use apply to create an array with one element\n",
    "opstack_protocols['chain_list'] = opstack_protocols.apply(lambda x: ['ethereum'], axis=1) #guess, we can manually override if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(op_superchain_chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new lists\n",
    "curated_chains_tmp = curated_chains.copy()\n",
    "protocols = curated_protocols.copy()\n",
    "# Iterate through the DataFrame and append data to 'chains'\n",
    "for index, row in opstack_chains.iterrows():\n",
    "    if all(row['defillama_slug'] != item[0] for item in curated_chains_tmp):\n",
    "        curated_chains_tmp.append([row['defillama_slug'], row['chain_layer']])\n",
    "# Iterate through the DataFrame and append data to 'chains'\n",
    "for index, row in opstack_protocols.iterrows():\n",
    "    if all(row['defillama_slug'] != item[0] for item in protocols):\n",
    "        protocols.append([row['defillama_slug'], row['chain_layer'],row['chain_list']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_chains_df = pd.DataFrame(curated_chains_tmp, columns = ['defillama_slug','layer'])\n",
    "chains_df = raw_chains_df.merge(considered_chains, on = ['defillama_slug','layer'], how = 'left')\n",
    "\n",
    "chains_df.fillna({'is_EVM': True, 'is_Rollup': True}, inplace=True)\n",
    "chains_df = chains_df.infer_objects(copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chains_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the chains already present in chains_df\n",
    "existing_chains = set(chains_df['defillama_slug'])\n",
    "\n",
    "# Filter out the chains from considered_chains that are not in chains_df\n",
    "to_append = considered_chains[~considered_chains['defillama_slug'].isin(existing_chains)]\n",
    "\n",
    "# Append the selected rows to chains_df\n",
    "chains_df = pd.concat([chains_df, to_append], ignore_index=True)\n",
    "\n",
    "\n",
    "# merged_chains_df.sort_values(by='chain').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_name_list = chains_df['defillama_slug'].unique().tolist()\n",
    "get_app_list = op_superchain_chains['defillama_slug'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('get tvls')\n",
    "\n",
    "p = dfl.get_all_protocol_tvls_by_chain_and_token(\n",
    "    min_tvl=min_tvl_to_count_apps, \n",
    "    chains = get_app_list,\n",
    "    do_aggregate = 'Yes'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p\n",
    "# print('gen flows')\n",
    "# p = dfl.generate_flows_column(p)\n",
    "# p = dfl.get_tvl(apistring= 'https://api.llama.fi/protocol/velodrome', chains = chain_name_list, prot = 'velodrome',prot_name='Velodrome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by date\n",
    "p = p.sort_values(by='date',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data\n",
    "\n",
    "app_level_dfl_list = p.groupby(['chain', 'date','parent_protocol','protocol']).agg(\n",
    "    sum_token_value_usd_flow=pd.NamedAgg(column='sum_token_value_usd_flow', aggfunc='sum'),\n",
    "    sum_token_value_usd_price_change=pd.NamedAgg(column='sum_token_value_usd_price_change', aggfunc='sum'),\n",
    "    avg_usd_tvl=pd.NamedAgg(column='sum_usd_value', aggfunc='mean')\n",
    "\n",
    ").reset_index()\n",
    "\n",
    "app_level_dfl_list = app_level_dfl_list.rename(columns={'date':'dt'})\n",
    "\n",
    "bqu.write_df_to_bq_table(app_level_dfl_list, 'daily_defillama_op_chain_app_tvl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try to use config API to get Apps by Chain, since this only shows TVLs\n",
    "\n",
    "app_dfl_list = p.groupby(['chain', 'date']).agg(\n",
    "    distinct_protocol=pd.NamedAgg(column='protocol', aggfunc=pd.Series.nunique),\n",
    "    distinct_parent_protocol=pd.NamedAgg(column='parent_protocol', aggfunc=pd.Series.nunique),\n",
    "    sum_token_value_usd_flow=pd.NamedAgg(column='sum_token_value_usd_flow', aggfunc='sum'),\n",
    "    sum_token_value_usd_price_change=pd.NamedAgg(column='sum_token_value_usd_price_change', aggfunc='sum'),\n",
    "    avg_usd_tvl=pd.NamedAgg(column='sum_usd_value', aggfunc='mean')\n",
    ")\n",
    "\n",
    "p = None #Clear Memory\n",
    "\n",
    "app_dfl_list = app_dfl_list.reset_index()\n",
    "app_dfl_list = app_dfl_list.rename(columns={'chain':'defillama_slug'})\n",
    "\n",
    "\n",
    "# display(app_dfl_list)\n",
    "# app_dfl_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(app_dfl_list[app_dfl_list['defillama_slug']=='Mode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_agg = []\n",
    "for p in protocols:\n",
    "    try:\n",
    "        d = dfl.get_single_tvl(p[0], p[2], print_api_str=False) #Set print_api_str=True for debugging\n",
    "        # print(f\"Raw content for {p}: {d}\")\n",
    "        d['chain_prot'] = p[0].title()\n",
    "        d['layer'] = p[1]\n",
    "        d['defillama_slug'] = 'protocols/' + p[0]\n",
    "        d['source'] = 'protocol'\n",
    "        # d['prot_chain'] = c\n",
    "        p_agg.append(d)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON for {p}: {e}\")\n",
    "        continue  # Continue to the next iteration of the loop\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for {p}: {e}\")\n",
    "        continue  # Continue to the next iteration of the loop\n",
    "\n",
    "df = pd.concat(p_agg)\n",
    "df = df.fillna(0)\n",
    "# prob use total_app_tvl since it has more history and we're not yet doing flows\n",
    "df_sum = df.groupby(['date', 'chain_prot', 'layer', 'defillama_slug', 'source']).agg({'usd_value':'sum'}).reset_index()\n",
    "# print(df_sum.columns)\n",
    "\n",
    "df_sum = df_sum[['date', 'usd_value', 'chain_prot', 'layer', 'defillama_slug', 'source']]\n",
    "df_sum = df_sum.rename(columns={'chain_prot': 'chain', 'usd_value': 'tvl'})\n",
    "\n",
    "# print(df_sum.tail(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_sum[df_sum['chain']=='Aevo'])\n",
    "# display(df_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_agg = []\n",
    "for c in curated_chains_tmp:\n",
    "        try:\n",
    "                d = dfl.get_historical_chain_tvl(c[0])\n",
    "                d['chain'] = c[0]\n",
    "                d['layer'] = c[1]\n",
    "                d['defillama_slug'] = c[0]\n",
    "                d['source'] = 'chain'\n",
    "                \n",
    "                c_agg.append(d)\n",
    "        except Exception as e:\n",
    "                print(f\"An unexpected error occurred for {c}: {e}\")\n",
    "                continue  # Continue to the next iteration of the loop\n",
    "\n",
    "\n",
    "df_ch = pd.concat(c_agg)\n",
    "df = pd.concat([df_ch,df_sum])\n",
    "# Rename\n",
    "df['chain'] = df['chain'].str.replace('%20', ' ', regex=False)\n",
    "df['chain'] = df['chain'].str.replace('-', ' ', regex=False)\n",
    "df.loc[df['chain'] == 'Optimism', 'chain'] = 'OP Mainnet'\n",
    "df.loc[df['chain'] == 'BSC', 'chain'] = 'BNB'\n",
    "df.loc[df['chain'] == 'Brine.Fi', 'chain'] = 'Brine'\n",
    "df.loc[df['chain'] == 'Immutablex', 'chain'] = 'ImmutableX'\n",
    "\n",
    "\n",
    "df = df[df['date'] <= current_utc_date ] #rm dupes at current datetime\n",
    "df['tvl'] = df['tvl'].fillna(0)\n",
    "df['tvl'] = df['tvl'].replace('', 0)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date']) - timedelta(days=1) #map to the prior day, since dfl adds an extra day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(app_dfl_list, on =['defillama_slug','date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df[df['chain']=='Mode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Metadata\n",
    "meta_cols = ['defillama_slug', 'is_op_chain','mainnet_chain_id','op_based_version', 'alignment','chain_name','display_name']\n",
    "\n",
    "df = df.merge(opstack_metadata[meta_cols], on='defillama_slug', how = 'left')\n",
    "\n",
    "df['alignment'] = df['alignment'].fillna('Other EVMs')\n",
    "df['is_op_chain'] = df['is_op_chain'].fillna(False)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['tvl'].fillna(0, inplace=True)  # Or use an appropriate default value\n",
    "\n",
    "#DFL Metadata cols\n",
    "df = df.merge(chains_df, on = ['defillama_slug'], how = 'left')\n",
    "\n",
    "# Coalesce layer_x and layer_y into a new column layer\n",
    "df['layer'] = df['layer_x'].combine_first(df['layer_y'])\n",
    "\n",
    "# Drop the original layer_x and layer_y columns\n",
    "df.drop(['layer_x', 'layer_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column to string\n",
    "df['is_EVM'] = df['is_EVM'].astype(str)\n",
    "df['is_Rollup'] = df['is_Rollup'].astype(str)\n",
    "df['chainId'] = df['chainId'].fillna(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=365)\n",
    "# Filter the DataFrame to the last 365 days\n",
    "df_365 = df[df['date'] >= cutoff_date]\n",
    "# df_365.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define aggregation functions for each column\n",
    "aggregations = {\n",
    "    'tvl': ['min', 'last', 'mean'],\n",
    "    'distinct_protocol': ['min', 'last', 'mean'],\n",
    "    'distinct_parent_protocol': ['min', 'last', 'mean'],\n",
    "    'sum_token_value_usd_flow': 'sum',\n",
    "    'sum_token_value_usd_price_change': 'sum'\n",
    "}\n",
    "# Function to perform aggregation based on frequency\n",
    "def aggregate_data(df, freq, date_col='date', groupby_cols=None, aggs=None):\n",
    "    if groupby_cols is None:\n",
    "        groupby_cols = ['chain', 'layer', 'defillama_slug', 'source', 'is_op_chain', 'mainnet_chain_id', 'op_based_version', 'alignment', 'chain_name', 'display_name', 'chainId']\n",
    "    if aggs is None:\n",
    "        aggs = aggregations\n",
    "\n",
    "    # Group by the specified frequency and other columns, then apply aggregations\n",
    "    df_agg = df.groupby([pd.Grouper(key=date_col, freq=freq, closed='left')] + groupby_cols, dropna=False).agg(aggs).reset_index()\n",
    "    # Flatten the hierarchical column index and concatenate aggregation function names with column names\n",
    "    df_agg.columns = [f'{col}_{func}' if func != '' else col for col, func in df_agg.columns]\n",
    "    # Rename the 'date' column based on the frequency\n",
    "    date_col_name = 'month' if freq == 'MS' else 'week'\n",
    "    df_agg.rename(columns={f'{date_col}_': date_col_name}, inplace=True)\n",
    "    df_agg['date'] = pd.to_datetime(df_agg['date'])\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "# Perform monthly aggregation\n",
    "df_monthly = aggregate_data(df, freq='MS')\n",
    "\n",
    "# Perform weekly aggregation\n",
    "df_weekly = aggregate_data(df, freq='W-MON')\n",
    "\n",
    "# Now df_monthly and df_weekly contain the aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['chain'] == 'Ethereum'].tail(5)\n",
    "# df_monthly.sample(10)\n",
    "# df_weekly.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# folder = 'outputs/'\n",
    "# df.to_csv(folder + 'dfl_chain_tvl.csv', index = False)\n",
    "# df_365.to_csv(folder + 'dfl_chain_tvl_t365d.csv', index = False)\n",
    "# df_monthly.to_csv(folder + 'dfl_chain_tvl_monthly.csv', index = False)\n",
    "# df_weekly.to_csv(folder + 'dfl_chain_tvl_weekly.csv', index = False)\n",
    "# Write to Dune\n",
    "du.write_dune_api_from_pandas(df, 'dfl_chain_tvl',\\\n",
    "                             'TVL for select chains from DefiLlama')\n",
    "du.write_dune_api_from_pandas(df_monthly, 'dfl_chain_tv_monthly',\\\n",
    "                             'Monthly TVL for select chains from DefiLlama')\n",
    "du.write_dune_api_from_pandas(df_weekly, 'dfl_chain_tv_weekly',\\\n",
    "                             'Weekly TVL for select chains from DefiLlama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BQ Upload\n",
    "bqu.write_df_to_bq_table(df, 'daily_defillama_chain_tvl')\n",
    "time.sleep(1)\n",
    "bqu.write_df_to_bq_table(df_monthly, 'monthly_defillama_chain_tvl')\n",
    "time.sleep(1)\n",
    "bqu.write_df_to_bq_table(df_weekly, 'weekly_defillama_chain_tvl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
