{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# DeFiLlama Data Backfills\n",
        "\n",
        "This notebook allows you to backfill various DeFiLlama datasources with configurable parameters.\n",
        "\n",
        "## Available Datasources:\n",
        "\n",
        "1. **Volume, Fees, Revenue** - DEX trading volume, protocol fees, and revenue data by chain and protocol\n",
        "2. **Protocols TVL** - Total Value Locked for individual protocols with detailed breakdowns\n",
        "3. **Chain TVL** - Historical TVL data aggregated by blockchain \n",
        "4. **Stablecoins** - Stablecoin circulation and bridging data by chain\n",
        "5. **Yield Pools** - Yield farming pool data and APY information\n",
        "6. **Lend/Borrow Pools** - Lending protocol data including rates and volumes\n",
        "\n",
        "## Configuration Options:\n",
        "\n",
        "- **BACKFILL_DAYS**: Number of days to backfill (default 365)\n",
        "- **SPECIFIC_CHAIN**: Filter to a specific chain (e.g. \"optimism\", \"base\") or None for all\n",
        "- **SPECIFIC_PROTOCOL**: Filter to a specific protocol slug or None for all\n",
        "\n",
        "## Usage:\n",
        "\n",
        "1. Modify the configuration variables in the first cell\n",
        "2. Run the cells for the datasources you want to backfill\n",
        "3. Comment/uncomment sections as needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from unittest.mock import patch\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from op_analytics.coreutils.partitioned import dailydatawrite\n",
        "from op_analytics.coreutils.partitioned.location import DataLocation\n",
        "\n",
        "\n",
        "def mock_location():\n",
        "    return DataLocation.GCS\n",
        "\n",
        "# Configuration\n",
        "os.environ[\"ALLOW_WRITE\"] = \"true\"\n",
        "\n",
        "# Backfill Configuration\n",
        "BACKFILL_DAYS = 365  # Number of days to backfill\n",
        "SPECIFIC_CHAIN = None  # Set to chain name (e.g. \"optimism\") to filter, or None for all chains\n",
        "SPECIFIC_PROTOCOL = None  # Set to protocol slug to filter, or None for all protocols\n",
        "\n",
        "# Force Complete Overwrite Settings\n",
        "FORCE_COMPLETE_OVERWRITE = True  # Set to True to overwrite existing data\n",
        "CLEAR_EXISTING_MARKERS = True    # Set to True to ignore existing completion markers\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Backfill Days: {BACKFILL_DAYS}\")\n",
        "print(f\"  Specific Chain: {SPECIFIC_CHAIN or 'All chains'}\")\n",
        "print(f\"  Specific Protocol: {SPECIFIC_PROTOCOL or 'All protocols'}\")\n",
        "print(f\"  Data Location: GCS\")\n",
        "print(f\"  Force Complete Overwrite: {FORCE_COMPLETE_OVERWRITE}\")\n",
        "print(f\"  Clear Existing Markers: {CLEAR_EXISTING_MARKERS}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Volume, Fees, Revenue (VFR) Data\n",
        "\n",
        "This datasource pulls DEX volume, protocol fees, and revenue data from DeFiLlama.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Volume, Fees, Revenue Backfill\n",
        "print(\"Starting Volume, Fees, Revenue backfill...\")\n",
        "print(f\"  Force overwrite: {FORCE_COMPLETE_OVERWRITE}\")\n",
        "\n",
        "def mock_marker_exists(*args, **kwargs):\n",
        "    \"\"\"Mock function to always return False, forcing overwrite.\"\"\"\n",
        "    if CLEAR_EXISTING_MARKERS:\n",
        "        return False\n",
        "    return original_marker_exists(*args, **kwargs)\n",
        "\n",
        "with patch.object(dailydatawrite, \"determine_location\", mock_location):\n",
        "    # Import required modules\n",
        "    from op_analytics.datasources.defillama.volumefeesrevenue import execute as vfr_execute\n",
        "    from op_analytics.coreutils.partitioned import dataaccess\n",
        "    \n",
        "    # Store original function\n",
        "    original_marker_exists = dataaccess.PartitionedDataAccess.marker_exists\n",
        "    \n",
        "    # Patch the constant to use our backfill days and optionally bypass markers\n",
        "    with patch.object(vfr_execute, 'TABLE_LAST_N_DAYS', BACKFILL_DAYS):\n",
        "        if CLEAR_EXISTING_MARKERS:\n",
        "            with patch.object(dataaccess.PartitionedDataAccess, 'marker_exists', mock_marker_exists):\n",
        "                result = vfr_execute.execute_pull()\n",
        "        else:\n",
        "            result = vfr_execute.execute_pull()\n",
        "        \n",
        "print(\"Volume, Fees, Revenue backfill completed!\")\n",
        "print(f\"Result summary: {result}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Protocols TVL Data\n",
        "\n",
        "This datasource pulls detailed TVL data for individual protocols with token-level breakdowns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Protocols TVL Backfill\n",
        "print(\"Starting Protocols TVL backfill...\")\n",
        "print(f\"  Force overwrite: {FORCE_COMPLETE_OVERWRITE}\")\n",
        "\n",
        "def mock_get_buffered(*args, **kwargs):\n",
        "    \"\"\"Mock function to return empty set, forcing all protocols to be fetched.\"\"\"\n",
        "    if FORCE_COMPLETE_OVERWRITE:\n",
        "        return set()  # Return empty set so all protocols are considered \"pending\"\n",
        "    return original_get_buffered(*args, **kwargs)\n",
        "\n",
        "def mock_marker_exists(*args, **kwargs):\n",
        "    \"\"\"Mock function to always return False, forcing overwrite.\"\"\"\n",
        "    if CLEAR_EXISTING_MARKERS:\n",
        "        return False\n",
        "    return original_marker_exists(*args, **kwargs)\n",
        "\n",
        "with patch.object(dailydatawrite, \"determine_location\", mock_location):\n",
        "    # Import required modules\n",
        "    from op_analytics.datasources.defillama.protocolstvl import execute as protocols_execute\n",
        "    from op_analytics.datasources.defillama.protocolstvl import buffereval\n",
        "    from op_analytics.coreutils.partitioned import dataaccess\n",
        "    \n",
        "    # Store original functions\n",
        "    original_get_buffered = buffereval.get_buffered\n",
        "    original_marker_exists = dataaccess.PartitionedDataAccess.marker_exists\n",
        "    \n",
        "    # Patch the constant and optionally bypass buffer/marker checks\n",
        "    with patch.object(protocols_execute, 'TVL_TABLE_LAST_N_DAYS', BACKFILL_DAYS):\n",
        "        patches = []\n",
        "        \n",
        "        # Force all protocols to be considered \"pending\" (not buffered)\n",
        "        if FORCE_COMPLETE_OVERWRITE:\n",
        "            patches.append(patch.object(buffereval, 'get_buffered', mock_get_buffered))\n",
        "            \n",
        "        # Bypass existing markers\n",
        "        if CLEAR_EXISTING_MARKERS:\n",
        "            patches.append(patch.object(dataaccess.PartitionedDataAccess, 'marker_exists', mock_marker_exists))\n",
        "        \n",
        "        # Apply all patches and run\n",
        "        if patches:\n",
        "            with patches[0]:\n",
        "                if len(patches) > 1:\n",
        "                    with patches[1]:\n",
        "                        result = protocols_execute.execute_pull()\n",
        "                else:\n",
        "                    result = protocols_execute.execute_pull()\n",
        "        else:\n",
        "            result = protocols_execute.execute_pull()\n",
        "        \n",
        "print(\"Protocols TVL backfill completed!\")\n",
        "print(f\"Result summary: {result}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chain TVL Backfill\n",
        "print(\"Starting Chain TVL backfill...\")\n",
        "print(f\"  Force overwrite: {FORCE_COMPLETE_OVERWRITE}\")\n",
        "\n",
        "def mock_marker_exists(*args, **kwargs):\n",
        "    \"\"\"Mock function to always return False, forcing overwrite.\"\"\"\n",
        "    if CLEAR_EXISTING_MARKERS:\n",
        "        return False\n",
        "    return original_marker_exists(*args, **kwargs)\n",
        "\n",
        "with patch.object(dailydatawrite, \"determine_location\", mock_location):\n",
        "    # Import required modules\n",
        "    from op_analytics.datasources.defillama.chaintvl import execute as chain_execute\n",
        "    from op_analytics.coreutils.partitioned import dataaccess\n",
        "    \n",
        "    # Store original function\n",
        "    original_marker_exists = dataaccess.PartitionedDataAccess.marker_exists\n",
        "    \n",
        "    # Patch the constant and optionally bypass markers\n",
        "    with patch.object(chain_execute, 'TVL_TABLE_LAST_N_DAYS', BACKFILL_DAYS):\n",
        "        if CLEAR_EXISTING_MARKERS:\n",
        "            with patch.object(dataaccess.PartitionedDataAccess, 'marker_exists', mock_marker_exists):\n",
        "                result = chain_execute.execute_pull()\n",
        "        else:\n",
        "            result = chain_execute.execute_pull()\n",
        "        \n",
        "print(\"Chain TVL backfill completed!\")\n",
        "print(f\"Result summary: {result}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assume 'result' is a list of dicts or DataFrame-like structure with 'dt' and 'protocol' fields\n",
        "# Convert result to DataFrame if not already\n",
        "if not isinstance(result, pd.DataFrame):\n",
        "    df = pd.DataFrame(result)\n",
        "else:\n",
        "    df = result\n",
        "\n",
        "# Ensure 'dt' is datetime for proper sorting/plotting\n",
        "df['dt'] = pd.to_datetime(df['dt'])\n",
        "\n",
        "# Group by day and count distinct protocols\n",
        "protocols_by_day = df.groupby('dt')['protocol'].nunique().reset_index()\n",
        "protocols_by_day = protocols_by_day.sort_values('dt')\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(protocols_by_day['dt'], protocols_by_day['protocol'], marker='o')\n",
        "plt.title('Distinct Protocols by Day')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Distinct Protocols')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Chain TVL Data\n",
        "\n",
        "This datasource pulls historical TVL data aggregated by blockchain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Stablecoins Data\n",
        "\n",
        "This datasource pulls stablecoin circulation and bridging data by chain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stablecoins Backfill\n",
        "print(\"Starting Stablecoins backfill...\")\n",
        "print(f\"  Force overwrite: {FORCE_COMPLETE_OVERWRITE}\")\n",
        "\n",
        "def mock_marker_exists(*args, **kwargs):\n",
        "    \"\"\"Mock function to always return False, forcing overwrite.\"\"\"\n",
        "    if CLEAR_EXISTING_MARKERS:\n",
        "        return False\n",
        "    return original_marker_exists(*args, **kwargs)\n",
        "\n",
        "with patch.object(dailydatawrite, \"determine_location\", mock_location):\n",
        "    # Import required modules\n",
        "    from op_analytics.datasources.defillama.stablecoins import execute as stablecoins_execute\n",
        "    from op_analytics.coreutils.partitioned import dataaccess\n",
        "    \n",
        "    # Store original function\n",
        "    original_marker_exists = dataaccess.PartitionedDataAccess.marker_exists\n",
        "    \n",
        "    # Patch the constant and optionally bypass markers\n",
        "    with patch.object(stablecoins_execute, 'BALANCES_TABLE_LAST_N_DAYS', BACKFILL_DAYS):\n",
        "        if CLEAR_EXISTING_MARKERS:\n",
        "            with patch.object(dataaccess.PartitionedDataAccess, 'marker_exists', mock_marker_exists):\n",
        "                result = stablecoins_execute.execute_pull()\n",
        "        else:\n",
        "            result = stablecoins_execute.execute_pull()\n",
        "        \n",
        "print(\"Stablecoins backfill completed!\")\n",
        "print(f\"Result summary: {result}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Yield Pools Data\n",
        "\n",
        "This datasource pulls yield farming pool data and APY information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Yield Pools Backfill\n",
        "print(\"Starting Yield Pools backfill...\")\n",
        "print(f\"  Force overwrite: {FORCE_COMPLETE_OVERWRITE}\")\n",
        "\n",
        "def mock_marker_exists(*args, **kwargs):\n",
        "    \"\"\"Mock function to always return False, forcing overwrite.\"\"\"\n",
        "    if CLEAR_EXISTING_MARKERS:\n",
        "        return False\n",
        "    return original_marker_exists(*args, **kwargs)\n",
        "\n",
        "with patch.object(dailydatawrite, \"determine_location\", mock_location):\n",
        "    # Import required modules\n",
        "    from op_analytics.datasources.defillama.yieldpools import execute as yield_execute\n",
        "    from op_analytics.coreutils.partitioned import dataaccess\n",
        "    \n",
        "    # Store original function\n",
        "    original_marker_exists = dataaccess.PartitionedDataAccess.marker_exists\n",
        "    \n",
        "    # Patch the constant and optionally bypass markers\n",
        "    with patch.object(yield_execute, 'YIELD_TABLE_LAST_N_DAYS', BACKFILL_DAYS):\n",
        "        if CLEAR_EXISTING_MARKERS:\n",
        "            with patch.object(dataaccess.PartitionedDataAccess, 'marker_exists', mock_marker_exists):\n",
        "                result = yield_execute.execute_pull()\n",
        "        else:\n",
        "            result = yield_execute.execute_pull()\n",
        "        \n",
        "print(\"Yield Pools backfill completed!\")\n",
        "print(f\"Result summary: {result}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Force Overwrite Configuration\n",
        "\n",
        "### IMPORTANT: Ensure Complete Overwrites\n",
        "\n",
        "The notebook now includes **force overwrite** settings to ensure existing data gets completely overwritten during backfills:\n",
        "\n",
        "#### Configuration Variables:\n",
        "- **`FORCE_COMPLETE_OVERWRITE = True`** - Forces all datasources to refetch and overwrite existing data\n",
        "- **`CLEAR_EXISTING_MARKERS = True`** - Bypasses completion markers that would otherwise skip existing dates\n",
        "\n",
        "#### How It Works:\n",
        "\n",
        "1. **Volume/Fees/Revenue & Chain TVL**: Bypasses completion markers to ensure all dates are reprocessed\n",
        "2. **Protocols TVL**: Forces all protocols to be refetched by bypassing buffer checks AND completion markers\n",
        "3. **Other datasources**: Use similar bypassing mechanisms for their respective completion checks\n",
        "\n",
        "#### To Use:\n",
        "1. Set both flags to `True` in the configuration cell\n",
        "2. Run the specific datasource cells you want to backfill\n",
        "3. Monitor the output - you should see \"Force overwrite: True\" messages\n",
        "\n",
        "#### Performance Notes:\n",
        "- Force overwrite will significantly increase runtime as it processes ALL data\n",
        "- For 365+ day backfills, consider running one datasource at a time\n",
        "- Protocols TVL may take the longest due to API rate limits per protocol\n",
        "\n",
        "#### Verification:\n",
        "After running, check that the result summaries show data for the full date range you expected.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Lend/Borrow Pools Data\n",
        "\n",
        "This datasource pulls lending protocol data including rates and volumes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lend/Borrow Pools Backfill\n",
        "print(\"Starting Lend/Borrow Pools backfill...\")\n",
        "print(f\"  Force overwrite: {FORCE_COMPLETE_OVERWRITE}\")\n",
        "\n",
        "def mock_marker_exists(*args, **kwargs):\n",
        "    \"\"\"Mock function to always return False, forcing overwrite.\"\"\"\n",
        "    if CLEAR_EXISTING_MARKERS:\n",
        "        return False\n",
        "    return original_marker_exists(*args, **kwargs)\n",
        "\n",
        "with patch.object(dailydatawrite, \"determine_location\", mock_location):\n",
        "    # Import required modules\n",
        "    from op_analytics.datasources.defillama.lendborrowpools import execute as lb_execute\n",
        "    from op_analytics.coreutils.partitioned import dataaccess\n",
        "    \n",
        "    # Store original function\n",
        "    original_marker_exists = dataaccess.PartitionedDataAccess.marker_exists\n",
        "    \n",
        "    # Patch the constant and optionally bypass markers\n",
        "    with patch.object(lb_execute, 'LEND_BORROW_TABLE_LAST_N_DAYS', BACKFILL_DAYS):\n",
        "        if CLEAR_EXISTING_MARKERS:\n",
        "            with patch.object(dataaccess.PartitionedDataAccess, 'marker_exists', mock_marker_exists):\n",
        "                result = lb_execute.execute_pull()\n",
        "        else:\n",
        "            result = lb_execute.execute_pull()\n",
        "        \n",
        "print(\"Lend/Borrow Pools backfill completed!\")\n",
        "print(f\"Result summary: {result}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Advanced Usage Examples\n",
        "\n",
        "### Running Specific Datasources\n",
        "To run only specific datasources, comment out the others and modify the configuration:\n",
        "\n",
        "```python\n",
        "# Example: Only backfill VFR data for 90 days\n",
        "BACKFILL_DAYS = 90\n",
        "SPECIFIC_CHAIN = None  \n",
        "SPECIFIC_PROTOCOL = None\n",
        "```\n",
        "\n",
        "### Chain-Specific Backfills\n",
        "To backfill data for a specific chain:\n",
        "\n",
        "```python\n",
        "BACKFILL_DAYS = 365\n",
        "SPECIFIC_CHAIN = \"optimism\"  # or \"base\", \"arbitrum\", etc.\n",
        "SPECIFIC_PROTOCOL = None\n",
        "```\n",
        "\n",
        "### Protocol-Specific Backfills\n",
        "To backfill data for a specific protocol:\n",
        "\n",
        "```python\n",
        "BACKFILL_DAYS = 365\n",
        "SPECIFIC_CHAIN = None\n",
        "SPECIFIC_PROTOCOL = \"uniswap\"  # Use the protocol slug from DeFiLlama\n",
        "```\n",
        "\n",
        "### Tips for Large Backfills\n",
        "- For backfills > 180 days, consider running datasources individually\n",
        "- Monitor memory usage for very large backfills\n",
        "- The protocols TVL datasource may take the longest due to API rate limits\n",
        "- Some datasources may have daily API limits - check DeFiLlama API documentation\n",
        "\n",
        "### Validation\n",
        "After running backfills, you can validate the data using the summary returned by each execute function.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
