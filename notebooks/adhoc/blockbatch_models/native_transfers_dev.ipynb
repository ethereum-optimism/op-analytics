{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from op_analytics.coreutils.partitioned.location import DataLocation\n",
    "from op_analytics.coreutils.partitioned.reader import DataReader\n",
    "from op_analytics.datapipeline.etl.ingestion.reader.byblock import construct_readers_byblock\n",
    "from op_analytics.datapipeline.etl.ingestion.reader.request import BlockBatchRequest\n",
    "from op_analytics.datapipeline.models.compute.modelspec import ModelsDataSpec\n",
    "from op_analytics.datapipeline.models.compute.testutils import setup_execution_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"native_transfers\"\n",
    "\n",
    "\n",
    "# Prepare data raeders\n",
    "data_spec = ModelsDataSpec(models=[model_name],\n",
    "    root_path_prefix=\"blockbatch\")\n",
    "blockbatch_request = BlockBatchRequest.build(\n",
    "    chains=[\"base\"],\n",
    "    range_spec=\"@20250622:+1\",\n",
    "    root_paths_to_read=data_spec.input_root_paths,\n",
    ")\n",
    "readers: list[DataReader] = construct_readers_byblock(\n",
    "    blockbatch_request=blockbatch_request,\n",
    "    read_from=DataLocation.GCS,\n",
    ")\n",
    "\n",
    "\n",
    "# Show details for the batch we are processing.\n",
    "pprint(readers[0])\n",
    "\n",
    "# Set up execution context and get handles to model input args.\n",
    "# In subsequent cells you can use the model input args however you want.\n",
    "ctx, input_datasets, auxiliary_templates = setup_execution_context(\n",
    "    model_name=model_name,\n",
    "    data_reader=readers[0],  # use the first reader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug: Test template execution step by step\n",
    "# print(\"=== DEBUG TEMPLATE EXECUTION ===\")\n",
    "\n",
    "# try:\n",
    "#     # Try to execute the template\n",
    "#     native_transfers = auxiliary_templates[\"native_transfers\"].to_relation(\n",
    "#         duckdb_context=ctx,\n",
    "#         template_parameters={\n",
    "#             \"raw_traces\": traces_view,\n",
    "#         },\n",
    "#     )\n",
    "#     print(\"✅ Template executed successfully!\")\n",
    "#     print(f\"Native transfers relation: {native_transfers}\")\n",
    "    \n",
    "#     # Check if table was created\n",
    "#     tables = ctx.client.sql(\"SHOW TABLES\").df()\n",
    "#     print(f\"Tables after template execution:\")\n",
    "#     print(tables)\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Template execution failed: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug: Check what we have\n",
    "# print(\"=== DEBUG SETUP ===\")\n",
    "# print(f\"Input datasets: {list(input_datasets.keys())}\")\n",
    "# print(f\"Auxiliary templates: {list(auxiliary_templates.keys())}\")\n",
    "# print(f\"Context client: {ctx.client}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug: Check if traces data exists\n",
    "# print(\"=== DEBUG TRACES DATA ===\")\n",
    "# traces_view = input_datasets[\"ingestion/traces_v1\"].create_view()\n",
    "# print(f\"Traces view created: {traces_view}\")\n",
    "\n",
    "# # Check if traces have data\n",
    "# traces_count = ctx.client.sql(f\"SELECT COUNT(*) as count FROM {traces_view}\").pl().to_dicts()[0][\"count\"]\n",
    "# print(f\"Traces count: {traces_count}\")\n",
    "\n",
    "# # Check if traces have value_lossless\n",
    "# sample_traces = ctx.client.sql(f\"SELECT value_lossless FROM {traces_view} WHERE value_lossless IS NOT NULL AND value_lossless != '0' LIMIT 5\").df()\n",
    "# print(f\"Sample value_lossless values:\")\n",
    "# print(sample_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug: Test template execution step by step\n",
    "# print(\"=== DEBUG TEMPLATE EXECUTION ===\")\n",
    "\n",
    "# try:\n",
    "#     # Try to execute the template\n",
    "#     native_transfers = auxiliary_templates[\"native_transfers\"].to_relation(\n",
    "#         duckdb_context=ctx,\n",
    "#         template_parameters={\n",
    "#             \"raw_traces\": traces_view,\n",
    "#         },\n",
    "#     )\n",
    "#     print(\"✅ Template executed successfully!\")\n",
    "#     print(f\"Native transfers relation: {native_transfers}\")\n",
    "    \n",
    "#     # Check if table was created\n",
    "#     tables = ctx.client.sql(\"SHOW TABLES\").df()\n",
    "#     print(f\"Tables after template execution:\")\n",
    "#     print(tables)\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Template execution failed: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ctx.client.sql(\"SHOW TABLES\").df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_view = input_datasets[\"ingestion/traces_v1\"].create_view()\n",
    "\n",
    "native_transfers = auxiliary_templates[\"native_transfers\"].to_relation(\n",
    "    duckdb_context=ctx,\n",
    "    template_parameters={\n",
    "        \"raw_traces\": traces_view,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_transfers = (\n",
    "    native_transfers\n",
    "    .filter(\"transfer_type = 'native'\")\n",
    "    .project(\"*\")\n",
    ")\n",
    "df = ctx.client.sql(f\"SELECT * FROM native_transfers\").df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This matches the test's overall count query\n",
    "block_filter_sql = \"(block_number IN (128145990, 128145989) OR block_number % 100 < 2)\"\n",
    "\n",
    "num_native_transfers = ctx.client.sql(\n",
    "    f\"SELECT COUNT(*) as num_native_transfers FROM native_transfers WHERE {block_filter_sql}\"\n",
    ").pl().to_dicts()[0][\"num_native_transfers\"]\n",
    "print(\"num_native_transfers:\", num_native_transfers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ctx.client.sql(f\"SELECT MIN(block_number) as min_bn, MAX(block_number) as max_bn FROM native_transfers\").df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ctx.client.sql(f\"SELECT * FROM native_transfers WHERE block_number = 31880531 and transaction_hash = lower('0x1b85d1be582a90c2ae9682e0e1adf72f29e3aed609bbc57714f5676493716162') and to_address = lower('0x9c3631dDE5c8316bE5B7554B0CcD2631C15a9A05')\").df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = ctx.client.sql(f\"SELECT * FROM native_transfers WHERE block_number = 31880531\").df()\n",
    "# df.head()\n",
    "# df.to_csv(\"native_transfers_31880531.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
