{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data reader and model execution context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-01-29 21:40:18\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mconnecting to GOLDSKY Clickhouse client...\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m37\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m39167\u001b[0m\n",
      "\u001b[2m2025-01-29 21:40:18\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mloaded vault from .env file   \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mvault.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m32\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m39167\u001b[0m\n",
      "\u001b[2m2025-01-29 21:40:18\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mloaded vault: 18 items        \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mvault.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m79\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m39167\u001b[0m\n",
      "\u001b[2m2025-01-29 21:40:19\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized GOLDSKY Clickhouse client.\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m42\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m39167\u001b[0m\n",
      "\u001b[2m2025-01-29 21:40:19\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mconnecting to OPLABS Clickhouse client...\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m37\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m39167\u001b[0m\n",
      "\u001b[2m2025-01-29 21:40:19\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized OPLABS Clickhouse client.\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m42\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m39167\u001b[0m\n",
      "\u001b[2m2025-01-29 21:40:19\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mprepared 1 input batches.     \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mbyblock.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m88\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m39167\u001b[0m\n",
      "DataReader(partitions=Partition(cols=[PartitionColumn(name='chain',\n",
      "                                                      value='base'),\n",
      "                                      PartitionColumn(name='dt',\n",
      "                                                      value='2024-09-17')]),\n",
      "           read_from=DataLocation.GCS,\n",
      "           dataset_paths={'ingestion/logs_v1': ['gs://oplabs-tools-data-sink/ingestion/logs_v1/chain=base/dt=2024-09-17/000019910000.parquet'],\n",
      "                          'ingestion/traces_v1': ['gs://oplabs-tools-data-sink/ingestion/traces_v1/chain=base/dt=2024-09-17/000019910000.parquet']},\n",
      "           inputs_ready=True,\n",
      "           extra_marker_data={'max_block': 19912000,\n",
      "                              'min_block': 19910000,\n",
      "                              'num_blocks': 2000})\n",
      "\u001b[2m2025-01-29 21:40:19\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1minitialized duckdb at /var/folders/tz/g2vwn0qx5ll32j7jfsshhps40000gn/T/asyhsrfl/op-analytics.duck.db\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m94\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m39167\u001b[0m\n",
      "\u001b[2m2025-01-29 21:40:19\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mreading dataset='ingestion/logs_v1' using 1/1 parquet paths, first path is gs://oplabs-tools-data-sink/ingestion/logs_v1/chain=base/dt=2024-09-17/000019910000.parquet\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mreader.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m81\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m39167\u001b[0m\n",
      "\u001b[2m2025-01-29 21:40:19\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mreading dataset='ingestion/traces_v1' using 1/1 parquet paths, first path is gs://oplabs-tools-data-sink/ingestion/traces_v1/chain=base/dt=2024-09-17/000019910000.parquet\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mreader.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m81\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m39167\u001b[0m\n",
      "\n",
      "INPUT: ingestion/logs_v1\n",
      "INPUT: ingestion/traces_v1\n",
      "\n",
      "AUX VIEW: account_abstraction_prefilter/entrypoint_logs\n",
      "AUX VIEW: account_abstraction_prefilter/entrypoint_prefiltered_traces\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from op_analytics.coreutils.partitioned.location import DataLocation\n",
    "from op_analytics.coreutils.partitioned.reader import DataReader\n",
    "from op_analytics.datapipeline.etl.ingestion.reader.byblock import construct_readers_byblock\n",
    "from op_analytics.datapipeline.etl.ingestion.reader.request import BlockBatchRequest\n",
    "from op_analytics.datapipeline.models.compute.markers import ModelsDataSpec\n",
    "from op_analytics.datapipeline.models.compute.testutils import setup_execution_context\n",
    "\n",
    "model_name = \"account_abstraction_prefilter\"\n",
    "\n",
    "\n",
    "# Select a model.\n",
    "data_spec = ModelsDataSpec(root_path_prefix=\"blockbatch\", models=[model_name])\n",
    "\n",
    "# Select a block batch.\n",
    "blockbatch_request = BlockBatchRequest.build(\n",
    "    chains=[\"base\"],\n",
    "    # range_spec=\"19894001:+1\",\n",
    "    range_spec=\"19910194:+1\",\n",
    "    root_paths_to_read=data_spec.input_root_paths,\n",
    ")\n",
    "\n",
    "# Construct readers\n",
    "readers: list[DataReader] = construct_readers_byblock(\n",
    "    blockbatch_request=blockbatch_request,\n",
    "    read_from=DataLocation.GCS,\n",
    ")\n",
    "\n",
    "# Show details for the batch we are processing.\n",
    "pprint(readers[0])\n",
    "\n",
    "# Ensure existence of data needed by the reader.\n",
    "assert readers[0].inputs_ready\n",
    "\n",
    "# Set up execution context and get handles to model input args.\n",
    "# In subsequent cells you can use the model input args however you want.\n",
    "ctx, input_datasets, auxiliary_templates = setup_execution_context(\n",
    "    model_name=model_name,\n",
    "    data_reader=readers[0],  # use the first reader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-01-29 21:40:19\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mconstructed read_parquet() string with 1 paths\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m263\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m39167\u001b[0m\n",
      "\u001b[2m2025-01-29 21:40:19\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRendering query               \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mquerybuilder.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m40\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m39167\u001b[0m \u001b[36mtemplate\u001b[0m=\u001b[35maccount_abstraction_prefilter/entrypoint_logs\u001b[0m\n",
      "\u001b[2m2025-01-29 21:40:21\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mduck db size: 12.3KB          \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m36\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m39167\u001b[0m\n",
      "\u001b[2m2025-01-29 21:40:21\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mconstructed read_parquet() string with 1 paths\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m263\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m39167\u001b[0m\n",
      "\u001b[2m2025-01-29 21:40:21\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRendering query               \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mquerybuilder.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m40\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m39167\u001b[0m \u001b[36mtemplate\u001b[0m=\u001b[35maccount_abstraction_prefilter/entrypoint_prefiltered_traces\u001b[0m\n",
      "\u001b[2m2025-01-29 21:40:25\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mduck db size: 274.5MB         \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m36\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m39167\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from op_analytics.datapipeline.models.code.account_abstraction_prefilter.model import account_abstraction_prefilter\n",
    "\n",
    "\n",
    "results = account_abstraction_prefilter(ctx, input_datasets, auxiliary_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────────────────────────────────────────────────────┐\n",
       "│                             name                             │\n",
       "│                           varchar                            │\n",
       "├──────────────────────────────────────────────────────────────┤\n",
       "│ account_abstraction_prefilter__entrypoint_logs               │\n",
       "│ account_abstraction_prefilter__entrypoint_prefiltered_traces │\n",
       "│ txhashes                                                     │\n",
       "└──────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.client.sql(\"SHOW TABLES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────┬──────────┐\n",
       "│  table  │ num_rows │\n",
       "│ varchar │  int64   │\n",
       "├─────────┼──────────┤\n",
       "│ logs    │    19725 │\n",
       "│ traces  │   145824 │\n",
       "└─────────┴──────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTES:\n",
    "# \n",
    "# Block batch filtering \n",
    "#\n",
    "# Batch=19910000:\n",
    "#  logs   :  731998  ->  19725  (2.7%)\n",
    "#  traces : 3997893  -> 199594  (4.9%)\n",
    "#  txs    :  248980  ->   5882  (2.3%)\n",
    "#\n",
    "# Batch=19910000:\n",
    "#  logs   :  680683  ->  30251  (4.4%)\n",
    "#  traces : 4036203  -> 348751  (8.6%)  245413 if we filter traces with !=delegatecall\n",
    "\n",
    "ctx.client.sql(\"\"\"\n",
    "SELECT 'logs' AS table, count(*) as num_rows FROM account_abstraction_prefilter__entrypoint_logs\n",
    "UNION ALL\n",
    "SELECT 'traces' AS table, count(*) as num_rows FROM account_abstraction_prefilter__entrypoint_prefiltered_traces\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
