{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data reader and model execution context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-01-29 21:33:08\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mconnecting to GOLDSKY Clickhouse client...\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m37\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m\n",
      "\u001b[2m2025-01-29 21:33:08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mloaded vault from .env file   \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mvault.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m32\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m\n",
      "\u001b[2m2025-01-29 21:33:08\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mloaded vault: 18 items        \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mvault.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m79\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m\n",
      "\u001b[2m2025-01-29 21:33:09\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized GOLDSKY Clickhouse client.\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m42\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m\n",
      "\u001b[2m2025-01-29 21:33:09\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mconnecting to OPLABS Clickhouse client...\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m37\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m\n",
      "\u001b[2m2025-01-29 21:33:09\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized OPLABS Clickhouse client.\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m42\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m\n",
      "\u001b[2m2025-01-29 21:33:09\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mprepared 1 input batches.     \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mbyblock.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m88\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m\n",
      "DataReader(partitions=Partition(cols=[PartitionColumn(name='chain',\n",
      "                                                      value='base'),\n",
      "                                      PartitionColumn(name='dt',\n",
      "                                                      value='2024-09-17')]),\n",
      "           read_from=DataLocation.GCS,\n",
      "           dataset_paths={'blockbatch/refined_traces/refined_transactions_fees_v1': ['gs://oplabs-tools-data-sink/blockbatch/refined_traces/refined_transactions_fees_v1/chain=base/dt=2024-09-17/000019910000.parquet'],\n",
      "                          'ingestion/logs_v1': ['gs://oplabs-tools-data-sink/ingestion/logs_v1/chain=base/dt=2024-09-17/000019910000.parquet'],\n",
      "                          'ingestion/traces_v1': ['gs://oplabs-tools-data-sink/ingestion/traces_v1/chain=base/dt=2024-09-17/000019910000.parquet']},\n",
      "           inputs_ready=True,\n",
      "           extra_marker_data={'max_block': 19912000,\n",
      "                              'min_block': 19910000,\n",
      "                              'num_blocks': 2000})\n",
      "\u001b[2m2025-01-29 21:33:09\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1minitialized duckdb at /var/folders/tz/g2vwn0qx5ll32j7jfsshhps40000gn/T/rw35ojb0/op-analytics.duck.db\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m94\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m\n",
      "\u001b[2m2025-01-29 21:33:09\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mreading dataset='ingestion/logs_v1' using 1/1 parquet paths, first path is gs://oplabs-tools-data-sink/ingestion/logs_v1/chain=base/dt=2024-09-17/000019910000.parquet\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mreader.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m81\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m\n",
      "\u001b[2m2025-01-29 21:33:09\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mreading dataset='ingestion/traces_v1' using 1/1 parquet paths, first path is gs://oplabs-tools-data-sink/ingestion/traces_v1/chain=base/dt=2024-09-17/000019910000.parquet\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mreader.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m81\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m\n",
      "\u001b[2m2025-01-29 21:33:09\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mreading dataset='blockbatch/refined_traces/refined_transactions_fees_v1' using 1/1 parquet paths, first path is gs://oplabs-tools-data-sink/blockbatch/refined_traces/refined_transactions_fees_v1/chain=base/dt=2024-09-17/000019910000.parquet\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mreader.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m81\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m\n",
      "\n",
      "INPUT: ingestion/logs_v1\n",
      "INPUT: ingestion/traces_v1\n",
      "INPUT: blockbatch/refined_traces/refined_transactions_fees_v1\n",
      "\n",
      "AUX VIEW: account_abstraction_prefilter/entrypoint_logs\n",
      "AUX VIEW: account_abstraction_prefilter/entrypoint_prefiltered_traces\n",
      "AUX VIEW: account_abstraction_prefilter/entrypoint_prefiltered_txs\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from op_analytics.coreutils.partitioned.location import DataLocation\n",
    "from op_analytics.coreutils.partitioned.reader import DataReader\n",
    "from op_analytics.datapipeline.etl.ingestion.reader.byblock import construct_readers_byblock\n",
    "from op_analytics.datapipeline.etl.ingestion.reader.request import BlockBatchRequest\n",
    "from op_analytics.datapipeline.models.compute.markers import ModelsDataSpec\n",
    "from op_analytics.datapipeline.models.compute.testutils import setup_execution_context\n",
    "\n",
    "model_name = \"account_abstraction_prefilter\"\n",
    "\n",
    "\n",
    "# Select a model.\n",
    "data_spec = ModelsDataSpec(root_path_prefix=\"blockbatch\", models=[model_name])\n",
    "\n",
    "# Select a block batch.\n",
    "blockbatch_request = BlockBatchRequest.build(\n",
    "    chains=[\"base\"],\n",
    "    # range_spec=\"19894001:+1\",\n",
    "    range_spec=\"19910194:+1\",\n",
    "    root_paths_to_read=data_spec.input_root_paths,\n",
    ")\n",
    "\n",
    "# Construct readers\n",
    "readers: list[DataReader] = construct_readers_byblock(\n",
    "    blockbatch_request=blockbatch_request,\n",
    "    read_from=DataLocation.GCS,\n",
    ")\n",
    "\n",
    "# Show details for the batch we are processing.\n",
    "pprint(readers[0])\n",
    "\n",
    "# Ensure existence of data needed by the reader.\n",
    "assert readers[0].inputs_ready\n",
    "\n",
    "# Set up execution context and get handles to model input args.\n",
    "# In subsequent cells you can use the model input args however you want.\n",
    "ctx, input_datasets, auxiliary_templates = setup_execution_context(\n",
    "    model_name=model_name,\n",
    "    data_reader=readers[0],  # use the first reader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-01-29 21:33:09\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mconstructed read_parquet() string with 1 paths\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m263\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m\n",
      "\u001b[2m2025-01-29 21:33:09\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRendering query               \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mquerybuilder.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m40\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m \u001b[36mtemplate\u001b[0m=\u001b[35maccount_abstraction_prefilter/entrypoint_logs\u001b[0m\n",
      "\u001b[2m2025-01-29 21:33:13\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mduck db size: 12.3KB          \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m36\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m\n",
      "\u001b[2m2025-01-29 21:33:13\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mconstructed read_parquet() string with 1 paths\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m263\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m\n",
      "\u001b[2m2025-01-29 21:33:13\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRendering query               \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mquerybuilder.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m40\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m \u001b[36mtemplate\u001b[0m=\u001b[35maccount_abstraction_prefilter/entrypoint_prefiltered_txs\u001b[0m\n",
      "\u001b[2m2025-01-29 21:33:22\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mduck db size: 12.3KB          \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m36\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m37785\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'entrypoint_traces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mop_analytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccount_abstraction_prefilter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m account_abstraction_prefilter\n\u001b[0;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43maccount_abstraction_prefilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauxiliary_templates\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/ethereum-optimism/op-analytics-alt/src/op_analytics/datapipeline/models/code/account_abstraction_prefilter/model.py:73\u001b[0m, in \u001b[0;36maccount_abstraction_prefilter\u001b[0;34m(ctx, input_datasets, auxiliary_templates)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Prefiltered traces.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# entrypoint_traces = auxiliary_templates[\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#     \"account_abstraction_prefilter/entrypoint_prefiltered_traces\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Prefiltered traces.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m entrypoint_txs \u001b[38;5;241m=\u001b[39m auxiliary_templates[\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccount_abstraction_prefilter/entrypoint_prefiltered_txs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m ]\u001b[38;5;241m.\u001b[39mcreate_table(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m     },\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentrypoint_logs_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m: entrypoint_logs,\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentrypoint_traces_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mentrypoint_traces\u001b[49m,\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentrypoint_refined_txs_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m: entrypoint_txs,\n\u001b[1;32m     75\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'entrypoint_traces' is not defined"
     ]
    }
   ],
   "source": [
    "from op_analytics.datapipeline.models.code.account_abstraction_prefilter.model import account_abstraction_prefilter\n",
    "\n",
    "\n",
    "results = account_abstraction_prefilter(ctx, input_datasets, auxiliary_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────────────────────────────────────────────────────┐\n",
       "│                             name                             │\n",
       "│                           varchar                            │\n",
       "├──────────────────────────────────────────────────────────────┤\n",
       "│ account_abstraction_prefilter__entrypoint_logs               │\n",
       "│ account_abstraction_prefilter__entrypoint_prefiltered_traces │\n",
       "│ account_abstraction_prefilter__entrypoint_prefiltered_txs    │\n",
       "│ txhashes                                                     │\n",
       "└──────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.client.sql(\"SHOW TABLES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────┬──────────┐\n",
       "│  table  │ num_rows │\n",
       "│ varchar │  int64   │\n",
       "├─────────┼──────────┤\n",
       "│ logs    │    19725 │\n",
       "│ traces  │   145824 │\n",
       "│ txs     │     5882 │\n",
       "└─────────┴──────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTES:\n",
    "# \n",
    "# Block batch filtering \n",
    "#\n",
    "# Batch=19910000:\n",
    "#  logs   :  731998  ->  19725  (2.7%)\n",
    "#  traces : 3997893  -> 199594  (4.9%)\n",
    "#  txs    :  248980  ->   5882  (2.3%)\n",
    "#\n",
    "# Batch=19910000:\n",
    "#  logs   :  680683  ->  30251  (4.4%)\n",
    "#  traces : 4036203  -> 348751  (8.6%)  245413 if we filter traces with !=delegatecall\n",
    "\n",
    "ctx.client.sql(\"\"\"\n",
    "SELECT 'logs' AS table, count(*) as num_rows FROM account_abstraction_prefilter__entrypoint_logs\n",
    "UNION ALL\n",
    "SELECT 'traces' AS table, count(*) as num_rows FROM account_abstraction_prefilter__entrypoint_prefiltered_traces\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
