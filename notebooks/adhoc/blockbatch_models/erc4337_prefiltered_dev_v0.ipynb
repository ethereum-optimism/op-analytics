{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data reader and model execution context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-01-29 15:15:47\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mprepared 1 input batches.     \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mbyblock.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m88\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m77753\u001b[0m\n",
      "DataReader(partitions=Partition(cols=[PartitionColumn(name='chain',\n",
      "                                                      value='base'),\n",
      "                                      PartitionColumn(name='dt',\n",
      "                                                      value='2024-09-17')]),\n",
      "           read_from=DataLocation.GCS,\n",
      "           dataset_paths={'ingestion/logs_v1': ['gs://oplabs-tools-data-sink/ingestion/logs_v1/chain=base/dt=2024-09-17/000019894000.parquet'],\n",
      "                          'ingestion/traces_v1': ['gs://oplabs-tools-data-sink/ingestion/traces_v1/chain=base/dt=2024-09-17/000019894000.parquet']},\n",
      "           inputs_ready=True,\n",
      "           extra_marker_data={'max_block': 19896000,\n",
      "                              'min_block': 19894000,\n",
      "                              'num_blocks': 2000})\n",
      "\u001b[2m2025-01-29 15:15:47\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mreading dataset='ingestion/logs_v1' using 1/1 parquet paths, first path is gs://oplabs-tools-data-sink/ingestion/logs_v1/chain=base/dt=2024-09-17/000019894000.parquet\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mreader.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m81\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m77753\u001b[0m\n",
      "\u001b[2m2025-01-29 15:15:47\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mreading dataset='ingestion/traces_v1' using 1/1 parquet paths, first path is gs://oplabs-tools-data-sink/ingestion/traces_v1/chain=base/dt=2024-09-17/000019894000.parquet\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mreader.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m81\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m77753\u001b[0m\n",
      "\n",
      "INPUT: ingestion/logs_v1\n",
      "INPUT: ingestion/traces_v1\n",
      "\n",
      "AUX VIEW: account_abstraction_prefilter/entrypoint_logs\n",
      "AUX VIEW: account_abstraction_prefilter/entrypoint_prefiltered_traces\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from op_analytics.coreutils.partitioned.location import DataLocation\n",
    "from op_analytics.coreutils.partitioned.reader import DataReader\n",
    "from op_analytics.datapipeline.etl.ingestion.reader.byblock import construct_readers_byblock\n",
    "from op_analytics.datapipeline.etl.ingestion.reader.request import BlockBatchRequest\n",
    "from op_analytics.datapipeline.models.compute.markers import ModelsDataSpec\n",
    "from op_analytics.datapipeline.models.compute.testutils import setup_execution_context\n",
    "\n",
    "model_name = \"account_abstraction_prefilter\"\n",
    "\n",
    "\n",
    "# Select a model.\n",
    "data_spec = ModelsDataSpec(root_path_prefix=\"blockbatch\", models=[model_name])\n",
    "\n",
    "# Select a block batch.\n",
    "blockbatch_request = BlockBatchRequest.build(\n",
    "    chains=[\"base\"],\n",
    "    range_spec=\"19894001:+1\",\n",
    "    # range_spec=\"19910194:+1\",\n",
    "    root_paths_to_read=data_spec.input_root_paths,\n",
    ")\n",
    "\n",
    "# Construct readers\n",
    "readers: list[DataReader] = construct_readers_byblock(\n",
    "    blockbatch_request=blockbatch_request,\n",
    "    read_from=DataLocation.GCS,\n",
    ")\n",
    "\n",
    "# Show details for the batch we are processing.\n",
    "pprint(readers[0])\n",
    "\n",
    "# Ensure existence of data needed by the reader.\n",
    "assert readers[0].inputs_ready\n",
    "\n",
    "# Set up execution context and get handles to model input args.\n",
    "# In subsequent cells you can use the model input args however you want.\n",
    "ctx, input_datasets, auxiliary_templates = setup_execution_context(\n",
    "    model_name=model_name,\n",
    "    data_reader=readers[0],  # use the first reader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-01-29 16:17:42\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mconstructed read_parquet() string with 1 paths\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m263\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m77753\u001b[0m\n",
      "\u001b[2m2025-01-29 16:17:42\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRendering query               \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mquerybuilder.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m40\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m77753\u001b[0m \u001b[36mtemplate\u001b[0m=\u001b[35maccount_abstraction_prefilter/entrypoint_logs\u001b[0m\n",
      "\u001b[2m2025-01-29 16:17:44\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mduck db size: 74.2MB          \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m36\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m77753\u001b[0m\n",
      "\u001b[2m2025-01-29 16:17:44\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mconstructed read_parquet() string with 1 paths\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m263\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m77753\u001b[0m\n",
      "\u001b[2m2025-01-29 16:17:44\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRendering query               \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mquerybuilder.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m40\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m77753\u001b[0m \u001b[36mtemplate\u001b[0m=\u001b[35maccount_abstraction_prefilter/entrypoint_prefiltered_traces\u001b[0m\n",
      "\u001b[2m2025-01-29 16:17:47\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mduck db size: 514.6MB         \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m36\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m77753\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# EntryPoint logs.\n",
    "entrypoint_logs = auxiliary_templates[\"account_abstraction_prefilter/entrypoint_logs\"].create_table(\n",
    "    duckdb_context=ctx,\n",
    "    template_parameters={\n",
    "        \"raw_logs\": input_datasets[\"ingestion/logs_v1\"].as_subquery(),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Table with EntryPoint transaction hashes. Used to filter the raw traces.\n",
    "ctx.client.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE txhashes AS\n",
    "SELECT DISTINCT transaction_hash FROM {entrypoint_logs}\n",
    "ORDER BY transaction_hash\n",
    "\"\"\")\n",
    "\n",
    "from op_analytics.datapipeline.models.code.account_abstraction.abis import (\n",
    "    INNER_HANDLE_OP_FUNCTION_METHOD_ID_v0_6_0,\n",
    "    INNER_HANDLE_OP_FUNCTION_METHOD_ID_v0_7_0,\n",
    ")\n",
    "\n",
    "# Prefiltered traces.\n",
    "entrypoint_traces = auxiliary_templates[\n",
    "    \"account_abstraction_prefilter/entrypoint_prefiltered_traces\"\n",
    "].create_table(\n",
    "    duckdb_context=ctx,\n",
    "    template_parameters={\n",
    "        \"raw_traces\": input_datasets[\"ingestion/traces_v1\"].as_subquery(),\n",
    "        \"entrypoint_txhashes\": \"txhashes\",\n",
    "        \"inner_handle_op_method_ids\": \", \".join(\n",
    "            [\n",
    "                f\"'{INNER_HANDLE_OP_FUNCTION_METHOD_ID_v0_6_0}'\",\n",
    "                f\"'{INNER_HANDLE_OP_FUNCTION_METHOD_ID_v0_7_0}'\",\n",
    "            ]\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────────────────────────────────────────────────────┐\n",
       "│                             name                             │\n",
       "│                           varchar                            │\n",
       "├──────────────────────────────────────────────────────────────┤\n",
       "│ account_abstraction_prefilter__entrypoint_logs               │\n",
       "│ account_abstraction_prefilter__entrypoint_prefiltered_traces │\n",
       "│ txhashes                                                     │\n",
       "└──────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx.client.sql(\"SHOW TABLES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────┬──────────┐\n",
       "│  table  │ num_rows │\n",
       "│ varchar │  int64   │\n",
       "├─────────┼──────────┤\n",
       "│ logs    │    30251 │\n",
       "│ traces  │   245413 │\n",
       "└─────────┴──────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTES:\n",
    "# \n",
    "# Block batch filtering \n",
    "#\n",
    "# Batch=19910000:\n",
    "#  logs   :  731998  ->  19725  (2.7%)\n",
    "#  traces : 3997893  -> 199594  (4.9%)\n",
    "#\n",
    "# Batch=19910000:\n",
    "#  logs   :  680683  ->  30251  (4.4%)\n",
    "#  traces : 4036203  -> 348751  (8.6%)  245413 if we filter traces with !=delegatecall\n",
    "\n",
    "ctx.client.sql(\"\"\"\n",
    "SELECT 'logs' AS table, count(*) as num_rows FROM account_abstraction_prefilter__entrypoint_logs\n",
    "UNION ALL\n",
    "SELECT 'traces' AS table, count(*) as num_rows FROM account_abstraction_prefilter__entrypoint_prefiltered_traces\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
