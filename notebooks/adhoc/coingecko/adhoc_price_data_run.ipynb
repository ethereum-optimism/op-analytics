{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import requests\n",
    "\n",
    "# Third-party imports\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "# Local imports\n",
    "from op_analytics.datapipeline.chains.load import load_chain_metadata\n",
    "from op_analytics.datasources.coingecko.price_data import CoinGeckoDataSource\n",
    "from op_analytics.datasources.coingecko.dataaccess import CoinGecko\n",
    "from op_analytics.coreutils.request import new_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load chain metadata to get token IDs\n",
    "from op_analytics.datapipeline.chains.load import load_chain_metadata\n",
    "\n",
    "# Load chain metadata\n",
    "chain_metadata = load_chain_metadata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique non-null CoinGecko API keys\n",
    "token_ids = (\n",
    "    chain_metadata.filter(pl.col(\"cgt_coingecko_api\").is_not_null())\n",
    "    .select(\"cgt_coingecko_api\")\n",
    "    .unique()\n",
    "    .to_series()\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "print(f\"Found {len(token_ids)} unique tokens with CoinGecko API keys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metadata pull with contract address exploration\n",
    "print(f\"Fetching metadata for {len(token_ids)} tokens...\")\n",
    "\n",
    "# Initialize data source if not already done\n",
    "if 'data_source' not in locals():\n",
    "    session = new_session()\n",
    "    data_source = CoinGeckoDataSource(session=session)\n",
    "\n",
    "# Fetch metadata for all tokens\n",
    "metadata_df = data_source.get_token_metadata(token_ids)\n",
    "\n",
    "# Add dt partition column\n",
    "from op_analytics.coreutils.time import now_dt\n",
    "metadata_df = metadata_df.with_columns(dt=pl.lit(now_dt()))\n",
    "\n",
    "print(f\"Successfully fetched metadata for {len(metadata_df)} tokens\")\n",
    "\n",
    "# Explore contract addresses structure\n",
    "if 'contract_addresses' in metadata_df.columns:\n",
    "    print(\"\\n=== CONTRACT ADDRESSES EXPLORATION ===\")\n",
    "    \n",
    "    # Get a sample token with contract addresses\n",
    "    sample_row = metadata_df.filter(pl.col('contract_addresses').is_not_null()).head(1)\n",
    "    if len(sample_row) > 0:\n",
    "        token_id = sample_row['token_id'][0]\n",
    "        contract_data = sample_row['contract_addresses'][0]\n",
    "        \n",
    "        print(f\"\\nSample token: {token_id}\")\n",
    "        print(f\"Contract addresses data type: {type(contract_data)}\")\n",
    "        \n",
    "        if isinstance(contract_data, str):\n",
    "            import json\n",
    "            try:\n",
    "                contract_dict = json.loads(contract_data)\n",
    "                print(f\"Parsed contract addresses:\")\n",
    "                for platform, address in contract_dict.items():\n",
    "                    print(f\"  {platform}: {address}\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Raw contract data: {contract_data}\")\n",
    "        elif isinstance(contract_data, dict):\n",
    "            print(f\"Contract addresses:\")\n",
    "            for platform, address in contract_data.items():\n",
    "                print(f\"  {platform}: {address}\")\n",
    "    \n",
    "    # Show all unique platforms/chains across all tokens\n",
    "    print(f\"\\n=== ALL PLATFORMS/CHAINS FOUND ===\")\n",
    "    all_platforms = set()\n",
    "    \n",
    "    for contract_data in metadata_df['contract_addresses'].drop_nulls():\n",
    "        if isinstance(contract_data, str):\n",
    "            import json\n",
    "            try:\n",
    "                contract_dict = json.loads(contract_data)\n",
    "                all_platforms.update(contract_dict.keys())\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "        elif isinstance(contract_data, dict):\n",
    "            all_platforms.update(contract_data.keys())\n",
    "    \n",
    "    print(f\"Found {len(all_platforms)} unique platforms/chains:\")\n",
    "    for platform in sorted(all_platforms):\n",
    "        print(f\"  - {platform}\")\n",
    "\n",
    "print(\"\\nSample metadata:\")\n",
    "print(metadata_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metadata for Celo token\n",
    "print(\"\\n=== CELO TOKEN METADATA ===\")\n",
    "celo_metadata = metadata_df.filter(pl.col(\"token_id\") == \"celo\")\n",
    "print(celo_metadata)\n",
    "\n",
    "celo_contracts = celo_metadata.select(\"contract_addresses\")[0,0]\n",
    "print(\"\\n=== CELO CONTRACT ADDRESSES ===\")\n",
    "if isinstance(celo_contracts, str):\n",
    "    # Print raw contract addresses data for Celo token\n",
    "    try:\n",
    "        contract_dict = json.loads(celo_contracts)\n",
    "        for platform, address in contract_dict.items():\n",
    "            print(f\"  {platform}: {address}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"  Raw data: {celo_contracts}\")\n",
    "elif isinstance(celo_contracts, dict):\n",
    "    for platform, address in celo_contracts.items():\n",
    "        print(f\"  {platform}: {address}\")\n",
    "print(\"\\nRaw contract_addresses data:\")\n",
    "print(celo_contracts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall run\n",
    "from op_analytics.datasources.coingecko.execute import execute_pull, execute_metadata_pull\n",
    "from op_analytics.coreutils.partitioned.dailydatawrite import write_to_prod\n",
    "\n",
    "# Path to your config file\n",
    "extra_token_ids_file = \"../../../src/op_analytics/datasources/coingecko/config/extra_token_ids.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with write_to_prod():\n",
    "    # Run the full pipeline, including extra tokens\n",
    "    result = execute_pull(days=365, extra_token_ids_file=extra_token_ids_file, include_top_tokens=25, fetch_metadata=True)\n",
    "    # result = execute_pull(days=365, fetch_metadata=True)\n",
    "    #Metadata Only\n",
    "    # result = execute_metadata_pull(extra_token_ids_file=extra_token_ids_file, include_top_tokens=25)\n",
    "    # result = execute_metadata_pull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = execute_pull(days=365, extra_token_ids_file=extra_token_ids_file, include_top_tokens=25, fetch_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
