{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Jovian Analysis - Consolidated Notebook\n",
        "\n",
        "**Unified notebook for Jovian calldata footprint gas scalar analysis**\n",
        "\n",
        "## Features\n",
        "- ‚úÖ **Multi-chain support**: Base, OP Mainnet etc.\n",
        "- ‚úÖ **Flexible sampling**: Top percentile or random sampling\n",
        "- ‚úÖ **Date range analysis**: Analyze multiple dates with per-date breakdown\n",
        "- ‚úÖ **Dynamic gas limits**: Uses per-chain historical CSV data\n",
        "- ‚úÖ **Compression analysis**: FastLZ compression ratios and DA efficiency\n",
        "- ‚úÖ **Enhanced visualizations**: Comprehensive charts with recommendations\n",
        "- ‚úÖ **Caching system**: Local parquet files for performance\n",
        "- ‚úÖ **Optimal recommendations**: Data-driven scalar suggestions\n",
        "\n",
        "## Quick Start\n",
        "1. **Configure parameters** in the cell below\n",
        "2. **Run all cells** to perform analysis\n",
        "3. **Review results** in the generated visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Modules loaded successfully\n",
            "‚úÖ Available scalars: [160, 400, 600, 800]\n"
          ]
        }
      ],
      "source": [
        "# Core imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import polars as pl\n",
        "from datetime import datetime, timedelta\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.append(str(Path.cwd().parent))\n",
        "\n",
        "# Import Jovian modules\n",
        "from jovian_src.chain_config import (\n",
        "    get_chain_display_name\n",
        ")\n",
        "from jovian_src.constants import (\n",
        "    DEFAULT_CALLDATA_FOOTPRINT_GAS_SCALARS\n",
        ")\n",
        "from jovian_src.clickhouse_fetcher import (\n",
        "    load_gas_limits,\n",
        "    fetch_top_percentile_blocks,\n",
        "    fetch_random_sample_blocks,\n",
        "    get_gas_limit_for_date\n",
        ")\n",
        "from jovian_src.analysis_functions import (\n",
        "    perform_jovian_analysis,\n",
        "    generate_jovian_recommendation\n",
        ")\n",
        "from jovian_src.visualization_jovian import (\n",
        "    generate_all_visualizations,\n",
        "    plot_block_metric_distribution\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modules loaded successfully\")\n",
        "print(f\"‚úÖ Available scalars: {DEFAULT_CALLDATA_FOOTPRINT_GAS_SCALARS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configuration\n",
        "\n",
        "**Edit these parameters to customize your analysis:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuration loaded:\n",
            "   Chain: op\n",
            "   Method: random\n",
            "   Sample: 1% random\n",
            "   Date range: 2025-07-01 to 2025-08-26\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ANALYSIS CONFIGURATION - EDIT THESE PARAMETERS\n",
        "# ============================================================================\n",
        "\n",
        "# Chain selection\n",
        "CHAIN = \"op\"  # Options: base, op\n",
        "\n",
        "# Sampling method\n",
        "SAMPLING_METHOD = \"random\"  # \"top_percentile\" or \"random\"\n",
        "PERCENTILE = 99.0    # For top percentile (99 = top 1%)\n",
        "NUM_BLOCKS = 100     # For random sampling - blocks per day\n",
        "RANDOM_SEED = 42     # For reproducible random sampling\n",
        "SAMPLE_FRACTION = 0.01  # For random sampling (1% = 0.01)\n",
        "\n",
        "# Date range\n",
        "START_DATE = \"2025-07-01\"\n",
        "END_DATE = \"2025-08-26\"  # Inclusive\n",
        "\n",
        "# Analysis parameters\n",
        "CALLDATA_FOOTPRINT_GAS_SCALARS = DEFAULT_CALLDATA_FOOTPRINT_GAS_SCALARS  # [160, 400, 600, 800]\n",
        "BLOCK_LIMIT = None   # None = all blocks for selected method\n",
        "FORCE_REFRESH = False  # Set True to ignore cache and re-download\n",
        "\n",
        "# Output settings\n",
        "SAVE_RESULTS = True\n",
        "SHOW_PLOTS = True\n",
        "\n",
        "print(\"‚úÖ Configuration loaded:\")\n",
        "print(f\"   Chain: {CHAIN}\")\n",
        "print(f\"   Method: {SAMPLING_METHOD}\")\n",
        "if SAMPLING_METHOD == \"top_percentile\":\n",
        "    print(f\"   Percentile: Top {100-PERCENTILE}%\")\n",
        "else:\n",
        "    print(f\"   Sample: {int(SAMPLE_FRACTION*100)}% random\")\n",
        "print(f\"   Date range: {START_DATE} to {END_DATE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Initialize Analysis Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Output directory: ../results/op/jovian_analysis_random_2025-07-01_2025-08-26\n",
            "üìä Loading gas limits for op...\n",
            "‚úÖ Loaded gas limits for 0 dates\n",
            "\n",
            "üìä Consistent gas limit across all dates: 40,000,000\n",
            "üîß Cache method: rand_frac0.01_seed42\n",
            "üìÖ Dates to analyze: 57\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ENVIRONMENT SETUP\n",
        "# ============================================================================\n",
        "\n",
        "# Paths\n",
        "DATA_PATH = Path(\"../gas_limits\")\n",
        "FILE_PATH = f\"{CHAIN}_gas_limits.csv\"\n",
        "CACHE_DIR = Path(\".cache\")\n",
        "OUTPUT_DIR = Path(f\"../results/{CHAIN}/jovian_analysis_{SAMPLING_METHOD}_{START_DATE}_{END_DATE}\")\n",
        "\n",
        "# Create output directory\n",
        "if SAVE_RESULTS:\n",
        "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"üìÅ Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# Generate date list\n",
        "start = datetime.strptime(START_DATE, \"%Y-%m-%d\")\n",
        "end = datetime.strptime(END_DATE, \"%Y-%m-%d\")\n",
        "DATES_TO_ANALYZE = [(start + timedelta(days=i)).strftime(\"%Y-%m-%d\") \n",
        "                     for i in range((end - start).days + 1)]\n",
        "\n",
        "# Load gas limits\n",
        "date_gas_limits = {}\n",
        "print(f\"üìä Loading gas limits for {CHAIN}...\")\n",
        "gas_limits_df = load_gas_limits(csv_path=f\"{DATA_PATH}/{FILE_PATH}\")\n",
        "print(f\"‚úÖ Loaded gas limits for {len(date_gas_limits)} dates\")\n",
        "\n",
        "# Get gas limits for all dates\n",
        "\n",
        "for date in DATES_TO_ANALYZE:\n",
        "    gas_limit = get_gas_limit_for_date(date, gas_limits_df, chain=CHAIN)\n",
        "    date_gas_limits[date] = gas_limit\n",
        "    if len(DATES_TO_ANALYZE) <= 7:\n",
        "        print(f\"üìä Gas limit for {date}: {gas_limit:,}\")\n",
        "\n",
        "# Determine gas limit for combined analysis\n",
        "unique_limits = set(date_gas_limits.values())\n",
        "if len(unique_limits) > 1:\n",
        "    print(f\"\\n‚ö†Ô∏è  Multiple gas limits detected: {', '.join(f'{gl:,}' for gl in sorted(unique_limits))}\")\n",
        "    # Use the most common gas limit for combined analysis\n",
        "    limit_counts = Counter(date_gas_limits.values())\n",
        "    ANALYSIS_GAS_LIMIT = limit_counts.most_common(1)[0][0]\n",
        "    print(f\"üìä Using most common for combined analysis: {ANALYSIS_GAS_LIMIT:,}\")\n",
        "else:\n",
        "    ANALYSIS_GAS_LIMIT = list(unique_limits)[0]\n",
        "    print(f\"\\nüìä Consistent gas limit across all dates: {ANALYSIS_GAS_LIMIT:,}\")\n",
        "\n",
        "\n",
        "# Cache configuration\n",
        "def _method_key():\n",
        "    \"\"\"Generate cache key based on sampling method and parameters.\"\"\"\n",
        "    if SAMPLING_METHOD == \"top_percentile\":\n",
        "        return f\"top{PERCENTILE*100:.1f}pct_lim{BLOCK_LIMIT}\"\n",
        "    return f\"rand_frac{SAMPLE_FRACTION}_seed{RANDOM_SEED}\"\n",
        "\n",
        "def _cache_path(date_str: str) -> Path:\n",
        "    \"\"\"Generate cache file path for a specific date.\"\"\"\n",
        "    return CACHE_DIR / CHAIN / _method_key() / f\"{date_str}.parquet\"\n",
        "\n",
        "print(f\"üîß Cache method: {_method_key()}\")\n",
        "print(f\"üìÖ Dates to analyze: {len(DATES_TO_ANALYZE)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Data Fetching and Caching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Fetching data for op using random method...\n",
            "============================================================\n",
            "üìä Method: 1% random blocks\n",
            "\u001b[2m2025-08-27 12:02:47\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mloaded vault from .env file   \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mvault.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m32\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m65404\u001b[0m\n",
            "\u001b[2m2025-08-27 12:02:47\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mloaded vault: 28 items        \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mvault.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m79\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m65404\u001b[0m\n",
            "\u001b[2m2025-08-27 12:02:47\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mconnecting to OPLABS Clickhouse client...\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m56\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m65404\u001b[0m\n",
            "\u001b[2m2025-08-27 12:02:48\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized OPLABS Clickhouse client.\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m61\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m65404\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# DATA FETCHING WITH CACHING\n",
        "# ============================================================================\n",
        "\n",
        "print(f\"üì• Fetching data for {CHAIN} using {SAMPLING_METHOD} method...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_dataframes = []\n",
        "fetch_summary = []\n",
        "per_date_dataframes = {}\n",
        "\n",
        "# Display method description\n",
        "method_display = f\"top {100-PERCENTILE}%\" if SAMPLING_METHOD == \"top_percentile\" else f\"{int(SAMPLE_FRACTION*100)}% random\"\n",
        "print(f\"üìä Method: {method_display} blocks\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for date in DATES_TO_ANALYZE:\n",
        "\n",
        "    # Get gas limit for this date from the dictionary\n",
        "    gas_limit = date_gas_limits[date]\n",
        "    cache_file = _cache_path(date)\n",
        "\n",
        "\n",
        "    # Show progress for shorter date ranges\n",
        "    if len(DATES_TO_ANALYZE) <= 7:\n",
        "        print(f\"\\nüìÖ {date} (gas limit: {gas_limit:,})\")\n",
        "    \n",
        "    # Check cache first\n",
        "    if cache_file.exists() and not FORCE_REFRESH:\n",
        "        df = pl.read_parquet(cache_file)\n",
        "        actual_gas_limit = gas_limit\n",
        "        if len(DATES_TO_ANALYZE) <= 7:\n",
        "            print(f\"   üì¶ Cache hit ‚Üí {len(df):,} transactions\")\n",
        "    else:\n",
        "        # Fetch from source\n",
        "        cache_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        try:\n",
        "            if SAMPLING_METHOD == \"top_percentile\":\n",
        "                df, actual_gas_limit = fetch_top_percentile_blocks(\n",
        "                    chain=CHAIN,\n",
        "                    date=date,\n",
        "                    percentile=PERCENTILE,\n",
        "                    limit=BLOCK_LIMIT,\n",
        "                    gas_limit=gas_limit\n",
        "                )\n",
        "            else:\n",
        "                df, actual_gas_limit = fetch_random_sample_blocks(\n",
        "                    chain=CHAIN,\n",
        "                    date=date,\n",
        "                    sample_fraction=SAMPLE_FRACTION,\n",
        "                    gas_limit=gas_limit,\n",
        "                    seed=RANDOM_SEED\n",
        "                )\n",
        "            \n",
        "            # Save to cache if we got data\n",
        "            if not df.is_empty():\n",
        "                df.write_parquet(cache_file)\n",
        "                if len(DATES_TO_ANALYZE) <= 7:\n",
        "                    print(f\"   üíæ Saved to cache ‚Üí {len(df):,} transactions\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error fetching {date}: {e}\")\n",
        "            df = pl.DataFrame()\n",
        "            actual_gas_limit = gas_limit\n",
        "    \n",
        "    # Process results\n",
        "    if not df.is_empty():\n",
        "        all_dataframes.append(df)\n",
        "        per_date_dataframes[date] = df\n",
        "        blocks = df['block_number'].n_unique()\n",
        "        txs = len(df)\n",
        "        fetch_summary.append({\n",
        "            'date': date,\n",
        "            'blocks': blocks,\n",
        "            'transactions': txs,\n",
        "            'gas_limit': actual_gas_limit,\n",
        "            'df': df\n",
        "        })\n",
        "        if len(DATES_TO_ANALYZE) <= 7:\n",
        "            print(f\"   ‚úÖ {blocks} blocks, {txs:,} transactions\")\n",
        "    else:\n",
        "        if len(DATES_TO_ANALYZE) <= 7:\n",
        "            print(f\"   ‚ö†Ô∏è  No data for {date}\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nüìä Fetching complete!\")\n",
        "print(f\"   Total time: {total_time:.1f}s\")\n",
        "print(f\"   Average per date: {total_time/len(DATES_TO_ANALYZE):.1f}s\")\n",
        "\n",
        "# Combine all data\n",
        "if all_dataframes:\n",
        "    combined_df = pl.concat(all_dataframes)\n",
        "    print(f\"\\n‚úÖ Data summary:\")\n",
        "    print(f\"   Total dates with data: {len(fetch_summary)}\")\n",
        "    print(f\"   Total blocks: {combined_df['block_number'].n_unique():,}\")\n",
        "    print(f\"   Total transactions: {len(combined_df):,}\")\n",
        "else:\n",
        "    print(\"\\n‚ùå No data retrieved. Please check your parameters.\")\n",
        "    combined_df = pl.DataFrame()\n",
        "\n",
        "# Determine gas limit for combined analysis\n",
        "unique_limits = set(date_gas_limits.values())\n",
        "if len(unique_limits) > 1:\n",
        "    print(f\"\\n‚ö†Ô∏è  Multiple gas limits detected: {', '.join(f'{gl:,}' for gl in sorted(unique_limits))}\")\n",
        "    # Use the most common gas limit for combined analysis\n",
        "    limit_counts = Counter(date_gas_limits.values())\n",
        "    ANALYSIS_GAS_LIMIT = limit_counts.most_common(1)[0][0]\n",
        "    print(f\"üìä Using most common for combined analysis: {ANALYSIS_GAS_LIMIT:,}\")\n",
        "else:\n",
        "    ANALYSIS_GAS_LIMIT = list(unique_limits)[0]\n",
        "    print(f\"\\nüìä Consistent gas limit across all dates: {ANALYSIS_GAS_LIMIT:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Jovian Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze combined data with multiple calldata footprint gas scalars\n",
        "if not combined_df.is_empty():\n",
        "    print(f\"üìä ANALYZING COMBINED DATA\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Chain: {get_chain_display_name(CHAIN)}\")\n",
        "    print(f\"Dates analyzed: {len(fetch_summary)}\")\n",
        "    print(f\"Gas limit: {ANALYSIS_GAS_LIMIT:,}\")\n",
        "    print(f\"Sampling method: {SAMPLING_METHOD}\")\n",
        "    \n",
        "    analysis_results = perform_jovian_analysis(\n",
        "        df=combined_df,\n",
        "        gas_limit=ANALYSIS_GAS_LIMIT,\n",
        "        calldata_footprint_gas_scalars=CALLDATA_FOOTPRINT_GAS_SCALARS,\n",
        "        chain=CHAIN,\n",
        "        sampling_method=SAMPLING_METHOD,\n",
        "        start_date=START_DATE,\n",
        "        end_date=END_DATE,\n",
        "        show_progress=True\n",
        "    )\n",
        "    \n",
        "    print(\"\\n‚úÖ Jovian analysis complete!\")\n",
        "else:\n",
        "    print(\"‚ùå No data to analyze\")\n",
        "    analysis_results = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: View Results with Compression Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display analysis results including compression metrics\n",
        "if analysis_results:\n",
        "    print(f\"üöÄ JOVIAN ANALYSIS RESULTS - {get_chain_display_name(CHAIN)}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for scalar in sorted(analysis_results.keys()):\n",
        "        result = analysis_results[scalar]\n",
        "        print(f\"\\nüîπ Calldata Footprint Gas Scalar: {scalar}\")\n",
        "        print(f\"   Effective limit: {result.gas_limit // scalar:,} bytes\")\n",
        "        print(f\"   Blocks exceeding: {result.blocks_exceeding}/{result.total_blocks} ({result.percentage_exceeding:.2f}%)\")\n",
        "        print(f\"   Avg utilization: {result.avg_utilization:.2%}\")\n",
        "        print(f\"   Max utilization: {result.max_utilization:.2%}\")\n",
        "\n",
        "        # ‚õΩ DA footprint vs *block gas used* (as % of gas used)\n",
        "        if getattr(result, \"blocks_with_gas_used\", 0) > 0:\n",
        "            mean_pct   = result.avg_util_vs_gas_used * 100\n",
        "            median_pct = result.median_util_vs_gas_used * 100\n",
        "            p95_pct    = result.p95_util_vs_gas_used * 100\n",
        "\n",
        "            over_cnt_measured = int(round(result.share_over_1_vs_used * result.blocks_with_gas_used))\n",
        "            over_pct_measured = result.share_over_1_vs_used * 100\n",
        "\n",
        "            # Optional: express as share of ALL blocks too (matches the ‚ÄúBlocks exceeding‚Äù style)\n",
        "            over_pct_all = (over_cnt_measured / result.total_blocks * 100) if result.total_blocks else 0\n",
        "\n",
        "            print(f\"\\n   ‚õΩ DA footprint vs Block Gas Used:\")\n",
        "            print(f\"      mean:   {mean_pct:.1f}%\")\n",
        "            print(f\"      median: {median_pct:.1f}%   |   p95: {p95_pct:.1f}%\")\n",
        "            print(f\"      Blocks over gas-used budget (>100%): \"\n",
        "                f\"{over_cnt_measured}/{result.blocks_with_gas_used} ({over_pct_measured:.1f}%) \")\n",
        "        else:\n",
        "            print(\"\\n   ‚õΩ DA footprint vs Block Gas Used: n/a (no blocks had block_gas_used)\")\n",
        "        \n",
        "        # Show compression metrics\n",
        "        if result.compression_metrics:\n",
        "            print(f\"\\n   üìä Compression Metrics:\")\n",
        "            print(f\"      Avg compression ratio: {result.compression_metrics.get('avg_compression_ratio', 0):.2f}x\")\n",
        "            print(f\"      Median compression: {result.compression_metrics.get('median_compression_ratio', 0):.2f}x\")\n",
        "            print(f\"      DA efficiency: {result.compression_metrics.get('avg_da_efficiency', 0):.1%}\")\n",
        "        \n",
        "        if result.blocks_exceeding > 0:\n",
        "            print(f\"   Avg excess: {result.avg_excess_percentage:.1f}%\")\n",
        "            print(f\"   Max excess: {result.max_excess_percentage:.1f}%\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Generate Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nüéØ RECOMMENDATION - {get_chain_display_name(CHAIN)}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "rec = generate_jovian_recommendation(\n",
        "    results=analysis_results,            \n",
        "    chain=CHAIN,\n",
        "    start_date=START_DATE,\n",
        "    end_date=END_DATE,\n",
        "    target_excess_rate=0.01\n",
        ")\n",
        "\n",
        "for opt in rec[\"all_options\"]:\n",
        "    s = opt[\"calldata_footprint_gas_scalar\"]\n",
        "    tick = \"‚úÖ\" if opt[\"blocks_exceeding_pct\"] <= 1 else (\"‚ö†Ô∏è\" if opt[\"blocks_exceeding_pct\"] <= 5 else \"‚ùå\")\n",
        "\n",
        "    # Avg compression from recommendation dict\n",
        "    avg_comp = opt.get(\"avg_compression_ratio\", 0.0)\n",
        "\n",
        "    # Median compression from analysis_results (no lib changes needed)\n",
        "    median_comp = 0.0\n",
        "    if s in analysis_results and analysis_results[s].compression_metrics:\n",
        "        median_comp = analysis_results[s].compression_metrics.get(\"median_compression_ratio\", 0.0)\n",
        "\n",
        "    # DA footprint > gas used share (only if measured)\n",
        "    measured = opt.get(\"measured_blocks_gas_used\", 0) or 0\n",
        "    over_pct = opt.get(\"over_gas_used_pct\", None)\n",
        "    over_str = (\n",
        "        f\" | DA footprint > block gas used: {over_pct:.2f}% of {measured:,} blocks\"\n",
        "        if (over_pct is not None and measured > 0) else \"\"\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"{tick} Scalar {s}: {opt['blocks_exceeding_pct']:.2f}% exceeds gas limit, {opt['assessment']} | \"\n",
        "        f\"avg comp {avg_comp:.2f}x (median {median_comp:.2f}x){over_str}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Generate Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate all Jovian visualizations with enhanced features\n",
        "if analysis_results:\n",
        "    print(f\"üìä Generating enhanced visualizations for {get_chain_display_name(CHAIN)}...\")\n",
        "    \n",
        "    # Generate all visualizations (includes new comprehensive summary)\n",
        "    figures = generate_all_visualizations(\n",
        "        results_by_scalar=analysis_results,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        chain=CHAIN,\n",
        "        start_date=START_DATE,\n",
        "        end_date=END_DATE\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n‚úÖ Generated {len(figures)} visualizations:\")\n",
        "    print(\"   1. Size estimates histogram (with vertical lines & zones)\")\n",
        "    print(\"   2. Compression ratio histogram (with percentiles & zones)\")\n",
        "    print(\"   3. Blocks exceeding limits analysis\")\n",
        "    print(\"   4. Excess distribution\")\n",
        "    print(\"   5. Over-utilization percentages\")\n",
        "    print(\"   6. Scalar comparison\")\n",
        "    print(\"   7. Comprehensive summary (NEW)\")\n",
        "    print(f\"\\nüìÅ Saved to: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 9: Verify Jovian Calculations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_block_metric_distribution(\n",
        "    scalar_to_peek=400,\n",
        "    analysis_results=analysis_results,\n",
        "    metric_fn=lambda b: b.total_size_estimate / (b.total_calldata_size + 100 * b.tx_count),\n",
        "    title=\"DA size estimate / raw calldata per block\",\n",
        "    xlabel=\"DA efficiency (%)\",\n",
        "    percent_scale=True,\n",
        "    chain_name=get_chain_display_name(CHAIN),\n",
        "    start_date=START_DATE,\n",
        "    end_date=END_DATE,\n",
        "    min_calldata=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_block_metric_distribution(\n",
        "    scalar_to_peek=400,\n",
        "    analysis_results=analysis_results,\n",
        "    metric_fn=lambda b: b.utilization_vs_gas_used,\n",
        "    title=\"Distribution of size_estimate / gas_used\",\n",
        "    xlabel=\"size_estimate √∑ block_gas_used (%)\",\n",
        "    percent_scale=True,\n",
        "    value_filter=lambda x: x is not None,\n",
        "    chain_name=get_chain_display_name(CHAIN),\n",
        "    start_date=START_DATE,\n",
        "    end_date=END_DATE\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Instructions\n",
        "\n",
        "### To Analyze Different Chains:\n",
        "1. Change `CHAIN = \"base\"` to `CHAIN = \"op\"` (or other chains)\n",
        "2. Re-run the configuration cell and all subsequent cells\n",
        "\n",
        "### To Change Sampling Method:\n",
        "1. Change `SAMPLING_METHOD = \"top_percentile\"` to `SAMPLING_METHOD = \"random\"`\n",
        "2. Adjust `PERCENTILE` or `SAMPLE_FRACTION` as needed\n",
        "3. Re-run the configuration cell and all subsequent cells\n",
        "\n",
        "### To Analyze Different Date Ranges:\n",
        "1. Change `START_DATE` and `END_DATE`\n",
        "2. Re-run the configuration cell and all subsequent cells\n",
        "\n",
        "### To Force Refresh Data:\n",
        "1. Set `FORCE_REFRESH = True`\n",
        "2. Re-run the data fetching cell\n",
        "\n",
        "### To Save Results:\n",
        "1. Set `SAVE_RESULTS = True` (default)\n",
        "2. Results will be saved to `../results/{chain}/`\n",
        "\n",
        "### To Show Plots:\n",
        "1. Set `SHOW_PLOTS = True` (default)\n",
        "2. Plots will be displayed inline"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
