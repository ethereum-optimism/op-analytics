#!/usr/bin/env python3
"""
Script to create a consolidated Jovian analysis notebook.
"""

import json

notebook_content = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Jovian Analysis - Consolidated Notebook\n",
                "\n",
                "**Unified notebook for Jovian calldata footprint gas scalar analysis**\n",
                "\n",
                "## Features\n",
                "- ✅ **Multi-chain support**: Base, Optimism, Mode, Zora, World, Ink, Soneium\n",
                "- ✅ **Flexible sampling**: Top percentile or random sampling\n",
                "- ✅ **Date range analysis**: Analyze multiple dates with per-date breakdown\n",
                "- ✅ **Dynamic gas limits**: Uses per-chain historical CSV data\n",
                "- ✅ **Compression analysis**: FastLZ compression ratios and DA efficiency\n",
                "- ✅ **Enhanced visualizations**: Comprehensive charts with recommendations\n",
                "- ✅ **Caching system**: Local parquet files for performance\n",
                "- ✅ **Optimal recommendations**: Data-driven scalar suggestions\n",
                "\n",
                "## Quick Start\n",
                "1. **Configure parameters** in the cell below\n",
                "2. **Run all cells** to perform analysis\n",
                "3. **Review results** in the generated visualizations"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Configuration\n",
                "\n",
                "**Edit these parameters to customize your analysis:**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# ANALYSIS CONFIGURATION - EDIT THESE PARAMETERS\n",
                "# ============================================================================\n",
                "\n",
                "# Chain selection\n",
                "CHAIN = \"base\"  # Options: base, op, mode, zora, world, ink, soneium\n",
                "\n",
                "# Sampling method\n",
                "SAMPLING_METHOD = \"top_percentile\"  # \"top_percentile\" or \"random\"\n",
                "PERCENTILE = 99.0    # For top percentile (99 = top 1%)\n",
                "NUM_BLOCKS = 100     # For random sampling - blocks per day\n",
                "RANDOM_SEED = 42     # For reproducible random sampling\n",
                "SAMPLE_FRACTION = 0.01  # For random sampling (1% = 0.01)\n",
                "\n",
                "# Date range\n",
                "START_DATE = \"2025-03-01\"\n",
                "END_DATE = \"2025-06-30\"  # Inclusive\n",
                "\n",
                "# Analysis parameters\n",
                "BLOCK_LIMIT = None   # None = all blocks for selected method\n",
                "FORCE_REFRESH = False  # Set True to ignore cache and re-download\n",
                "\n",
                "# Output settings\n",
                "SAVE_RESULTS = True\n",
                "SHOW_PLOTS = True\n",
                "\n",
                "print(\"✅ Configuration loaded:\")\n",
                "print(f\"   Chain: {CHAIN}\")\n",
                "print(f\"   Method: {SAMPLING_METHOD}\")\n",
                "if SAMPLING_METHOD == \"top_percentile\":\n",
                "    print(f\"   Percentile: Top {100-PERCENTILE}%\")\n",
                "else:\n",
                "    print(f\"   Sample: {int(SAMPLE_FRACTION*100)}% random\")\n",
                "print(f\"   Date range: {START_DATE} to {END_DATE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Core imports\n",
                "import sys\n",
                "from pathlib import Path\n",
                "import polars as pl\n",
                "from datetime import datetime, timedelta\n",
                "from collections import Counter\n",
                "import matplotlib.pyplot as plt\n",
                "import time\n",
                "import numpy as np\n",
                "\n",
                "# Add parent directory to path\n",
                "sys.path.append(str(Path.cwd().parent))\n",
                "\n",
                "# Import Jovian modules\n",
                "from jovian_src.chain_config import (\n",
                "    get_chain_display_name,\n",
                "    get_chain_color,\n",
                "    DEFAULT_CALLDATA_FOOTPRINT_GAS_SCALARS\n",
                ")\n",
                "from jovian_src.clickhouse_fetcher import (\n",
                "    load_gas_limits,\n",
                "    fetch_top_percentile_blocks,\n",
                "    fetch_random_sample_blocks\n",
                ")\n",
                "from jovian_src.analysis_functions import (\n",
                "    perform_jovian_analysis,\n",
                "    generate_jovian_recommendation\n",
                ")\n",
                "from jovian_src.visualization_jovian import (\n",
                "    generate_all_visualizations\n",
                ")\n",
                "\n",
                "print(\"✅ Modules loaded successfully\")\n",
                "print(f\"✅ Available scalars: {DEFAULT_CALLDATA_FOOTPRINT_GAS_SCALARS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Initialize Analysis Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# ENVIRONMENT SETUP\n",
                "# ============================================================================\n",
                "\n",
                "# Paths\n",
                "DATA_PATH = Path(\"../gas_limits\")\n",
                "CACHE_DIR = Path(\".cache\")\n",
                "OUTPUT_DIR = Path(f\"../results/{CHAIN}/jovian_analysis_{SAMPLING_METHOD}_{START_DATE}_{END_DATE}\")\n",
                "\n",
                "# Create output directory\n",
                "if SAVE_RESULTS:\n",
                "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "    print(f\"📁 Output directory: {OUTPUT_DIR}\")\n",
                "\n",
                "# Generate date list\n",
                "start = datetime.strptime(START_DATE, \"%Y-%m-%d\")\n",
                "end = datetime.strptime(END_DATE, \"%Y-%m-%d\")\n",
                "DATES_TO_ANALYZE = [(start + timedelta(days=i)).strftime(\"%Y-%m-%d\") \n",
                "                     for i in range((end - start).days + 1)]\n",
                "\n",
                "# Load gas limits\n",
                "print(f\"📊 Loading gas limits for {CHAIN}...\")\n",
                "date_gas_limits = load_gas_limits(CHAIN)\n",
                "print(f\"✅ Loaded gas limits for {len(date_gas_limits)} dates\")\n",
                "\n",
                "# Cache configuration\n",
                "def _method_key():\n",
                "    \"\"\"Generate cache key based on sampling method and parameters.\"\"\"\n",
                "    if SAMPLING_METHOD == \"top_percentile\":\n",
                "        return f\"top{PERCENTILE*100:.1f}pct_lim{BLOCK_LIMIT}\"\n",
                "    return f\"rand_frac{SAMPLE_FRACTION}_seed{RANDOM_SEED}\"\n",
                "\n",
                "def _cache_path(date_str: str) -> Path:\n",
                "    \"\"\"Generate cache file path for a specific date.\"\"\"\n",
                "    return CACHE_DIR / CHAIN / _method_key() / f\"{date_str}.parquet\"\n",
                "\n",
                "print(f\"🔧 Cache method: {_method_key()}\")\n",
                "print(f\"📅 Dates to analyze: {len(DATES_TO_ANALYZE)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Data Fetching and Caching"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# DATA FETCHING WITH CACHING\n",
                "# ============================================================================\n",
                "\n",
                "print(f\"📥 Fetching data for {CHAIN} using {SAMPLING_METHOD} method...\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "all_dataframes = []\n",
                "fetch_summary = []\n",
                "per_date_dataframes = {}\n",
                "\n",
                "# Display method description\n",
                "method_display = f\"top {100-PERCENTILE}%\" if SAMPLING_METHOD == \"top_percentile\" else f\"{int(SAMPLE_FRACTION*100)}% random\"\n",
                "print(f\"📊 Method: {method_display} blocks\")\n",
                "\n",
                "start_time = time.time()\n",
                "\n",
                "for date in DATES_TO_ANALYZE:\n",
                "    gas_limit = date_gas_limits.get(date, 240_000_000)  # Default fallback\n",
                "    cache_file = _cache_path(date)\n",
                "    \n",
                "    # Show progress for shorter date ranges\n",
                "    if len(DATES_TO_ANALYZE) <= 7:\n",
                "        print(f\"\\n📅 {date} (gas limit: {gas_limit:,})\")\n",
                "    \n",
                "    # Check cache first\n",
                "    if cache_file.exists() and not FORCE_REFRESH:\n",
                "        df = pl.read_parquet(cache_file)\n",
                "        actual_gas_limit = gas_limit\n",
                "        if len(DATES_TO_ANALYZE) <= 7:\n",
                "            print(f\"   📦 Cache hit → {len(df):,} transactions\")\n",
                "    else:\n",
                "        # Fetch from source\n",
                "        cache_file.parent.mkdir(parents=True, exist_ok=True)\n",
                "        \n",
                "        try:\n",
                "            if SAMPLING_METHOD == \"top_percentile\":\n",
                "                df, actual_gas_limit = fetch_top_percentile_blocks(\n",
                "                    chain=CHAIN,\n",
                "                    date=date,\n",
                "                    percentile=PERCENTILE,\n",
                "                    limit=BLOCK_LIMIT,\n",
                "                    gas_limit=gas_limit\n",
                "                )\n",
                "            else:\n",
                "                df, actual_gas_limit = fetch_random_sample_blocks(\n",
                "                    chain=CHAIN,\n",
                "                    date=date,\n",
                "                    sample_fraction=SAMPLE_FRACTION,\n",
                "                    gas_limit=gas_limit,\n",
                "                    seed=RANDOM_SEED\n",
                "                )\n",
                "            \n",
                "            # Save to cache if we got data\n",
                "            if not df.is_empty():\n",
                "                df.write_parquet(cache_file)\n",
                "                if len(DATES_TO_ANALYZE) <= 7:\n",
                "                    print(f\"   💾 Saved to cache → {len(df):,} transactions\")\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"   ❌ Error fetching {date}: {e}\")\n",
                "            df = pl.DataFrame()\n",
                "            actual_gas_limit = gas_limit\n",
                "    \n",
                "    # Process results\n",
                "    if not df.is_empty():\n",
                "        all_dataframes.append(df)\n",
                "        per_date_dataframes[date] = df\n",
                "        blocks = df['block_number'].n_unique()\n",
                "        txs = len(df)\n",
                "        fetch_summary.append({\n",
                "            'date': date,\n",
                "            'blocks': blocks,\n",
                "            'transactions': txs,\n",
                "            'gas_limit': actual_gas_limit,\n",
                "            'df': df\n",
                "        })\n",
                "        if len(DATES_TO_ANALYZE) <= 7:\n",
                "            print(f\"   ✅ {blocks} blocks, {txs:,} transactions\")\n",
                "    else:\n",
                "        if len(DATES_TO_ANALYZE) <= 7:\n",
                "            print(f\"   ⚠️  No data for {date}\")\n",
                "\n",
                "total_time = time.time() - start_time\n",
                "print(f\"\\n📊 Fetching complete!\")\n",
                "print(f\"   Total time: {total_time:.1f}s\")\n",
                "print(f\"   Average per date: {total_time/len(DATES_TO_ANALYZE):.1f}s\")\n",
                "\n",
                "# Combine all data\n",
                "if all_dataframes:\n",
                "    combined_df = pl.concat(all_dataframes)\n",
                "    print(f\"\\n✅ Data summary:\")\n",
                "    print(f\"   Total dates with data: {len(fetch_summary)}\")\n",
                "    print(f\"   Total blocks: {combined_df['block_number'].n_unique():,}\")\n",
                "    print(f\"   Total transactions: {len(combined_df):,}\")\n",
                "else:\n",
                "    print(\"\\n❌ No data retrieved. Please check your parameters.\")\n",
                "    combined_df = pl.DataFrame()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Jovian Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# JOVIAN ANALYSIS\n",
                "# ============================================================================\n",
                "\n",
                "if combined_df.is_empty():\n",
                "    print(\"❌ No data to analyze. Please check the previous step.\")\n",
                "else:\n",
                "    print(f\"🔬 Performing Jovian analysis for {CHAIN}...\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    # Get gas limit for analysis (use first available)\n",
                "    analysis_gas_limit = fetch_summary[0]['gas_limit'] if fetch_summary else 240_000_000\n",
                "    \n",
                "    # Perform analysis with multiple scalars\n",
                "    analysis_results = perform_jovian_analysis(\n",
                "        df=combined_df,\n",
                "        gas_limit=analysis_gas_limit,\n",
                "        calldata_footprint_gas_scalars=DEFAULT_CALLDATA_FOOTPRINT_GAS_SCALARS\n",
                "    )\n",
                "    \n",
                "    print(\"✅ Analysis complete!\")\n",
                "    print(f\"   Gas limit used: {analysis_gas_limit:,}\")\n",
                "    print(f\"   Scalars tested: {len(analysis_results)}\")\n",
                "    \n",
                "    # Show summary for each scalar\n",
                "    for scalar, result in analysis_results.items():\n",
                "        blocks_over = result.blocks_exceeding_limit\n",
                "        total_blocks = result.total_blocks\n",
                "        pct_over = (blocks_over / total_blocks * 100) if total_blocks > 0 else 0\n",
                "        print(f\"   Scalar {scalar}: {blocks_over}/{total_blocks} blocks over limit ({pct_over:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Generate Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# VISUALIZATION GENERATION\n",
                "# ============================================================================\n",
                "\n",
                "if 'analysis_results' in locals() and analysis_results:\n",
                "    print(f\"🎨 Generating visualizations for {CHAIN}...\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    try:\n",
                "        # Generate all visualizations\n",
                "        figures = generate_all_visualizations(\n",
                "            results_by_scalar=analysis_results,\n",
                "            output_dir=str(OUTPUT_DIR) if SAVE_RESULTS else None,\n",
                "            chain_name=CHAIN,\n",
                "            sampling_method=SAMPLING_METHOD\n",
                "        )\n",
                "        \n",
                "        print(\"✅ Visualizations generated successfully!\")\n",
                "        \n",
                "        if SAVE_RESULTS:\n",
                "            print(f\"📁 Saved to: {OUTPUT_DIR}\")\n",
                "        \n",
                "        if SHOW_PLOTS:\n",
                "            print(\"\\n📊 Displaying plots...\")\n",
            "            for name, fig in figures.items():\n",
            "                plt.figure(fig.number)\n",
            "                plt.show()\n",
            "        \n",
            "    except Exception as e:\n",
            "        print(f\"❌ Error generating visualizations: {e}\")\n",
            "        print(\"Continuing with analysis...\")\n",
            "else:\n",
            "    print(\"❌ No analysis results to visualize.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Generate Recommendations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# RECOMMENDATIONS AND SUMMARY\n",
                "# ============================================================================\n",
                "\n",
                "if 'analysis_results' in locals() and analysis_results:\n",
                "    print(f\"💡 Generating recommendations for {CHAIN}...\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    try:\n",
                "        # Generate recommendations\n",
                "        recommendation = generate_jovian_recommendation(\n",
                "            results_by_scalar=analysis_results,\n",
                "            target_excess_rate=0.01  # Target 1% excess rate\n",
                "        )\n",
                "        \n",
                "        print(\"\\n🎯 RECOMMENDATIONS:\")\n",
                "        print(f\"   Optimal scalar: {recommendation.optimal_scalar}\")\n",
                "        print(f\"   Expected excess rate: {recommendation.expected_excess_rate:.2%}\")\n",
                "        print(f\"   Recommendation type: {recommendation.recommendation_type}\")\n",
                "        \n",
                "        if recommendation.recommendation_type == \"constant\":\n",
                "            print(f\"   Use constant scalar: {recommendation.optimal_scalar}\")\n",
                "        else:\n",
                "            print(f\"   Consider configurable scalar based on conditions\")\n",
                "        \n",
                "        # Save recommendation if requested\n",
                "        if SAVE_RESULTS:\n",
                "            import json\n",
                "            rec_file = OUTPUT_DIR / \"recommendation.json\"\n",
                "            with open(rec_file, 'w') as f:\n",
                "                json.dump({\n",
                "                    'chain': CHAIN,\n",
                "                    'sampling_method': SAMPLING_METHOD,\n",
                "                    'date_range': f\"{START_DATE} to {END_DATE}\",\n",
                "                    'optimal_scalar': recommendation.optimal_scalar,\n",
                "                    'expected_excess_rate': recommendation.expected_excess_rate,\n",
                "                    'recommendation_type': recommendation.recommendation_type\n",
                "                }, f, indent=2)\n",
                "            print(f\"\\n💾 Recommendation saved to: {rec_file}\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"❌ Error generating recommendations: {e}\")\n",
                "        \n",
                "    # Final summary\n",
                "    print(f\"\\n📋 ANALYSIS SUMMARY:\")\n",
                "    print(f\"   Chain: {CHAIN}\")\n",
                "    print(f\"   Method: {SAMPLING_METHOD}\")\n",
                "    print(f\"   Date range: {START_DATE} to {END_DATE}\")\n",
                "    print(f\"   Total transactions: {len(combined_df):,}\")\n",
                "    print(f\"   Total blocks: {combined_df['block_number'].n_unique():,}\")\n",
                "    print(f\"   Gas limit: {analysis_gas_limit:,}\")\n",
                "    \n",
                "    if SAVE_RESULTS:\n",
                "        print(f\"\\n🎉 Analysis complete! Results saved to: {OUTPUT_DIR}\")\n",
                "    else:\n",
                "        print(f\"\\n🎉 Analysis complete!\")\n",
                "else:\n",
                "    print(\"❌ No analysis results available for summary.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Usage Instructions\n",
                "\n",
                "### To Analyze Different Chains:\n",
                "1. Change `CHAIN = \"base\"` to `CHAIN = \"op\"` (or other chains)\n",
                "2. Re-run the configuration cell and all subsequent cells\n",
                "\n",
                "### To Change Sampling Method:\n",
                "1. Change `SAMPLING_METHOD = \"top_percentile\"` to `SAMPLING_METHOD = \"random\"`\n",
                "2. Adjust `PERCENTILE` or `SAMPLE_FRACTION` as needed\n",
                "3. Re-run the configuration cell and all subsequent cells\n",
                "\n",
                "### To Analyze Different Date Ranges:\n",
                "1. Change `START_DATE` and `END_DATE`\n",
                "2. Re-run the configuration cell and all subsequent cells\n",
                "\n",
                "### To Force Refresh Data:\n",
                "1. Set `FORCE_REFRESH = True`\n",
                "2. Re-run the data fetching cell\n",
                "\n",
                "### To Save Results:\n",
                "1. Set `SAVE_RESULTS = True` (default)\n",
                "2. Results will be saved to `../results/{chain}/`\n",
                "\n",
                "### To Show Plots:\n",
                "1. Set `SHOW_PLOTS = True` (default)\n",
                "2. Plots will be displayed inline"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

# Write the notebook
with open('jovian_analysis.ipynb', 'w') as f:
    json.dump(notebook_content, f, indent=2)

print("✅ Consolidated notebook created: jovian_analysis.ipynb")
