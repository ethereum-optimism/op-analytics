{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Arkham Entity Transfers Explorer\n",
        "\n",
        "This notebook queries the Arkham Intel API to:\n",
        "\n",
        "- Search for an entity (e.g., Revolut, Kraken)\n",
        "- Fetch that entity's on-chain transfers/transactions with tags, labels, and metadata\n",
        "- Filter by date range, chain set, and minimum USD value\n",
        "- Display and export a normalized table\n",
        "\n",
        "Docs: [Arkham Intel API](https://intel.arkm.com/docs/)\n",
        "\n",
        "Instructions:\n",
        "1. Put your API key in a `.env` at the repo root as `ARKHAM_API_KEY=...`\n",
        "2. Adjust the API base URL if needed.\n",
        "3. Use the widgets below to run queries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install deps if needed\n",
        "# %pip install requests python-dotenv ipywidgets pandas --quiet\n",
        "\n",
        "from __future__ import annotations\n",
        "from datetime import datetime, timedelta, timezone, date\n",
        "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# Try to load API key from .env if available\n",
        "try:\n",
        "    from dotenv import load_dotenv  # type: ignore\n",
        "    load_dotenv()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "ARKHAM_API_KEY = os.getenv(\"ARKHAM_API_KEY\") or os.getenv(\"ARKHAM_INTEL_API_KEY\")\n",
        "if not ARKHAM_API_KEY:\n",
        "    raise RuntimeError(\"Set ARKHAM_API_KEY in your .env or environment.\")\n",
        "\n",
        "# Default base URL (adjust if the docs specify a different one)\n",
        "ARKHAM_API_BASE_URL = os.getenv(\"ARKHAM_API_BASE_URL\", \"https://intel.arkm.com/api\")\n",
        "\n",
        "# Reasonable default timeout and backoff\n",
        "HTTP_TIMEOUT_SECS = float(os.getenv(\"ARKHAM_HTTP_TIMEOUT\", \"30\"))\n",
        "HTTP_BACKOFF_SECS = float(os.getenv(\"ARKHAM_HTTP_BACKOFF\", \"0.75\"))\n",
        "MAX_RETRIES = int(os.getenv(\"ARKHAM_HTTP_MAX_RETRIES\", \"3\"))\n",
        "\n",
        "SESSION = requests.Session()\n",
        "\n",
        "\n",
        "def build_headers(api_key: str) -> Dict[str, str]:\n",
        "    \"\"\"Construct headers. Arkham may accept either Bearer or X-API-Key schemes.\n",
        "    We send both for compatibility.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"X-API-Key\": api_key,\n",
        "        \"Accept\": \"application/json\",\n",
        "        \"User-Agent\": \"op-analytics/arkham-entity-transfers-notebook\",\n",
        "    }\n",
        "\n",
        "\n",
        "HEADERS = build_headers(ARKHAM_API_KEY)\n",
        "\n",
        "\n",
        "def _http_request(method: str, path: str, *, params: Optional[Dict[str, Any]] = None, json_body: Optional[Dict[str, Any]] = None) -> requests.Response:\n",
        "    url = path if path.startswith(\"http\") else f\"{ARKHAM_API_BASE_URL.rstrip('/')}/{path.lstrip('/')}\"\n",
        "    last_exc: Optional[Exception] = None\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            resp = SESSION.request(method=method, url=url, headers=HEADERS, params=params, json=json_body, timeout=HTTP_TIMEOUT_SECS)\n",
        "            if resp.status_code == 429 or (500 <= resp.status_code < 600):\n",
        "                # Backoff and retry on rate limit and server errors\n",
        "                time.sleep(HTTP_BACKOFF_SECS * attempt)\n",
        "                continue\n",
        "            return resp\n",
        "        except requests.RequestException as exc:  # network hiccup\n",
        "            last_exc = exc\n",
        "            time.sleep(HTTP_BACKOFF_SECS * attempt)\n",
        "    if last_exc is not None:\n",
        "        raise last_exc\n",
        "    raise RuntimeError(\"HTTP request failed without exception.\")\n",
        "\n",
        "\n",
        "def _try_paths_get(paths: Iterable[str], params: Optional[Dict[str, Any]] = None) -> Tuple[str, requests.Response]:\n",
        "    \"\"\"Try multiple endpoint paths until one returns HTTP 200.\n",
        "    Returns the successful path and response. Raises if none succeed.\n",
        "    \"\"\"\n",
        "    errors: List[str] = []\n",
        "    for p in paths:\n",
        "        r = _http_request(\"GET\", p, params=params)\n",
        "        if r.ok:\n",
        "            return p, r\n",
        "        errors.append(f\"{p} -> {r.status_code} {r.text[:200]}\")\n",
        "    raise RuntimeError(\"No endpoint succeeded. Tried: \\n\" + \"\\n\".join(errors))\n",
        "\n",
        "\n",
        "# --- Arkham API wrappers -----------------------------------------------------\n",
        "\n",
        "ENTITY_SEARCH_PATHS = [\n",
        "    \"/entities/search\",           # common pattern\n",
        "    \"/search/entities\",           # alternate\n",
        "    \"/entities\",                  # some APIs use a single list endpoint with q=...\n",
        "]\n",
        "\n",
        "ENTITY_DETAILS_PATHS = [\n",
        "    \"/entities/{entity_id}\",\n",
        "]\n",
        "\n",
        "TRANSFERS_PATHS_TEMPLATES = [\n",
        "    \"/entities/{entity_id}/transfers\",\n",
        "    \"/entities/{entity_id}/transactions\",\n",
        "    \"/transactions\",  # fallback with entityId as a query parameter\n",
        "]\n",
        "\n",
        "\n",
        "def search_entities(query: str, limit: int = 10) -> List[Dict[str, Any]]:\n",
        "    params = {\"q\": query, \"limit\": limit}\n",
        "    path, resp = _try_paths_get(ENTITY_SEARCH_PATHS, params=params)\n",
        "    data = resp.json()\n",
        "    # Some APIs wrap results; try common shapes\n",
        "    if isinstance(data, dict):\n",
        "        if \"results\" in data and isinstance(data[\"results\"], list):\n",
        "            return data[\"results\"]\n",
        "        if \"data\" in data and isinstance(data[\"data\"], list):\n",
        "            return data[\"data\"]\n",
        "    if isinstance(data, list):\n",
        "        return data\n",
        "    return []\n",
        "\n",
        "\n",
        "def get_entity_details(entity_id: str) -> Dict[str, Any]:\n",
        "    paths = [p.format(entity_id=entity_id) for p in ENTITY_DETAILS_PATHS]\n",
        "    _, resp = _try_paths_get(paths)\n",
        "    data = resp.json()\n",
        "    if isinstance(data, dict):\n",
        "        return data.get(\"data\", data)\n",
        "    return data\n",
        "\n",
        "\n",
        "def _format_iso8601(dt: datetime | date) -> str:\n",
        "    if isinstance(dt, date) and not isinstance(dt, datetime):\n",
        "        dt = datetime(dt.year, dt.month, dt.day, tzinfo=timezone.utc)\n",
        "    if dt.tzinfo is None:\n",
        "        dt = dt.replace(tzinfo=timezone.utc)\n",
        "    return dt.isoformat()\n",
        "\n",
        "\n",
        "def fetch_entity_transfers(\n",
        "    entity_id: str,\n",
        "    *,\n",
        "    start_time: Optional[datetime | date] = None,\n",
        "    end_time: Optional[datetime | date] = None,\n",
        "    chains: Optional[List[str]] = None,\n",
        "    min_usd: Optional[float] = None,\n",
        "    page_size: int = 200,\n",
        "    max_pages: int = 20,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"Fetch transfers/transactions for an entity, following cursors/pages if available.\n",
        "    We attempt multiple likely endpoints and parameter names for best compatibility.\n",
        "    \"\"\"\n",
        "    # Common param names used by APIs\n",
        "    params: Dict[str, Any] = {\"limit\": page_size}\n",
        "    if start_time is not None:\n",
        "        params[\"startTime\"] = _format_iso8601(start_time)\n",
        "        params.setdefault(\"fromTime\", params[\"startTime\"])  # alias\n",
        "    if end_time is not None:\n",
        "        params[\"endTime\"] = _format_iso8601(end_time)\n",
        "        params.setdefault(\"toTime\", params[\"endTime\"])  # alias\n",
        "    if chains:\n",
        "        params[\"chains\"] = \",\".join(chains)\n",
        "        params.setdefault(\"chain\", params[\"chains\"])  # alias\n",
        "    if min_usd is not None:\n",
        "        params[\"minUsdValue\"] = float(min_usd)\n",
        "        params.setdefault(\"minUsd\", float(min_usd))\n",
        "\n",
        "    all_items: List[Dict[str, Any]] = []\n",
        "    next_cursor: Optional[str] = None\n",
        "\n",
        "    for page in range(max_pages):\n",
        "        page_params = dict(params)\n",
        "        if next_cursor:\n",
        "            page_params[\"cursor\"] = next_cursor\n",
        "            page_params.setdefault(\"pageCursor\", next_cursor)\n",
        "        paths = [t.format(entity_id=entity_id) for t in TRANSFERS_PATHS_TEMPLATES]\n",
        "        try:\n",
        "            _, resp = _try_paths_get(paths, params=page_params)\n",
        "        except Exception as exc:\n",
        "            # Last resort: try a generic transactions endpoint with entityId in query\n",
        "            fallback_params = dict(page_params)\n",
        "            fallback_params[\"entityId\"] = entity_id\n",
        "            _, resp = _try_paths_get([\"/transactions\", \"/transfers\"], params=fallback_params)\n",
        "        data = resp.json()\n",
        "        # Common shapes\n",
        "        items: List[Dict[str, Any]] = []\n",
        "        if isinstance(data, dict):\n",
        "            if isinstance(data.get(\"results\"), list):\n",
        "                items = data[\"results\"]\n",
        "            elif isinstance(data.get(\"data\"), list):\n",
        "                items = data[\"data\"]\n",
        "            elif isinstance(data.get(\"items\"), list):\n",
        "                items = data[\"items\"]\n",
        "            # cursors\n",
        "            next_cursor = (\n",
        "                data.get(\"nextCursor\")\n",
        "                or data.get(\"next_page_token\")\n",
        "                or data.get(\"next\")\n",
        "                or data.get(\"cursor\")\n",
        "            )\n",
        "        elif isinstance(data, list):\n",
        "            items = data\n",
        "            next_cursor = None\n",
        "        else:\n",
        "            items = []\n",
        "            next_cursor = None\n",
        "        if not items:\n",
        "            break\n",
        "        all_items.extend(items)\n",
        "        if not next_cursor:\n",
        "            break\n",
        "    return all_items\n",
        "\n",
        "\n",
        "# --- Normalization helpers ---------------------------------------------------\n",
        "\n",
        "def _get_nested(d: Dict[str, Any], keys: List[str], default: Any = None) -> Any:\n",
        "    cur: Any = d\n",
        "    for k in keys:\n",
        "        if not isinstance(cur, dict) or k not in cur:\n",
        "            return default\n",
        "        cur = cur[k]\n",
        "    return cur\n",
        "\n",
        "\n",
        "def _labels_str(labels: Any) -> str:\n",
        "    if isinstance(labels, list):\n",
        "        parts: List[str] = []\n",
        "        for x in labels:\n",
        "            if isinstance(x, str):\n",
        "                parts.append(x)\n",
        "            elif isinstance(x, dict):\n",
        "                name = x.get(\"name\") or x.get(\"label\") or x.get(\"tag\")\n",
        "                if name:\n",
        "                    parts.append(str(name))\n",
        "        return \", \".join(sorted(set(parts)))\n",
        "    return str(labels) if labels is not None else \"\"\n",
        "\n",
        "\n",
        "def normalize_transfers(raw_items: List[Dict[str, Any]]) -> pd.DataFrame:\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "    for it in raw_items:\n",
        "        # Times\n",
        "        ts = it.get(\"timestamp\") or it.get(\"time\") or it.get(\"blockTime\")\n",
        "        # Basic transfer fields\n",
        "        usd = it.get(\"usdValue\") or it.get(\"valueUsd\") or it.get(\"usd\")\n",
        "        amount = it.get(\"amount\") or it.get(\"quantity\") or it.get(\"value\")\n",
        "        token = (\n",
        "            it.get(\"tokenSymbol\")\n",
        "            or it.get(\"assetSymbol\")\n",
        "            or _get_nested(it, [\"token\", \"symbol\"]) or _get_nested(it, [\"asset\", \"symbol\"]) or \"\"\n",
        "        )\n",
        "        token_addr = (\n",
        "            it.get(\"tokenAddress\")\n",
        "            or _get_nested(it, [\"token\", \"address\"]) or _get_nested(it, [\"asset\", \"address\"]) or \"\"\n",
        "        )\n",
        "        tx_hash = it.get(\"txHash\") or it.get(\"hash\") or it.get(\"transactionHash\")\n",
        "        chain = it.get(\"chain\") or it.get(\"chainName\") or it.get(\"network\")\n",
        "        direction = it.get(\"direction\") or it.get(\"flow\") or (\"inflow\" if (usd and usd >= 0) else \"outflow\")\n",
        "        # Parties and labels\n",
        "        from_addr = it.get(\"fromAddress\") or it.get(\"from\") or _get_nested(it, [\"from\", \"address\"]) or \"\"\n",
        "        to_addr = it.get(\"toAddress\") or it.get(\"to\") or _get_nested(it, [\"to\", \"address\"]) or \"\"\n",
        "\n",
        "        from_entity = (\n",
        "            _get_nested(it, [\"fromEntity\", \"name\"]) or _get_nested(it, [\"from\", \"entity\", \"name\"]) or _get_nested(it, [\"fromEntity\", \"label\"]) or \"\"\n",
        "        )\n",
        "        to_entity = (\n",
        "            _get_nested(it, [\"toEntity\", \"name\"]) or _get_nested(it, [\"to\", \"entity\", \"name\"]) or _get_nested(it, [\"toEntity\", \"label\"]) or \"\"\n",
        "        )\n",
        "        from_labels = _labels_str(_get_nested(it, [\"from\", \"labels\"]) or it.get(\"fromLabels\"))\n",
        "        to_labels = _labels_str(_get_nested(it, [\"to\", \"labels\"]) or it.get(\"toLabels\"))\n",
        "        tx_labels = _labels_str(it.get(\"labels\") or it.get(\"tags\"))\n",
        "\n",
        "        rows.append({\n",
        "            \"timestamp\": ts,\n",
        "            \"chain\": chain,\n",
        "            \"direction\": direction,\n",
        "            \"token\": token,\n",
        "            \"token_address\": token_addr,\n",
        "            \"amount\": amount,\n",
        "            \"usd_value\": usd,\n",
        "            \"tx_hash\": tx_hash,\n",
        "            \"from_address\": from_addr,\n",
        "            \"from_entity\": from_entity,\n",
        "            \"from_labels\": from_labels,\n",
        "            \"to_address\": to_addr,\n",
        "            \"to_entity\": to_entity,\n",
        "            \"to_labels\": to_labels,\n",
        "            \"tx_labels\": tx_labels,\n",
        "            \"raw\": it,\n",
        "        })\n",
        "    df = pd.DataFrame(rows)\n",
        "    # Best-effort typing\n",
        "    if not df.empty:\n",
        "        for col in [\"usd_value\", \"amount\"]:\n",
        "            if col in df.columns:\n",
        "                df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "        if \"timestamp\" in df.columns:\n",
        "            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\", utc=True)\n",
        "        # Sort by time desc\n",
        "        if \"timestamp\" in df.columns:\n",
        "            df = df.sort_values(\"timestamp\", ascending=False).reset_index(drop=True)\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# UI controls\n",
        "entity_name_w = widgets.Text(value=\"Revolut\", description=\"Entity\", placeholder=\"e.g., Revolut\")\n",
        "start_w = widgets.DatePicker(description=\"Start\", value=(datetime.now(timezone.utc) - timedelta(days=90)).date())\n",
        "end_w = widgets.DatePicker(description=\"End\", value=datetime.now(timezone.utc).date())\n",
        "chains_options = (\n",
        "    \"ethereum\",\n",
        "    \"optimism\",\n",
        "    \"arbitrum\",\n",
        "    \"base\",\n",
        "    \"polygon\",\n",
        "    \"bsc\",\n",
        "    \"avalanche\",\n",
        "    \"tron\",\n",
        "    \"bitcoin\",\n",
        ")\n",
        "chains_w = widgets.SelectMultiple(options=chains_options, value=(\"optimism\",), description=\"Chains\")\n",
        "min_usd_w = widgets.FloatText(value=1.0, description=\"Min USD\")\n",
        "limit_w = widgets.IntSlider(value=200, min=50, max=1000, step=50, description=\"Page size\")\n",
        "max_pages_w = widgets.IntSlider(value=10, min=1, max=50, step=1, description=\"Max pages\")\n",
        "run_button = widgets.Button(description=\"Run\", button_style=\"primary\")\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    widgets.HBox([entity_name_w]),\n",
        "    widgets.HBox([start_w, end_w, min_usd_w]),\n",
        "    widgets.HBox([chains_w]),\n",
        "    widgets.HBox([limit_w, max_pages_w]),\n",
        "    run_button,\n",
        "])\n",
        "\n",
        "display(ui)\n",
        "\n",
        "out = widgets.Output()\n",
        "display(out)\n",
        "\n",
        "\n",
        "LATEST_DF: Optional[pd.DataFrame] = None\n",
        "LATEST_ENTITY: Optional[Dict[str, Any]] = None\n",
        "\n",
        "\n",
        "def _format_entity_match(e: Dict[str, Any]) -> str:\n",
        "    name = e.get(\"name\") or e.get(\"label\") or e.get(\"title\") or e.get(\"slug\") or e.get(\"id\")\n",
        "    eid = e.get(\"id\") or e.get(\"entityId\") or e.get(\"entity_id\")\n",
        "    typ = e.get(\"type\") or e.get(\"entityType\")\n",
        "    return f\"{name} (id={eid}, type={typ})\"\n",
        "\n",
        "\n",
        "@out.capture(clear_output=True)\n",
        "def on_run_click(_):\n",
        "    global LATEST_DF, LATEST_ENTITY\n",
        "    print(\"Searching entitiesâ€¦\")\n",
        "    candidates = search_entities(entity_name_w.value, limit=10)\n",
        "    if not candidates:\n",
        "        print(\"No candidates found.\")\n",
        "        return\n",
        "    # Pick top candidate\n",
        "    top = candidates[0]\n",
        "    eid = top.get(\"id\") or top.get(\"entityId\") or top.get(\"entity_id\")\n",
        "    if not eid:\n",
        "        print(\"Top candidate missing entity id:\", json.dumps(top, indent=2)[:800])\n",
        "        return\n",
        "    LATEST_ENTITY = top\n",
        "    print(\"Using:\", _format_entity_match(top))\n",
        "\n",
        "    start = start_w.value\n",
        "    end = end_w.value\n",
        "    chains = list(chains_w.value) if chains_w.value else None\n",
        "    items = fetch_entity_transfers(\n",
        "        str(eid),\n",
        "        start_time=start,\n",
        "        end_time=end,\n",
        "        chains=chains,\n",
        "        min_usd=min_usd_w.value if min_usd_w.value else None,\n",
        "        page_size=int(limit_w.value),\n",
        "        max_pages=int(max_pages_w.value),\n",
        "    )\n",
        "    print(f\"Fetched {len(items)} records.\")\n",
        "\n",
        "    df = normalize_transfers(items)\n",
        "    LATEST_DF = df\n",
        "    if df.empty:\n",
        "        print(\"No rows after normalization.\")\n",
        "        return\n",
        "\n",
        "    # Brief summary\n",
        "    by_chain = df.groupby(\"chain\").agg(count=(\"tx_hash\", \"count\"), usd=(\"usd_value\", \"sum\")).sort_values(\"usd\", ascending=False)\n",
        "    display(by_chain)\n",
        "    display(df.head(25))\n",
        "\n",
        "\n",
        "run_button.on_click(on_run_click)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export helpers\n",
        "import pathlib\n",
        "\n",
        "def export_latest_csv(prefix: str = \"arkham_transfers\") -> Optional[str]:\n",
        "    global LATEST_DF, LATEST_ENTITY\n",
        "    if LATEST_DF is None or LATEST_DF.empty:\n",
        "        print(\"Nothing to export. Run a query first.\")\n",
        "        return None\n",
        "    ent_name = (LATEST_ENTITY.get(\"name\") if isinstance(LATEST_ENTITY, dict) else \"entity\") or \"entity\"\n",
        "    safe_ent = str(ent_name).lower().replace(\" \", \"_\")[:64]\n",
        "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%S\")\n",
        "    outdir = pathlib.Path(\"notebooks/adhoc/csv_outputs\")\n",
        "    outdir.mkdir(parents=True, exist_ok=True)\n",
        "    outpath = outdir / f\"{prefix}-{safe_ent}-{ts}.csv\"\n",
        "    LATEST_DF.to_csv(outpath, index=False)\n",
        "    print(f\"Saved: {outpath}\")\n",
        "    return str(outpath)\n",
        "\n",
        "# Convenience button\n",
        "export_button = widgets.Button(description=\"Export CSV\", button_style=\"\")\n",
        "\n",
        "def on_export_click(_):\n",
        "    export_latest_csv()\n",
        "\n",
        "export_button.on_click(on_export_click)\n",
        "display(export_button)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
