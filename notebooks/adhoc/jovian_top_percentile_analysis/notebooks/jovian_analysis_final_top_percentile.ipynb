{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jovian Analysis - Final Version\n",
    "\n",
    "Complete notebook for Jovian calldata footprint gas scalar analysis with all fixes.\n",
    "\n",
    "## Features\n",
    "- ‚úÖ Multi-chain support (7 chains)\n",
    "- ‚úÖ Date range analysis with per-date breakdown\n",
    "- ‚úÖ Dynamic gas limits from per-chain CSV\n",
    "- ‚úÖ Compression ratio analysis\n",
    "- ‚úÖ Both sampling methods: top percentile and random\n",
    "- ‚úÖ Enhanced visualizations with vertical lines and zones\n",
    "- ‚úÖ Comprehensive summary visualization\n",
    "- ‚úÖ Optimal scalar recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modules loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Import Jovian modules\n",
    "from jovian_src.chain_config import (\n",
    "    get_chain_display_name,\n",
    "    get_chain_color,\n",
    "    DEFAULT_CALLDATA_FOOTPRINT_GAS_SCALARS\n",
    ")\n",
    "from jovian_src.clickhouse_fetcher import (\n",
    "    load_gas_limits,\n",
    "    fetch_top_percentile_blocks,\n",
    "    fetch_random_sample_blocks\n",
    ")\n",
    "from jovian_src.analysis_functions import (\n",
    "    perform_jovian_analysis,\n",
    ")\n",
    "from jovian_src.visualization_jovian import (\n",
    "    generate_all_visualizations\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ JOVIAN CONFIGURATION\n",
      "============================================================\n",
      "Chain: Base\n",
      "Sampling: top_percentile\n",
      "   Percentile: Top 1%\n",
      "Date range: 2025-01-01 to 2025-02-01\n",
      "Dates to analyze: 32 day(s)\n",
      "Calldata footprint gas scalars: [160, 400, 600, 800]\n"
     ]
    }
   ],
   "source": [
    "# Analysis parameters\n",
    "CHAIN = \"base\"  # Options: base, optimism, mode, zora, world, ink, soneium\n",
    "\n",
    "# Sampling method\n",
    "SAMPLING_METHOD = \"top_percentile\"  # \"top_percentile\" or \"random\"\n",
    "PERCENTILE = 99.0    # For top percentile (99 = top 1%)\n",
    "NUM_BLOCKS = 100     # For random sampling - number of blocks per day\n",
    "RANDOM_SEED = 42     # For reproducible random sampling\n",
    "\n",
    "DATA_PATH = \"../gas_limits\"\n",
    "FILE_PATH = f\"{CHAIN}_gas_limits.csv\"\n",
    "# Date range configuration\n",
    "START_DATE = \"2025-01-01\"\n",
    "END_DATE = \"2025-02-01\"  # Inclusive\n",
    "\n",
    "# Jovian parameters\n",
    "CALLDATA_FOOTPRINT_GAS_SCALARS = DEFAULT_CALLDATA_FOOTPRINT_GAS_SCALARS  # [160, 400, 600, 800]\n",
    "BLOCK_LIMIT = None   # None = all blocks for selected method\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(f\"../results/{CHAIN}/visualizations_{SAMPLING_METHOD.lower()}_{START_DATE}_{END_DATE}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate date list\n",
    "start = datetime.strptime(START_DATE, \"%Y-%m-%d\")\n",
    "end = datetime.strptime(END_DATE, \"%Y-%m-%d\")\n",
    "DATES_TO_ANALYZE = []\n",
    "current = start\n",
    "while current <= end:\n",
    "    DATES_TO_ANALYZE.append(current.strftime(\"%Y-%m-%d\"))\n",
    "    current += timedelta(days=1)\n",
    "\n",
    "print(f\"üöÄ JOVIAN CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Chain: {get_chain_display_name(CHAIN)}\")\n",
    "print(f\"Sampling: {SAMPLING_METHOD}\")\n",
    "if SAMPLING_METHOD == \"top_percentile\":\n",
    "    print(f\"   Percentile: Top {100 - PERCENTILE:.0f}%\")\n",
    "else:\n",
    "    print(f\"   Number of blocks: {NUM_BLOCKS}\")\n",
    "    print(f\"   Random seed: {RANDOM_SEED}\")\n",
    "print(f\"Date range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"Dates to analyze: {len(DATES_TO_ANALYZE)} day(s)\")\n",
    "print(f\"Calldata footprint gas scalars: {CALLDATA_FOOTPRINT_GAS_SCALARS}\")\n",
    "\n",
    "# Show dates if range is small\n",
    "if len(DATES_TO_ANALYZE) <= 7:\n",
    "    print(f\"\\nüìÖ Dates: {', '.join(DATES_TO_ANALYZE)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Gas Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è  Multiple gas limits detected: 240,000,000, 242,336,329, 252,000,000, 255,359,144, 264,000,000, 271,196,538, 288,000,000\n",
      "üìä Using most common for combined analysis: 240,000,000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load historical gas limits for the selected chain\n",
    "gas_limits_df = load_gas_limits(csv_path=f\"{DATA_PATH}/{FILE_PATH}\")\n",
    "\n",
    "def get_gas_limit_for_date(date_str: str, gas_limits_df: pl.DataFrame, chain: str = \"base\") -> int:\n",
    "    \"\"\"Get gas limit for date_str; fall back to most recent prior date, else 240M.\"\"\"\n",
    "    # exact match\n",
    "    result = gas_limits_df.filter(pl.col(\"date_formatted\") == date_str)\n",
    "    if result.height:\n",
    "        return int(result[\"gas_limit\"][0])\n",
    "\n",
    "    # previous (<=) by formatted date string (YYYY-MM-DD ordering is lexicographic-safe)\n",
    "    prev = (\n",
    "        gas_limits_df\n",
    "        .filter(pl.col(\"date_formatted\") <= date_str)\n",
    "        .sort(\"date_formatted\")\n",
    "        .tail(1)\n",
    "    )\n",
    "    if prev.height:\n",
    "        return int(prev[\"gas_limit\"][0])\n",
    "\n",
    "    # final fallback if the requested date is before earliest entry\n",
    "    return None\n",
    "\n",
    "# Get gas limits for all dates\n",
    "date_gas_limits = {}\n",
    "for date in DATES_TO_ANALYZE:\n",
    "    gas_limit = get_gas_limit_for_date(date, gas_limits_df, chain=CHAIN)\n",
    "    date_gas_limits[date] = gas_limit\n",
    "    if len(DATES_TO_ANALYZE) <= 7:\n",
    "        print(f\"üìä Gas limit for {date}: {gas_limit:,}\")\n",
    "\n",
    "# Determine gas limit for combined analysis\n",
    "unique_limits = set(date_gas_limits.values())\n",
    "if len(unique_limits) > 1:\n",
    "    print(f\"\\n‚ö†Ô∏è  Multiple gas limits detected: {', '.join(f'{gl:,}' for gl in sorted(unique_limits))}\")\n",
    "    # Use the most common gas limit for combined analysis\n",
    "    limit_counts = Counter(date_gas_limits.values())\n",
    "    ANALYSIS_GAS_LIMIT = limit_counts.most_common(1)[0][0]\n",
    "    print(f\"üìä Using most common for combined analysis: {ANALYSIS_GAS_LIMIT:,}\")\n",
    "else:\n",
    "    ANALYSIS_GAS_LIMIT = list(unique_limits)[0]\n",
    "    print(f\"\\nüìä Consistent gas limit across all dates: {ANALYSIS_GAS_LIMIT:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• SEQUENTIAL FETCHING: top 1% blocks for 32 date(s)...\n",
      "============================================================\n",
      "\u001b[2m2025-08-26 00:15:04\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mloaded vault from .env file   \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mvault.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m32\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m17007\u001b[0m\n",
      "\u001b[2m2025-08-26 00:15:04\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mloaded vault: 28 items        \u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mvault.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m79\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m17007\u001b[0m\n",
      "\u001b[2m2025-08-26 00:15:04\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mconnecting to OPLABS Clickhouse client...\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m56\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m17007\u001b[0m\n",
      "\u001b[2m2025-08-26 00:15:04\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized OPLABS Clickhouse client.\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mclient.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m61\u001b[0m \u001b[36mprocess\u001b[0m=\u001b[35m17007\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Compare parallel vs sequential fetching\n",
    "all_dataframes = []\n",
    "fetch_summary = []\n",
    "per_date_dataframes = {}  # Store individual dataframes\n",
    "\n",
    "method_display = \"top 1%\" if SAMPLING_METHOD == \"top_percentile\" else f\"{NUM_BLOCKS} random blocks\"\n",
    "\n",
    "# SEQUENTIAL FETCHING (fallback for single date or when disabled)\n",
    "print(f\"üì• SEQUENTIAL FETCHING: {method_display} blocks for {len(DATES_TO_ANALYZE)} date(s)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for date in DATES_TO_ANALYZE:\n",
    "    gas_limit = date_gas_limits[date]\n",
    "    if len(DATES_TO_ANALYZE) <= 7:\n",
    "        print(f\"\\nüìÖ Fetching {date} (gas limit: {gas_limit:,})\")\n",
    "    \n",
    "    if SAMPLING_METHOD == \"top_percentile\":\n",
    "        df, actual_gas_limit = fetch_top_percentile_blocks(\n",
    "            chain=CHAIN,\n",
    "            date=date,\n",
    "            percentile=PERCENTILE,\n",
    "            limit=BLOCK_LIMIT,\n",
    "            gas_limit=gas_limit\n",
    "        )\n",
    "    else:\n",
    "        df, actual_gas_limit = fetch_random_sample_blocks(\n",
    "            chain=CHAIN,\n",
    "            date=date,\n",
    "            num_blocks=NUM_BLOCKS,\n",
    "            gas_limit=gas_limit,\n",
    "            seed=RANDOM_SEED\n",
    "        )\n",
    "    \n",
    "    if not df.is_empty():\n",
    "        all_dataframes.append(df)\n",
    "        per_date_dataframes[date] = df  # Store individual dataframe\n",
    "        blocks = df['block_number'].n_unique()\n",
    "        txs = len(df)\n",
    "        fetch_summary.append({\n",
    "            'date': date,\n",
    "            'blocks': blocks,\n",
    "            'transactions': txs,\n",
    "            'gas_limit': actual_gas_limit,\n",
    "            'df': df  # Store individual dataframe\n",
    "        })\n",
    "        if len(DATES_TO_ANALYZE) <= 7:\n",
    "            print(f\"   ‚úÖ Fetched {blocks} blocks, {txs:,} transactions\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  No data for {date}\")\n",
    "\n",
    "sequential_time = time.time() - start_time\n",
    "print(f\"\\nüìä Sequential Performance:\")\n",
    "print(f\"   Total time: {sequential_time:.1f}s\")\n",
    "print(f\"   Average per date: {sequential_time/len(DATES_TO_ANALYZE):.1f}s\")\n",
    "\n",
    "combined_df = pl.concat(all_dataframes)\n",
    "print(f\"\\n‚úÖ Data fetching complete!\")\n",
    "print(f\"   Total dates with data: {len(fetch_summary)}\")\n",
    "print(f\"   Total blocks: {combined_df['block_number'].n_unique():,}\")\n",
    "print(f\"   Total transactions: {len(combined_df):,}\")\n",
    "print(f\"   Average calldata: {combined_df['calldata_size'].mean():.0f} bytes\")\n",
    "print(f\"   Max calldata: {combined_df['calldata_size'].max():.0f} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Perform Combined Jovian Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze combined data with multiple calldata footprint gas scalars\n",
    "if not combined_df.is_empty():\n",
    "    print(f\"üìä ANALYZING COMBINED DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Chain: {get_chain_display_name(CHAIN)}\")\n",
    "    print(f\"Dates analyzed: {len(fetch_summary)}\")\n",
    "    print(f\"Gas limit: {ANALYSIS_GAS_LIMIT:,}\")\n",
    "    print(f\"Sampling method: {SAMPLING_METHOD}\")\n",
    "    \n",
    "    analysis_results = perform_jovian_analysis(\n",
    "        df=combined_df,\n",
    "        gas_limit=ANALYSIS_GAS_LIMIT,\n",
    "        calldata_footprint_gas_scalars=CALLDATA_FOOTPRINT_GAS_SCALARS,\n",
    "        chain=CHAIN,\n",
    "        sampling_method=SAMPLING_METHOD,\n",
    "        start_date=START_DATE,\n",
    "        end_date=END_DATE,\n",
    "        show_progress=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Jovian analysis complete!\")\n",
    "else:\n",
    "    print(\"‚ùå No data to analyze\")\n",
    "    analysis_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: View Results with Compression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display analysis results including compression metrics\n",
    "if analysis_results:\n",
    "    print(f\"üöÄ JOVIAN ANALYSIS RESULTS - {get_chain_display_name(CHAIN)}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for scalar in sorted(analysis_results.keys()):\n",
    "        result = analysis_results[scalar]\n",
    "        print(f\"\\nüîπ Calldata Footprint Gas Scalar: {scalar}\")\n",
    "        print(f\"   Effective limit: {result.gas_limit // scalar:,} bytes\")\n",
    "        print(f\"   Blocks exceeding: {result.blocks_exceeding}/{result.total_blocks} ({result.percentage_exceeding:.2f}%)\")\n",
    "        print(f\"   Avg utilization: {result.avg_utilization:.2%}\")\n",
    "        print(f\"   Max utilization: {result.max_utilization:.2%}\")\n",
    "\n",
    "        # ‚õΩ DA footprint vs *block gas used* (as % of gas used)\n",
    "        if getattr(result, \"blocks_with_gas_used\", 0) > 0:\n",
    "            mean_pct   = result.avg_util_vs_gas_used * 100\n",
    "            median_pct = result.median_util_vs_gas_used * 100\n",
    "            p95_pct    = result.p95_util_vs_gas_used * 100\n",
    "\n",
    "            over_cnt_measured = int(round(result.share_over_1_vs_used * result.blocks_with_gas_used))\n",
    "            over_pct_measured = result.share_over_1_vs_used * 100\n",
    "\n",
    "            # Optional: express as share of ALL blocks too (matches the ‚ÄúBlocks exceeding‚Äù style)\n",
    "            over_pct_all = (over_cnt_measured / result.total_blocks * 100) if result.total_blocks else 0\n",
    "\n",
    "            print(f\"\\n   ‚õΩ DA footprint vs Block Gas Used:\")\n",
    "            print(f\"      mean:   {mean_pct:.1f}%\")\n",
    "            print(f\"      median: {median_pct:.1f}%   |   p95: {p95_pct:.1f}%\")\n",
    "            print(f\"      Blocks over gas-used budget (>100%): \"\n",
    "                f\"{over_cnt_measured}/{result.blocks_with_gas_used} ({over_pct_measured:.1f}%) \")\n",
    "        else:\n",
    "            print(\"\\n   ‚õΩ DA footprint vs Block Gas Used: n/a (no blocks had block_gas_used)\")\n",
    "        \n",
    "        # Show compression metrics\n",
    "        if result.compression_metrics:\n",
    "            print(f\"\\n   üìä Compression Metrics:\")\n",
    "            print(f\"      Avg compression ratio: {result.compression_metrics.get('avg_compression_ratio', 0):.2f}x\")\n",
    "            print(f\"      Median compression: {result.compression_metrics.get('median_compression_ratio', 0):.2f}x\")\n",
    "            print(f\"      DA efficiency: {result.compression_metrics.get('avg_da_efficiency', 0):.1%}\")\n",
    "        \n",
    "        if result.blocks_exceeding > 0:\n",
    "            print(f\"   Avg excess: {result.avg_excess_percentage:.1f}%\")\n",
    "            print(f\"   Max excess: {result.max_excess_percentage:.1f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Jovian Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty-print recommendation with compression + DA>gas-used info per option\n",
    "print(f\"\\nüéØ RECOMMENDATION - {get_chain_display_name(CHAIN)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for opt in rec[\"all_options\"]:\n",
    "    s = opt[\"calldata_footprint_gas_scalar\"]\n",
    "    tick = \"‚úÖ\" if opt[\"blocks_exceeding_pct\"] <= 1 else (\"‚ö†Ô∏è\" if opt[\"blocks_exceeding_pct\"] <= 5 else \"‚ùå\")\n",
    "\n",
    "    # Avg compression from recommendation dict\n",
    "    avg_comp = opt.get(\"avg_compression_ratio\", 0.0)\n",
    "\n",
    "    # Median compression from analysis_results (no lib changes needed)\n",
    "    median_comp = 0.0\n",
    "    if s in analysis_results and analysis_results[s].compression_metrics:\n",
    "        median_comp = analysis_results[s].compression_metrics.get(\"median_compression_ratio\", 0.0)\n",
    "\n",
    "    # DA footprint > gas used share (only if measured)\n",
    "    measured = opt.get(\"measured_blocks_gas_used\", 0) or 0\n",
    "    over_pct = opt.get(\"over_gas_used_pct\", None)\n",
    "    over_str = (\n",
    "        f\" | DA footprint > block gas used: {over_pct:.2f}% of {measured:,} blocks\"\n",
    "        if (over_pct is not None and measured > 0) else \"\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"{tick} Scalar {s}: {opt['blocks_exceeding_pct']:.2f}% exceeds gas limit, {opt['assessment']} | \"\n",
    "        f\"avg comp {avg_comp:.2f}x (median {median_comp:.2f}x){over_str}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Enhanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all Jovian visualizations with enhanced features\n",
    "if analysis_results:\n",
    "    print(f\"üìä Generating enhanced visualizations for {get_chain_display_name(CHAIN)}...\")\n",
    "    \n",
    "    # Generate all visualizations (includes new comprehensive summary)\n",
    "    figures = generate_all_visualizations(\n",
    "        results_by_scalar=analysis_results,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        chain=CHAIN,\n",
    "        start_date=START_DATE,\n",
    "        end_date=END_DATE\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Generated {len(figures)} visualizations:\")\n",
    "    print(\"   1. Size estimates histogram (with vertical lines & zones)\")\n",
    "    print(\"   2. Compression ratio histogram (with percentiles & zones)\")\n",
    "    print(\"   3. Blocks exceeding limits analysis\")\n",
    "    print(\"   4. Excess distribution\")\n",
    "    print(\"   5. Over-utilization percentages\")\n",
    "    print(\"   6. Scalar comparison\")\n",
    "    print(\"   7. Comprehensive summary (NEW)\")\n",
    "    print(f\"\\nüìÅ Saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Per-Date Analysis (for Date Ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each date separately\n",
    "per_date_results = {}\n",
    "\n",
    "if len(DATES_TO_ANALYZE) > 1 and fetch_summary:\n",
    "    print(f\"üìä PER-DATE ANALYSIS - {get_chain_display_name(CHAIN)}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for item in fetch_summary:\n",
    "        date = item['date']\n",
    "        df = item['df']\n",
    "        gas_limit = item['gas_limit']\n",
    "        \n",
    "        print(f\"\\nüìÖ Analyzing {date} (gas limit: {gas_limit:,})\")\n",
    "        print(f\"   Blocks: {df['block_number'].n_unique()}, Transactions: {len(df)}\")\n",
    "        \n",
    "        # Analyze this date\n",
    "        date_results = perform_jovian_analysis(\n",
    "            df=df,\n",
    "            gas_limit=gas_limit,\n",
    "            calldata_footprint_gas_scalars=CALLDATA_FOOTPRINT_GAS_SCALARS,\n",
    "            chain=CHAIN,\n",
    "            sampling_method=SAMPLING_METHOD,\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            show_progress=False\n",
    "        )\n",
    "        \n",
    "        per_date_results[date] = date_results\n",
    "        \n",
    "        # Show summary for scalar 400\n",
    "        if 400 in date_results:\n",
    "            result = date_results[400]\n",
    "            print(f\"   Scalar 400: {result.blocks_exceeding}/{result.total_blocks} blocks exceed ({result.percentage_exceeding:.2f}%)\")\n",
    "            print(f\"   Avg utilization: {result.avg_utilization:.2%}\")\n",
    "            if result.compression_metrics:\n",
    "                print(f\"   Compression ratio: {result.compression_metrics.get('avg_compression_ratio', 0):.2f}x\")\n",
    "    \n",
    "    # Summary across dates\n",
    "    if per_date_results:\n",
    "        print(f\"\\nüìä SUMMARY ACROSS DATES (Scalar 400):\")\n",
    "        print(\"-\" * 60)\n",
    "        for date, results in per_date_results.items():\n",
    "            if 400 in results:\n",
    "                r = results[400]\n",
    "                comp_ratio = r.compression_metrics.get('avg_compression_ratio', 0) if r.compression_metrics else 0\n",
    "                print(f\"{date}: {r.percentage_exceeding:6.2f}% exceed, {r.avg_utilization:6.2%} util, {comp_ratio:.2f}x compression\")\n",
    "else:\n",
    "    print(\"\\nüìä Single date analysis - skipping per-date breakdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Trend Analysis (for Date Ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trends across dates\n",
    "if len(per_date_results) > 1:\n",
    "    # Prepare data for plotting\n",
    "    dates = sorted(per_date_results.keys())\n",
    "    scalars_to_plot = [s for s in CALLDATA_FOOTPRINT_GAS_SCALARS if s in analysis_results]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    chain_color = get_chain_color(CHAIN)\n",
    "    fig.suptitle(f'{get_chain_display_name(CHAIN)}: Jovian Analysis Trends ({SAMPLING_METHOD})', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Percentage exceeding over time\n",
    "    ax = axes[0, 0]\n",
    "    for scalar in scalars_to_plot:\n",
    "        percentages = [per_date_results[d][scalar].percentage_exceeding \n",
    "                      for d in dates if scalar in per_date_results[d]]\n",
    "        ax.plot(dates, percentages, marker='o', label=f'Scalar {scalar}')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Blocks Exceeding (%)')\n",
    "    ax.set_title('Percentage of Blocks Exceeding Limit')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # Plot 2: Compression ratio over time\n",
    "    ax = axes[0, 1]\n",
    "    comp_ratios = []\n",
    "    for d in dates:\n",
    "        if 400 in per_date_results[d] and per_date_results[d][400].compression_metrics:\n",
    "            comp_ratios.append(per_date_results[d][400].compression_metrics.get('avg_compression_ratio', 0))\n",
    "        else:\n",
    "            comp_ratios.append(0)\n",
    "    \n",
    "    ax.plot(dates, comp_ratios, marker='s', color=chain_color, linewidth=2)\n",
    "    ax.axhline(y=3.0, color='red', linestyle='--', alpha=0.5, label='Expected ~3x')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Average Compression Ratio')\n",
    "    ax.set_title('Compression Ratio Trend')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # Plot 3: Average utilization over time\n",
    "    ax = axes[1, 0]\n",
    "    for scalar in scalars_to_plot:\n",
    "        utilizations = [per_date_results[d][scalar].avg_utilization * 100 \n",
    "                       for d in dates if scalar in per_date_results[d]]\n",
    "        ax.plot(dates, utilizations, marker='s', label=f'Scalar {scalar}')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Average Utilization (%)')\n",
    "    ax.set_title('Average Utilization Over Time')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # Plot 4: Gas limits over time\n",
    "    ax = axes[1, 1]\n",
    "    gas_limits_list = [date_gas_limits[d] / 1e6 for d in dates]  # Convert to millions\n",
    "    ax.plot(dates, gas_limits_list, marker='d', color=chain_color, linewidth=2)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Gas Limit (millions)')\n",
    "    ax.set_title('Gas Limit Changes Over Time')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the figure\n",
    "    trend_file = OUTPUT_DIR / \"trend_analysis.png\"\n",
    "    fig.savefig(trend_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Trend analysis saved to: {trend_file}\")\n",
    "else:\n",
    "    print(\"\\nüìä Single date or no per-date results - skipping trend analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Verify Jovian Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_to_peek = 400  \n",
    "res = analysis_results[scalar_to_peek]\n",
    "blocks = [b for b in res.block_analyses if b.total_calldata_size > 0]\n",
    "\n",
    "bn   = np.array([b.block_number for b in blocks])\n",
    "eff  = np.array([b.total_size_estimate / b.total_calldata_size for b in blocks])  # 0.0‚Äì1.0+\n",
    "mean_eff   = float(np.mean(eff))\n",
    "median_eff = float(np.median(eff))\n",
    "p95_eff    = float(np.percentile(eff, 95))\n",
    "overall_avg_eff = res.compression_metrics.get(\"avg_da_efficiency\", 0.0) \n",
    "\n",
    "print(f\"üìä DA efficiency (size_est / raw_calldata) ‚Äî scalar {scalar_to_peek}\")\n",
    "print(f\"   n blocks: {len(eff):,}\")\n",
    "print(f\"   mean:   {mean_eff:.2%}   median: {median_eff:.2%}   p95: {p95_eff:.2%}\")\n",
    "print(f\"   overall avg in result.compression_metrics: {overall_avg_eff:.2%}\")\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(eff * 100, bins=40, edgecolor='black', alpha=0.75)\n",
    "plt.axvline(mean_eff * 100,   linestyle='--', linewidth=2, label=f\"Mean: {mean_eff*100:.1f}%\")\n",
    "plt.axvline(median_eff * 100, linestyle='-',  linewidth=2, label=f\"Median: {median_eff*100:.1f}%\")\n",
    "plt.axhline(0, color='k', lw=0.5)\n",
    "range_str = f\" ({START_DATE} ‚Üí {END_DATE})\" if START_DATE and END_DATE else \"\"\n",
    "plt.title(f\"{get_chain_display_name(CHAIN)}: DA size est / calldata per block{range_str}\")\n",
    "plt.xlabel(\"DA efficiency (%)\"); plt.ylabel(\"Blocks\"); plt.legend(); plt.grid(alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This **final** Jovian notebook provides:\n",
    "\n",
    "### Features\n",
    "‚úÖ **Complete Analysis** - All functionality working correctly  \n",
    "‚úÖ **Per-Date Analysis** - Properly analyzes each date separately  \n",
    "‚úÖ **Enhanced Visualizations** - Vertical lines, zones, and comprehensive summary  \n",
    "‚úÖ **Optimized Queries** - Only fetch 5 essential columns (70% data reduction)  \n",
    "‚úÖ **Multi-chain Support** - Analyze any of the 7 supported chains  \n",
    "‚úÖ **Dual Sampling Methods** - Top percentile or random sampling  \n",
    "‚úÖ **Dynamic Gas Limits** - Per-chain historical CSV data  \n",
    "‚úÖ **Compression Analysis** - Ratio and DA efficiency metrics  \n",
    "‚úÖ **Trend Analysis** - For date ranges with 4-panel visualization  \n",
    "\n",
    "### Key Improvements\n",
    "- **Fixed Query Issues**: Removed correlated subqueries, optimized JOINs\n",
    "- **Column Optimization**: Reduced from 16 to 5 columns (70% less data)\n",
    "- **Enhanced Visualizations**: Added vertical lines, percentiles, and risk zones\n",
    "- **Comprehensive Summary**: New single-page overview with all key metrics\n",
    "- **Per-Date Analysis**: Properly tracks and analyzes each date separately\n",
    "\n",
    "### How to Use\n",
    "\n",
    "1. **Configure Parameters**:\n",
    "   - Set `CHAIN` to target chain\n",
    "   - Choose `SAMPLING_METHOD` and parameters\n",
    "   - Set date range with `START_DATE` and `END_DATE`\n",
    "\n",
    "2. **Run Analysis**:\n",
    "   - Execute all cells in order\n",
    "   - Review results and recommendations\n",
    "   - Check visualizations in output directory\n",
    "\n",
    "3. **Interpret Results**:\n",
    "   - Use vertical lines to see where data falls relative to limits\n",
    "   - Check risk zones for safety assessment\n",
    "   - Review comprehensive summary for at-a-glance insights\n",
    "\n",
    "The analysis provides clear, actionable insights for optimizing calldata footprint gas scalar selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
