{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from op_analytics.datasources.platform_metrics.pg_daily_pull import PostgresDailyPull\n",
    "\n",
    "meta = PostgresDailyPull.fetch()\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from op_analytics.datasources.platform_metrics.prometheus_daily_pull import PrometheusDailyPull\n",
    "\n",
    "meta_prom = PrometheusDailyPull.fetch()\n",
    "meta_prom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from op_analytics.datasources.platform_metrics import execute\n",
    "import os\n",
    "\n",
    "# Override to force BQ write.\n",
    "os.environ[\"OPLABS_ENV\"] = \"prod\"\n",
    "\n",
    "result = execute.execute_pull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from op_analytics.datasources.platform_metrics import execute\n",
    "from op_analytics.coreutils.partitioned.dailydatawrite import write_to_prod\n",
    "\n",
    "with write_to_prod():\n",
    "    result = execute.execute_pull_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backfill tables [Currently troubleshooting]\n",
    "#from op_analytics.coreutils.partitioned.location import DataLocation\n",
    "\n",
    "# # Override location for local. Othersise It will default to DataLocation.LOCAL when running from laptop.\n",
    "#     def mock_location():\n",
    "#         return DataLocation.GCS\n",
    "\n",
    "import os\n",
    "\n",
    "from op_analytics.coreutils.bigquery.write import (\n",
    "    overwrite_unpartitioned_table,\n",
    ")\n",
    "from op_analytics.coreutils.logger import structlog\n",
    "\n",
    "from op_analytics.datasources.platform_metrics.pg_daily_pull import PostgresDailyPull\n",
    "from op_analytics.datasources.platform_metrics.prometheus_daily_pull import PrometheusDailyPull\n",
    "from op_analytics.coreutils.partitioned.dailydatawrite import write_to_prod\n",
    "\n",
    "import os\n",
    "from unittest.mock import patch\n",
    "\n",
    "from op_analytics.coreutils.partitioned import dailydatawrite\n",
    "from op_analytics.coreutils.partitioned.location import DataLocation\n",
    "from op_analytics.datasources.platform_metrics.dataaccess import PlatformMetrics\n",
    "\n",
    "\n",
    "with write_to_prod():\n",
    "\n",
    "    log = structlog.get_logger()\n",
    "\n",
    "    # BigQuery Dataset and Tables\n",
    "    BQ_DATASET = \"platform_metrics\"\n",
    "    ANALYTICS_TABLE_PG = \"platform_metrics_jobs_v1\"\n",
    "    ANALYTICS_TABLE_PROM = \"platform_metrics_prometheus_metrics_v1\"\n",
    "\n",
    "    # current_env = OPLabsEnvironment.PROD\n",
    "    def write_pg_to_bq(data: PostgresDailyPull):\n",
    "        \"\"\"Write to BigQuery.\n",
    "\n",
    "        This operation will be retired soon. Data will move to GCS + Clickhouse.\n",
    "        \"\"\"\n",
    "        jobs_df = data.jobs_df.rename({\"dt\": \"date\"})\n",
    "        # jobs_df = data.jobs_df_truncated.rename({\"dt\": \"date\"})\n",
    "        \n",
    "        overwrite_unpartitioned_table(\n",
    "            df=jobs_df,\n",
    "            dataset=BQ_DATASET,\n",
    "            table_name=ANALYTICS_TABLE_PG,\n",
    "        )\n",
    "\n",
    "        return {\"jobs_df\": len(jobs_df)}\n",
    "\n",
    "    def write_prom_to_bq(data: PrometheusDailyPull):\n",
    "        \"\"\"Write to BigQuery.\n",
    "\n",
    "        This operation will be retired soon. Data will move to GCS + Clickhouse.\n",
    "        \"\"\"\n",
    "        metrics_df = data.metrics_df.rename({\"dt\": \"date\"})\n",
    "        # metrics_df = data.metrics_df_truncated.rename({\"dt\": \"date\"})\n",
    "\n",
    "        overwrite_unpartitioned_table(\n",
    "            df=metrics_df,\n",
    "            dataset=BQ_DATASET,\n",
    "            table_name=ANALYTICS_TABLE_PG,\n",
    "        )\n",
    "\n",
    "        return {\"metrics_df\": len(metrics_df)}\n",
    "\n",
    "    data_pg = PostgresDailyPull.fetch()\n",
    "    data_prom = PrometheusDailyPull.fetch()\n",
    "\n",
    "    summary = {}\n",
    "    summary[\"bigquery\"] = {\"pg\": write_pg_to_bq(data_pg), \"prom\": write_prom_to_bq(data_prom)}\n",
    "    summary\n",
    "\n",
    "    metrics = PrometheusDailyPull.fetch()\n",
    "    jobs = PostgresDailyPull.fetch()\n",
    "\n",
    "\n",
    "    def overwrite_reviews(metrics, jobs):\n",
    "        \"\"\"Overwrite GCS with the backfill reviews df.\"\"\"\n",
    "        \n",
    "        # Required to allow writing data to GCS from local.\n",
    "        os.environ[\"ALLOW_WRITE\"] = \"true\"\n",
    "\n",
    "        # Override location for local. Othersise It will default to DataLocation.LOCAL when running from laptop.\n",
    "        def mock_location():\n",
    "            return DataLocation.GCS\n",
    "\n",
    "        with patch.object(dailydatawrite, \"determine_location\", mock_location):\n",
    "            PlatformMetrics.JOBS.write(\n",
    "                dataframe=metrics,\n",
    "                sort_by=[\"dt\", \"id\", \"pipeline_id\", \"workflow_id\"],\n",
    "            )\n",
    "\n",
    "            PlatformMetrics.PROMETHEUS_METRICS.write(\n",
    "                dataframe=jobs,\n",
    "                sort_by=[\"dt\", \"unix_time\"],\n",
    "            )\n",
    "\n",
    "\n",
    "    overwrite_reviews(metrics, jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unittest.mock import patch\n",
    "\n",
    "from op_analytics.coreutils.partitioned import dailydatawrite\n",
    "from op_analytics.coreutils.partitioned.location import DataLocation\n",
    "from op_analytics.datasources.platform_metrics.dataaccess import PlatformMetrics\n",
    "\n",
    "metrics = PrometheusDailyPull.fetch()\n",
    "jobs = PostgresDailyPull.fetch()\n",
    "\n",
    "\n",
    "def overwrite_reviews(metrics, jobs):\n",
    "    \"\"\"Overwrite GCS with the backfill reviews df.\"\"\"\n",
    "    \n",
    "    # Required to allow writing data to GCS from local.\n",
    "    os.environ[\"ALLOW_WRITE\"] = \"true\"\n",
    "\n",
    "    # Override location for local. Othersise It will default to DataLocation.LOCAL when running from laptop.\n",
    "    def mock_location():\n",
    "        return DataLocation.GCS\n",
    "\n",
    "    with patch.object(dailydatawrite, \"determine_location\", mock_location):\n",
    "        PlatformMetrics.JOBS.write(\n",
    "            dataframe=metrics,\n",
    "            sort_by=[\"dt\", \"id\", \"pipeline_id\", \"workflow_id\"],\n",
    "        )\n",
    "\n",
    "        PlatformMetrics.PROMETHEUS_METRICS.write(\n",
    "            dataframe=jobs,\n",
    "            sort_by=[\"dt\", \"unix_time\"],\n",
    "        )\n",
    "\n",
    "\n",
    "overwrite_reviews(metrics, jobs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
