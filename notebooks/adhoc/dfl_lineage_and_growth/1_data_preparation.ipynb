{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_preparation.py\n",
    "\n",
    "\"\"\"\n",
    "Part 1: Data Preparation\n",
    "------------------------\n",
    "Pulls raw data (TVL, metadata) from DuckDB, merges protocol info with chain alignment,\n",
    "handles token mappings, and computes misrepresentation flags. Exports 'protocol_data_cleaned.csv'.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib3\n",
    "import warnings\n",
    "\n",
    "from op_analytics.cli.subcommands.pulls.defillama.dataaccess import DefiLlama\n",
    "from op_analytics.coreutils.duckdb_inmem.client import init_client as init_duckdb\n",
    "\n",
    "urllib3.disable_warnings()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize DuckDB client\n",
    "duckdb_client = init_client = init_duckdb()\n",
    "\n",
    "# Minimum date for data pull\n",
    "MINIMUM_DATE = \"2025-01-01\"\n",
    "\n",
    "# 1) Pull protocol TVL data from DuckDB\n",
    "view_tvl_data = DefiLlama.PROTOCOLS_TOKEN_TVL.read(min_date=MINIMUM_DATE)\n",
    "df_protocol_tvl_data = duckdb_client.client.sql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "        dt,\n",
    "        protocol_slug,\n",
    "        chain,\n",
    "        token,\n",
    "        app_token_tvl,\n",
    "        app_token_tvl_usd\n",
    "    FROM {view_tvl_data}\n",
    "    \"\"\"\n",
    ").to_df()\n",
    "\n",
    "# 2) Pull protocol metadata from DuckDB\n",
    "view_meta_data = DefiLlama.PROTOCOLS_METADATA.read(min_date=MINIMUM_DATE)\n",
    "df_protocol_metadata = duckdb_client.client.sql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "        protocol_name,\n",
    "        protocol_slug,\n",
    "        protocol_category,\n",
    "        parent_protocol,\n",
    "        misrepresented_tokens\n",
    "    FROM {view_meta_data}\n",
    "    \"\"\"\n",
    ").to_df()\n",
    "\n",
    "# Convert misrepresented_tokens to int\n",
    "df_protocol_metadata[\"misrepresented_tokens\"] = df_protocol_metadata[\"misrepresented_tokens\"].map(\n",
    "    {\"True\": 1, \"False\": 0}\n",
    ").fillna(0).astype(int)\n",
    "\n",
    "df_source_tokens = pd.read_csv(\"token_mapping.csv\")\n",
    "\n",
    "# 4) Extra mappings and helper data\n",
    "UNWANTED_PATTERNS = [\n",
    "    \"-borrowed\", \"-vesting\", \"-staking\", \"-pool2\", \"-treasury\", \"-cex\",\n",
    "    \"^treasury$\", \"^borrowed$\", \"^staking$\", \"^pool2$\", \"polygon-bridge-&-staking\", \".*-cex$\"\n",
    "]\n",
    "UNWANTED_CATEGORIES = [\"CEX\", \"Chain\"]\n",
    "\n",
    "chain_alignment_map = {\n",
    "    \"Metis\": \"OP Stack Fork\",\n",
    "    \"Blast\": \"OP Stack Fork\",\n",
    "    \"Mantle\": \"OP Stack Fork\",\n",
    "    \"Zircuit\": \"OP Stack Fork\",\n",
    "    \"RSS3\": \"OP Stack Fork\",\n",
    "    \"Rollux\": \"OP Stack Fork\",\n",
    "    \"Ancient8\": \"OP Stack Fork\",\n",
    "    \"Manta\": \"OP Stack Fork\",\n",
    "    \"Cyber\": \"OP Chain\",\n",
    "    \"Mint\": \"OP Chain\",\n",
    "    \"Ham\": \"OP Chain\",\n",
    "    \"Polynomial\": \"OP Chain\",\n",
    "    \"Lisk\": \"OP Chain\",\n",
    "    \"BOB\": \"OP Chain\",\n",
    "    \"Mode\": \"OP Chain\",\n",
    "    \"World Chain\": \"OP Chain\",\n",
    "    \"Base\": \"OP Chain\",\n",
    "    \"Kroma\": \"OP Chain\",\n",
    "    \"Boba\": \"OP Chain\",\n",
    "    \"Fraxtal\": \"OP Chain\",\n",
    "    \"Optimism\": \"OP Chain\",\n",
    "    \"Shape\": \"OP Chain\",\n",
    "    \"Zora\": \"OP Chain\"\n",
    "}\n",
    "\n",
    "token_category_data = [\n",
    "    {\"token\": \"ETH\", \"token_category\": \"Native Asset\"},\n",
    "    {\"token\": \"WETH\", \"token_category\": \"Native Asset\"},\n",
    "    {\"token\": \"SOL\", \"token_category\": \"Native Asset\"},\n",
    "    {\"token\": \"WBTC\", \"token_category\": \"Wrapped Assets\"}\n",
    "]\n",
    "\n",
    "protocol_category_map = {\n",
    "    \"Dexes\": \"Dexes\",\n",
    "    \"Liquidity manager\": \"Yield\",\n",
    "    \"Derivatives\": \"Derivatives\",\n",
    "    \"Yield Aggregator\": \"Yield\",\n",
    "    \"Indexes\": \"Yield\",\n",
    "    \"Bridge\": \"Bridge\",\n",
    "    \"Leveraged Farming\": \"Yield\",\n",
    "    \"Cross Chain\": \"Bridge\",\n",
    "    \"CDP\": \"Lending\",\n",
    "    \"Farm\": \"Yield\",\n",
    "    \"Options\": \"Other Trading\",\n",
    "    \"DCA Tools\": \"Other Trading\",\n",
    "    \"Services\": \"TradFi/Fintech\",\n",
    "    \"Chain\": \"TradFi/Fintech\",\n",
    "    \"Privacy\": \"TradFi/Fintech\",\n",
    "    \"RWA\": \"TradFi/Fintech\",\n",
    "    \"Payments\": \"TradFi/Fintech\",\n",
    "    \"Launchpad\": \"TradFi/Fintech\",\n",
    "    \"Synthetics\": \"Derivatives\",\n",
    "    \"SoFi\": \"TradFi/Fintech\",\n",
    "    \"Prediction Market\": \"Other Trading\",\n",
    "    \"Token Locker\": \"Yield\",\n",
    "    \"Yield Lottery\": \"Yield\",\n",
    "    \"Algo-Stables\": \"Stablecoins\",\n",
    "    \"DEX Aggregator\": \"Dexes\",\n",
    "    \"Liquid Restaking\": \"Restaking/Liquid Restaking\",\n",
    "    \"Governance Incentives\": \"Yield\",\n",
    "    \"Restaking\": \"Restaking/Liquid Restaking\",\n",
    "    \"Liquid Staking\": \"Liquid Staking\",\n",
    "    \"Uncollateralized Lending\": \"Lending\",\n",
    "    \"Managed Token Pools\": \"Other Trading\",\n",
    "    \"Insurance\": \"TradFi/Fintech\",\n",
    "    \"NFT Marketplace\": \"Other Trading\",\n",
    "    \"NFT Lending\": \"Lending\",\n",
    "    \"Options Vault\": \"Other Trading\",\n",
    "    \"NftFi\": \"Other Trading\",\n",
    "    \"Basis Trading\": \"Other Trading\",\n",
    "    \"Bug Bounty\": \"TradFi/Fintech\",\n",
    "    \"OTC Marketplace\": \"Other Trading\",\n",
    "    \"Reserve Currency\": \"Stablecoins\",\n",
    "    \"Gaming\": \"Other\",\n",
    "    \"AI Agents\": \"TradFi/Fintech\",\n",
    "    \"Treasury Manager\": \"TradFi/Fintech\",\n",
    "    \"CDP Manager\": \"Lending\",\n",
    "    \"Decentralized Stablecoin\": \"Stablecoins\",\n",
    "    \"Restaked BTC\": \"Restaking/Liquid Restaking\",\n",
    "    \"RWA Lending\": \"Lending\",\n",
    "    \"Staking Pool\": \"Staking/Liquid Staking\",\n",
    "    \"CeDeFi\": \"TradFi/Fintech\",\n",
    "    \"Staking\": \"Staking/Liquid Staking\",\n",
    "    \"Oracle\": \"Other\",\n",
    "    \"Ponzi\": \"Other\",\n",
    "    \"Anchor BTC\": \"Other\",\n",
    "    \"Decentralized BTC\": \"Other\",\n",
    "    \"CEX\": \"Other\",\n",
    "    \"Lending\": \"Lending\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate cleaned protocol_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5) Merge metadata + TVL\n",
    "df_merged = pd.merge(\n",
    "    df_protocol_metadata.drop_duplicates(),\n",
    "    df_protocol_tvl_data.drop_duplicates(),\n",
    "    on=\"protocol_slug\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 6) Merge chain alignment info\n",
    "df_chain_alignment = pd.DataFrame(list(chain_alignment_map.items()), columns=[\"chain\", \"alignment\"])\n",
    "df_merged = pd.merge(df_merged, df_chain_alignment, on=\"chain\", how=\"left\")\n",
    "df_merged[\"alignment\"] = df_merged[\"alignment\"].fillna(\"Other\")\n",
    "\n",
    "# 7) Merge token category info\n",
    "df_token_lookup = pd.DataFrame(token_category_data)\n",
    "df_token_lookup[\"token\"] = df_token_lookup[\"token\"].str.upper()\n",
    "df_merged[\"token\"] = df_merged[\"token\"].str.upper()\n",
    "df_merged = pd.merge(df_merged, df_token_lookup, on=\"token\", how=\"left\")\n",
    "df_merged[\"token_category\"] = df_merged[\"token_category\"].fillna(\"Other\")\n",
    "\n",
    "# 8) Compute chain-level misrepresentation\n",
    "def calculate_misrepresentation_flags(df_input):\n",
    "    ref_date = df_input[\"dt\"].max() - pd.Timedelta(days=1)\n",
    "    df_flags = (\n",
    "        df_input[df_input.dt == ref_date]\n",
    "        [[\"protocol_slug\", \"chain\", \"misrepresented_tokens\", \"token\"]]\n",
    "        .groupby([\"protocol_slug\", \"chain\", \"misrepresented_tokens\"])\n",
    "        .agg(\n",
    "            token_count=(\"token\", \"nunique\"),\n",
    "            has_usdt=(\"token\", lambda x: 1 if \"USDT\" in x.values else 0)\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    df_flags[\"chain_misrepresented_tokens\"] = (\n",
    "        (df_flags[\"misrepresented_tokens\"] == 1)\n",
    "        & (df_flags[\"token_count\"] == 1)\n",
    "        & (df_flags[\"has_usdt\"] == 1)\n",
    "    ).astype(int)\n",
    "\n",
    "    return df_flags\n",
    "\n",
    "df_chain_misrep = calculate_misrepresentation_flags(df_merged)\n",
    "df_merged = pd.merge(\n",
    "    df_merged,\n",
    "    df_chain_misrep[[\"protocol_slug\", \"chain\", \"chain_misrepresented_tokens\"]],\n",
    "    on=[\"protocol_slug\", \"chain\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df_merged[\"dt\"] = pd.to_datetime(df_merged[\"dt\"])\n",
    "df_merged[\"parent_protocol\"] = df_merged[\"parent_protocol\"].astype(str).str.replace(\"parent#\", \"\", regex=False)\n",
    "\n",
    "# 9) If chain-level misrep is true, override token category\n",
    "df_merged[\"token_category_misrep\"] = np.where(\n",
    "    (df_merged[\"chain_misrepresented_tokens\"] == 1),\n",
    "    \"Misrepresented TVL\",\n",
    "    df_merged[\"token_category\"]\n",
    ")\n",
    "\n",
    "# 10) Map protocol categories to simpler classification\n",
    "df_merged[\"protocol_category_mapped\"] = df_merged[\"protocol_category\"].map(protocol_category_map, na_action=\"ignore\")\n",
    "df_merged.loc[df_merged[\"protocol_category_mapped\"].isna(), \"protocol_category_mapped\"] = df_merged[\"protocol_category\"]\n",
    "\n",
    "# 11) Merge in source token info\n",
    "df_merged = pd.merge(\n",
    "    df_merged,\n",
    "    df_source_tokens[[\"token\", \"project\", \"source_protocol\"]].drop_duplicates(),\n",
    "    on=\"token\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 12) Export final CSV\n",
    "cleaned_export_name = \"protocol_data_cleaned.csv\"\n",
    "df_merged.to_csv(cleaned_export_name, index=False)\n",
    "print(f\"Exported {df_merged.shape[0]} rows to {cleaned_export_name}\")\n",
    "\n",
    "\n",
    "## Optimize the final csv Dataframe\n",
    "df = df_merged\n",
    "df[\"dt\"] = pd.to_datetime(df[\"dt\"], errors=\"coerce\")\n",
    "df[\"dt\"] = df[\"dt\"].astype(\"category\")\n",
    "\n",
    "categorical_cols = [\n",
    "\"protocol_name\",\n",
    "\"protocol_slug\",\n",
    "\"protocol_category\",\n",
    "\"parent_protocol\",\n",
    "\"alignment\",\n",
    "\"token_category\",\n",
    "\"token_category_misrep\",\n",
    "\"protocol_category_mapped\",\n",
    "\"project\",\n",
    "\"source_protocol\",\n",
    "\"chain\",\n",
    "\"token\"\n",
    "]\n",
    "for c in categorical_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(\"category\")\n",
    "\n",
    "df[\"misrepresented_tokens\"] = df[\"misrepresented_tokens\"].astype(\"int8\")\n",
    "df[\"chain_misrepresented_tokens\"] = df[\"chain_misrepresented_tokens\"].fillna(0).astype(\"int8\")\n",
    "\n",
    "float_cols = [\"app_token_tvl\", \"app_token_tvl_usd\"]\n",
    "for fc in float_cols:\n",
    "    if fc in df.columns:\n",
    "        df[fc] = df[fc].astype(\"float32\")\n",
    "\n",
    "df.info()\n",
    "df.to_parquet(\"protocol_data_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Filtering & Aggregation for the Growth measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_filter_aggregate.py\n",
    "\n",
    "\"\"\"\n",
    "Part 2: Data Filtering & Aggregation\n",
    "------------------------------------\n",
    "Loads 'protocol_data_cleaned.csv', applies pattern/category filters, \n",
    "excludes protocols under 500k TVL in the final week, and aggregates \n",
    "cross-chain TVL by parent_protocol. Exports 'final_tvl_aggregate.csv'.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# File path to the cleaned data\n",
    "CLEANED_DATA_PATH = \"protocol_data_cleaned.csv\"\n",
    "\n",
    "ANALYSIS_START = \"2024-01-01\"\n",
    "ANALYSIS_END = \"2025-01-13\"\n",
    "\n",
    "# Patterns and categories from data_preparation.py\n",
    "UNWANTED_PATTERNS = [\n",
    "    \"-borrowed\", \"-vesting\", \"-staking\", \"-pool2\", \"-treasury\", \"-cex\",\n",
    "    \"^treasury$\", \"^borrowed$\", \"^staking$\", \"^pool2$\", \"polygon-bridge-&-staking\", \".*-cex$\"\n",
    "]\n",
    "UNWANTED_CATEGORIES = [\"CEX\", \"Chain\"]\n",
    "\n",
    "# Suppose you unify or map your protocol categories:\n",
    "SIMPLE_CATEGORY_MAP = {\n",
    "    # Core categories from the original snippet\n",
    "    \"Dexes\": \"Trading\",\n",
    "    \"Liquidity manager\": \"Yield\",\n",
    "    \"Derivatives\": \"Derivatives\",\n",
    "    \"CEX\": \"Other\",\n",
    "    \"Staking\": \"Staking/Liquid Staking\",\n",
    "    \"Farm\": \"Yield\",\n",
    "    \"CDP\": \"Lending\",\n",
    "    \"Bridge\": \"Trading\",\n",
    "    \"Chain\": \"Other\",\n",
    "    \"Lending\": \"Lending\",\n",
    "    \"Cross Chain\": \"Trading\",\n",
    "    \"Algo-Stables\": \"Stablecoins\",\n",
    "    \"Synthetics\": \"Derivatives\",\n",
    "    \"Stablecoins\": \"Stablecoins\",\n",
    "    \"Yield\": \"Yield\",\n",
    "    \"Trading\": \"Trading\",\n",
    "    \"TradFi/Fintech\": \"TradFi/Fintech\",\n",
    "    \"Other\": \"Other\",\n",
    "    \"Restaking/Liquid Restaking\": \"Restaking/Liquid Restaking\",\n",
    "    \"Liquid Staking\": \"Liquid Staking\",\n",
    "    \"Staking/Liquid Staking\": \"Staking/Liquid Staking\",\n",
    "\n",
    "    # Additional sub-categories appearing in df_tvl_agg/final_growth_df\n",
    "    \"Yield Aggregator\": \"Yield\",\n",
    "    \"RWA\": \"TradFi/Fintech\",\n",
    "    \"Indexes\": \"Yield\",\n",
    "    \"Liquid Restaking\": \"Restaking/Liquid Restaking\",  # same bucket as above\n",
    "    \"Insurance\": \"TradFi/Fintech\",\n",
    "    \"NFT Lending\": \"Lending\",\n",
    "    \"Options\": \"Trading\",\n",
    "    \"Privacy\": \"TradFi/Fintech\",\n",
    "    \"Leveraged Farming\": \"Yield\",\n",
    "    \"Prediction Market\": \"Trading\",\n",
    "    \"Payments\": \"TradFi/Fintech\",\n",
    "    \"Uncollateralized Lending\": \"Lending\",\n",
    "    \"NFT Marketplace\": \"Trading\",\n",
    "    \"Launchpad\": \"TradFi/Fintech\",\n",
    "    \"Token Locker\": \"Yield\",\n",
    "    \"Restaking\": \"Restaking/Liquid Restaking\",         # same bucket as above\n",
    "    \"Basis Trading\": \"Trading\",\n",
    "    \"DEX Aggregator\": \"Trading\",\n",
    "    \"Options Vault\": \"Trading\",\n",
    "    \"CDP Manager\": \"Lending\",\n",
    "    \"Reserve Currency\": \"Stablecoins\",\n",
    "    \"Managed Token Pools\": \"Trading\",\n",
    "    \"Yield Lottery\": \"Yield\",\n",
    "    \"Liquidity Automation\": \"Yield\",\n",
    "    \"RWA Lending\": \"Lending\",\n",
    "    \"Bug Bounty\": \"TradFi/Fintech\",\n",
    "    \"Governance Incentives\": \"Yield\",\n",
    "    \"Treasury Manager\": \"TradFi/Fintech\",\n",
    "    \"Decentralized BTC\": \"Other\",\n",
    "    \"AI Agents\": \"TradFi/Fintech\",\n",
    "    \"OTC Marketplace\": \"Trading\",\n",
    "    \"SoFi\": \"TradFi/Fintech\",\n",
    "    \"Anchor BTC\": \"Other\",\n",
    "    \"Staking Pool\": \"Staking/Liquid Staking\"\n",
    "}\n",
    "\n",
    "def does_match_unwanted_pattern(s: str) -> bool:\n",
    "    \"\"\"Check if a string hits any of the undesired patterns.\"\"\"\n",
    "    return any(re.search(pattern, s, re.IGNORECASE) for pattern in UNWANTED_PATTERNS)\n",
    "\n",
    "def exclude_small_tvl_protocols(df: pd.DataFrame, analysis_start: str, analysis_end: str, min_tvl: int = 500000):\n",
    "    \"\"\"\n",
    "    Removes protocols that remain under 500k TVL in the last 7 days\n",
    "    of the given analysis period.\n",
    "    \"\"\"\n",
    "    df[\"dt\"] = pd.to_datetime(df[\"dt\"], errors=\"coerce\")\n",
    "    start = pd.to_datetime(analysis_start)\n",
    "    end = pd.to_datetime(analysis_end)\n",
    "    protocol_names = df[\"protocol_name\"].unique()\n",
    "\n",
    "    kept_records = []\n",
    "    excluded_list = []\n",
    "\n",
    "    for proto in tqdm(protocol_names):\n",
    "        df_single = df[df[\"protocol_name\"] == proto].copy()\n",
    "        if df_single.empty:\n",
    "            excluded_list.append({\"protocol\": proto, \"reason\": \"No data\"})\n",
    "            continue\n",
    "\n",
    "        df_single = df_single[(df_single[\"dt\"] >= start) & (df_single[\"dt\"] <= end)]\n",
    "        if df_single.empty:\n",
    "            excluded_list.append({\"protocol\": proto, \"reason\": \"No data in period\"})\n",
    "            continue\n",
    "\n",
    "        df_single.set_index(\"dt\", inplace=True)\n",
    "        daily_tvl_series = df_single[\"app_token_tvl_usd\"].resample(\"D\").last()\n",
    "        last_week = daily_tvl_series.last(\"7D\")\n",
    "\n",
    "        if last_week.mean() < min_tvl:\n",
    "            excluded_list.append({\"protocol\": proto, \"reason\": \"<500k TVL last 7 days\"})\n",
    "            continue\n",
    "\n",
    "        kept_records.append(df_single.reset_index())\n",
    "\n",
    "    if kept_records:\n",
    "        final_filtered_df = pd.concat(kept_records, ignore_index=True)\n",
    "    else:\n",
    "        final_filtered_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    return final_filtered_df, excluded_list\n",
    "\n",
    "# 1. Load cleaned data\n",
    "df_main_cleaned = pd.read_csv(CLEANED_DATA_PATH)\n",
    "print(f\"Loaded df_main_cleaned with shape {df_main_cleaned.shape}\")\n",
    "\n",
    "# 2. Convert dt to datetime\n",
    "df_main_cleaned[\"dt\"] = pd.to_datetime(df_main_cleaned[\"dt\"], errors=\"coerce\")\n",
    "df_main_cleaned[\"chain\"] = df_main_cleaned[\"chain\"].astype(str)\n",
    "\n",
    "# 3. Map protocol_category to a simpler classification\n",
    "df_main_cleaned[\"protocol_category_mapped\"] = (\n",
    "    df_main_cleaned[\"protocol_category\"].map(SIMPLE_CATEGORY_MAP, na_action=\"ignore\")\n",
    ")\n",
    "df_main_cleaned.loc[\n",
    "    df_main_cleaned[\"protocol_category_mapped\"].isna(), \n",
    "    \"protocol_category_mapped\"\n",
    "] = df_main_cleaned[\"protocol_category\"]\n",
    "\n",
    "# 4. Create a helper DataFrame to mark undesired protocols\n",
    "df_chain_proto_helper = df_main_cleaned[[\"chain\", \"protocol_slug\", \"protocol_category\"]].drop_duplicates()\n",
    "df_chain_proto_helper[\"unwanted_mark\"] = (\n",
    "    df_chain_proto_helper[\"chain\"].apply(does_match_unwanted_pattern)\n",
    "    | df_chain_proto_helper[\"protocol_slug\"].str.endswith(\"-cex\")\n",
    "    | df_chain_proto_helper[\"protocol_slug\"].eq(\"polygon-bridge-&-staking\")\n",
    "    | df_chain_proto_helper[\"protocol_category\"].isin(UNWANTED_CATEGORIES)\n",
    ").astype(int)\n",
    "\n",
    "# 5. Example chain selection logic: keep only Ethereum \n",
    "df_chain_proto_helper[\"chain_selected\"] = (df_chain_proto_helper[\"chain\"] == \"Ethereum\").astype(int)\n",
    "\n",
    "# 6. Final filter\n",
    "final_chain_mask = (\n",
    "    (df_chain_proto_helper[\"unwanted_mark\"] == 0)\n",
    "    & (df_chain_proto_helper[\"chain_selected\"] == 1)\n",
    ")\n",
    "\n",
    "# 7. Apply the filter\n",
    "df_postfilter = pd.merge(\n",
    "    df_main_cleaned,\n",
    "    df_chain_proto_helper[final_chain_mask][[\"chain\",\"protocol_slug\",\"protocol_category\"]],\n",
    "    on=[\"chain\",\"protocol_slug\",\"protocol_category\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "print(f\"df_postfilter shape: {df_postfilter.shape}\")\n",
    "\n",
    "# 8. Aggregate by (parent_protocol, dt) => cross-chain TVL\n",
    "df_rollup = (\n",
    "    df_postfilter.groupby([\"parent_protocol\",\"dt\"], as_index=False)\n",
    "    .agg(\n",
    "        app_token_tvl_usd=(\"app_token_tvl_usd\",\"sum\"),\n",
    "        app_token_tvl=(\"app_token_tvl\",\"sum\"),\n",
    "        chains_count=(\"chain\",\"nunique\"),\n",
    "        protocol_category=(\"protocol_category_mapped\", lambda x: x.mode()[0] if not x.mode().empty else x.iloc[0])\n",
    "    )\n",
    "    .sort_values(by=\"app_token_tvl_usd\", ascending=False)\n",
    ")\n",
    "df_rollup.rename(columns={\"parent_protocol\":\"protocol_name\"}, inplace=True)\n",
    "print(f\"df_rollup shape: {df_rollup.shape}\")\n",
    "\n",
    "# 9. Exclude protocols < 500k TVL in final week\n",
    "df_final_tvl, excluded_list = exclude_small_tvl_protocols(\n",
    "    df=df_rollup,\n",
    "    analysis_start=ANALYSIS_START,\n",
    "    analysis_end=ANALYSIS_END\n",
    ")\n",
    "print(f\"Final df_final_tvl shape: {df_final_tvl.shape}\")\n",
    "print(f\"Excluded {len(excluded_list)} protocols due to filters or low TVL.\")\n",
    "\n",
    "# 10. Save final aggregated dataset\n",
    "df_final_tvl.to_csv(\"final_tvl_aggregate.csv\", index=False)\n",
    "print(\"Saved final_tvl_aggregate.csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Growth Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# growth_calculations.py\n",
    "\n",
    "\"\"\"\n",
    "Part 3: Growth Calculations\n",
    "---------------------------\n",
    "Loads 'final_tvl_aggregate.csv', computes monthly logistic growth rate, \n",
    "MoM growth, and overall percent growth. Exports 'protocol_growth_metrics.csv'.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "FINAL_AGGREGATE_PATH = \"final_tvl_aggregate.csv\"\n",
    "GROWTH_OUTPUT_PATH = \"protocol_growth_metrics.csv\"\n",
    "\n",
    "df_tvl_agg = pd.read_csv(FINAL_AGGREGATE_PATH)\n",
    "print(f\"Loaded df_tvl_agg: {df_tvl_agg.shape}\")\n",
    "\n",
    "def extract_monthly_tvl(df_protocol_slice, start_period, months_count):\n",
    "    \"\"\"\n",
    "    Resamples TVL at monthly intervals. \n",
    "    Returns a Series indexed by each month.\n",
    "    \"\"\"\n",
    "    df_protocol_slice = df_protocol_slice.set_index(\"dt\")\n",
    "    monthly_series = df_protocol_slice[\"app_token_tvl_usd\"].resample(\"M\").last()\n",
    "    full_month_range = pd.date_range(start=start_period, periods=months_count, freq=\"M\")\n",
    "    monthly_series = monthly_series.reindex(full_month_range)\n",
    "    return monthly_series\n",
    "\n",
    "def fit_logistic_growth(monthly_values):\n",
    "    \"\"\"\n",
    "    Fits logistic curve: K / (1 + exp(-r(x - t0))) \n",
    "    and returns the 'r' (growth rate).\n",
    "    \"\"\"\n",
    "    monthly_values = monthly_values.dropna()\n",
    "    if len(monthly_values) < 3:\n",
    "        return None\n",
    "    time_index = np.arange(len(monthly_values))\n",
    "    y_vals = monthly_values.values\n",
    "\n",
    "    guess_k = max(y_vals)\n",
    "    guess_r = 1.0\n",
    "    guess_t0 = np.median(time_index)\n",
    "    try:\n",
    "        popt, _ = curve_fit(\n",
    "            lambda x, K, r, t0: K / (1 + np.exp(-r*(x - t0))),\n",
    "            time_index, y_vals,\n",
    "            p0=[guess_k, guess_r, guess_t0],\n",
    "            maxfev=10000\n",
    "        )\n",
    "        return popt[1]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def calculate_percent_growth(monthly_values):\n",
    "    monthly_values = monthly_values.dropna()\n",
    "    if len(monthly_values) < 2 or monthly_values.iloc[0] == 0:\n",
    "        return None\n",
    "    return (monthly_values.iloc[-1] - monthly_values.iloc[0]) / monthly_values.iloc[0]\n",
    "\n",
    "def calculate_average_mom_growth(monthly_values):\n",
    "    monthly_values = monthly_values.dropna()\n",
    "    if len(monthly_values) < 2:\n",
    "        return None, None\n",
    "    diffs = monthly_values.diff().dropna()\n",
    "    avg_abs_diff = diffs.mean()\n",
    "    avg_pct_diff = (diffs / monthly_values.shift(1)).mean()\n",
    "    return avg_abs_diff, avg_pct_diff\n",
    "\n",
    "def compute_weighted_ranks(monthly_data_dict):\n",
    "    monthly_df = pd.DataFrame(monthly_data_dict)\n",
    "    rank_df = monthly_df.rank(axis=1, method=\"min\", ascending=False)\n",
    "    weights = np.log1p(np.arange(1, len(rank_df)+1))\n",
    "    weights = weights / weights.sum()\n",
    "    weighted_ranks = (rank_df * weights[:, None]).sum()\n",
    "    return weighted_ranks\n",
    "\n",
    "def analyze_growth_for_category(category_name, df_input, months_window=4, cutoff_date=None):\n",
    "    \"\"\"\n",
    "    For each protocol within a given category, resample monthly,\n",
    "    compute logistic growth, percent growth, MoM growth, etc.\n",
    "    \"\"\"\n",
    "    df_cat_slice = df_input[df_input[\"protocol_category\"] == category_name].copy()\n",
    "    df_cat_slice[\"dt\"] = pd.to_datetime(df_cat_slice[\"dt\"], errors=\"coerce\")\n",
    "    protocol_list = df_cat_slice[\"protocol_name\"].unique()\n",
    "\n",
    "    if cutoff_date:\n",
    "        cutoff_date = pd.to_datetime(cutoff_date)\n",
    "\n",
    "    excluded_info = []\n",
    "    results_data = []\n",
    "    monthly_tvl_records = {}\n",
    "\n",
    "    for proto in tqdm(protocol_list):\n",
    "        df_proto = df_cat_slice[df_cat_slice[\"protocol_name\"] == proto]\n",
    "        if df_proto.empty:\n",
    "            excluded_info.append({\"protocol\": proto, \"reason\": \"No data\"})\n",
    "            continue\n",
    "\n",
    "        if cutoff_date:\n",
    "            df_proto = df_proto[df_proto[\"dt\"] <= cutoff_date]\n",
    "        latest_date = df_proto[\"dt\"].max()\n",
    "        if pd.isna(latest_date):\n",
    "            excluded_info.append({\"protocol\": proto, \"reason\": \"No valid dt\"})\n",
    "            continue\n",
    "\n",
    "        end_period = latest_date.replace(day=1)\n",
    "        start_period = end_period - pd.DateOffset(months=months_window-1)\n",
    "        monthly_series = extract_monthly_tvl(df_proto, start_period, months_window)\n",
    "\n",
    "        if monthly_series.isna().any():\n",
    "            excluded_info.append({\"protocol\": proto, \"reason\": \"NaN in monthly TVL\"})\n",
    "            continue\n",
    "\n",
    "        monthly_tvl_records[proto] = monthly_series\n",
    "\n",
    "    # Weighted rank approach\n",
    "    average_ranks = compute_weighted_ranks(monthly_tvl_records)\n",
    "\n",
    "    for proto_name, series_vals in monthly_tvl_records.items():\n",
    "        percent_growth_val = calculate_percent_growth(series_vals)\n",
    "        if percent_growth_val is None:\n",
    "            excluded_info.append({\"protocol\": proto_name, \"reason\": \"Invalid percent growth\"})\n",
    "            continue\n",
    "\n",
    "        logistic_r_val = fit_logistic_growth(series_vals)\n",
    "        if logistic_r_val is None:\n",
    "            excluded_info.append({\"protocol\": proto_name, \"reason\": \"Invalid logistic fit\"})\n",
    "            continue\n",
    "\n",
    "        abs_mom_val, pct_mom_val = calculate_average_mom_growth(series_vals)\n",
    "        df_proto_slice = df_cat_slice[df_cat_slice[\"protocol_name\"] == proto_name]\n",
    "        chain_count_val = df_proto_slice[\"chains_count\"].iloc[0] if not df_proto_slice.empty else 1\n",
    "\n",
    "        results_data.append({\n",
    "            \"protocol_name\": proto_name,\n",
    "            \"chains_count\": chain_count_val,\n",
    "            \"tvl_percent_growth\": percent_growth_val,\n",
    "            \"avg_tvl_rank\": average_ranks[proto_name],\n",
    "            \"logistic_growth_rate\": logistic_r_val,\n",
    "            \"avg_mom_growth_abs\": abs_mom_val,\n",
    "            \"avg_mom_growth_percent\": pct_mom_val\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results_data), pd.DataFrame(excluded_info)\n",
    "\n",
    "def process_growth_for_all_categories(df_input, months_span=6, cutoff_date=None):\n",
    "    \"\"\"\n",
    "    Iterates through all protocol categories, collects growth stats \n",
    "    into a combined DataFrame.\n",
    "    \"\"\"\n",
    "    df_input[\"dt\"] = pd.to_datetime(df_input[\"dt\"], errors=\"coerce\")\n",
    "    if cutoff_date:\n",
    "        cutoff_date = pd.to_datetime(cutoff_date)\n",
    "    else:\n",
    "        cutoff_date = df_input[\"dt\"].max()\n",
    "\n",
    "    unique_categories = df_input[\"protocol_category\"].unique()\n",
    "    print(\"Computing growth measures for categories:\", unique_categories)\n",
    "\n",
    "    combined_results_list = []\n",
    "    combined_excluded_list = []\n",
    "\n",
    "    for cat_item in unique_categories:\n",
    "        print(f\"\\nCategory => {cat_item}\")\n",
    "        cat_df, cat_excluded = analyze_growth_for_category(\n",
    "            category_name=cat_item,\n",
    "            df_input=df_input,\n",
    "            months_window=months_span,\n",
    "            cutoff_date=cutoff_date\n",
    "        )\n",
    "        cat_df[\"protocol_category\"] = cat_item\n",
    "\n",
    "        if not cat_df.empty:\n",
    "            combined_results_list.append(cat_df)\n",
    "        if not cat_excluded.empty:\n",
    "            combined_excluded_list.append(cat_excluded)\n",
    "\n",
    "    if combined_results_list:\n",
    "        df_growth = pd.concat(combined_results_list, ignore_index=True)\n",
    "    else:\n",
    "        df_growth = pd.DataFrame()\n",
    "\n",
    "    if combined_excluded_list:\n",
    "        df_excluded = pd.concat(combined_excluded_list, ignore_index=True)\n",
    "    else:\n",
    "        df_excluded = pd.DataFrame()\n",
    "\n",
    "    return df_growth, df_excluded\n",
    "\n",
    "# def main():\n",
    "df_tvl_agg[\"dt\"] = pd.to_datetime(df_tvl_agg[\"dt\"], errors=\"coerce\")\n",
    "final_growth_df, excluded_df = process_growth_for_all_categories(\n",
    "    df_input=df_tvl_agg,\n",
    "    months_span=6,\n",
    "    cutoff_date=\"2024-12-01\"\n",
    ")\n",
    "print(\"Calculated growth measures. Sample:\")\n",
    "print(final_growth_df.head())\n",
    "\n",
    "# Save\n",
    "final_growth_df.to_csv(GROWTH_OUTPUT_PATH, index=False)\n",
    "print(f\"Saved '{GROWTH_OUTPUT_PATH}' with shape:\", final_growth_df.shape)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tvl_agg[\"protocol_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_growth_df[\"protocol_category\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
