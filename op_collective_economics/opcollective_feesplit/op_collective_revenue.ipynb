{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../helper_functions\")\n",
    "import duneapi_utils as du\n",
    "import pandas_utils as p\n",
    "import google_bq_utils as bqu\n",
    "sys.path.pop()\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'op_collective_revenue_transactions'\n",
    "\n",
    "trailing_days = 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     dune runs\n",
      "Results available at https://dune.com/queries/3046107?trailing_days=365\n",
      "getting latest result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 14:30:01,661 INFO dune_client.api.base executing 3046107 on medium cluster\n",
      "2025-11-03 14:30:01,969 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.PENDING (queue position: None)\n",
      "2025-11-03 14:30:03,345 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.PENDING (queue position: None)\n",
      "2025-11-03 14:30:04,492 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:05,634 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:06,797 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:07,949 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:09,102 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:10,243 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:11,401 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:12,558 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:13,691 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:14,852 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:16,007 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:17,199 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:18,370 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:19,528 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:20,679 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:21,818 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:22,979 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:24,124 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:25,263 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:26,417 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:27,575 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:28,748 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:29,961 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:31,107 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:32,260 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:33,406 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:34,561 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:35,729 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:36,894 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:38,067 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:39,223 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:40,376 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:41,519 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:42,688 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:43,832 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:44,960 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:46,110 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:47,285 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:48,546 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:49,689 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:50,842 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:51,996 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:53,150 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:54,303 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:55,461 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:56,605 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:57,774 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:30:58,924 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:00,100 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:01,257 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:02,404 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:03,541 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:04,744 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:06,086 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:07,256 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:08,479 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:09,697 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:10,834 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:12,004 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:13,167 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:14,314 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:15,463 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:16,617 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:17,816 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:18,957 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:20,167 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:21,321 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:22,851 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:23,992 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:25,147 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:26,312 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:27,487 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:28,721 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:29,868 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:31,008 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:32,159 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:33,287 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:34,443 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:35,588 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:36,727 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:37,866 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:39,015 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:40,170 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:41,455 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:42,594 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:43,743 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:44,906 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:46,056 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:47,203 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:48,386 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:49,555 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:50,700 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:51,850 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:52,994 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:54,144 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:55,291 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:56,461 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:57,615 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:58,769 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:31:59,922 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:01,082 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:02,224 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:03,397 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:04,546 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:05,708 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:06,865 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:08,022 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:09,165 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:10,312 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:11,467 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:12,631 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:13,782 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:14,941 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:16,103 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:17,239 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:18,387 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:19,565 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:20,744 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:21,886 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:23,030 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:24,274 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:25,441 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:26,650 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:27,792 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:28,936 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:30,092 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:31,230 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:32,521 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:33,672 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:34,837 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:35,993 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:37,141 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:38,271 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:39,408 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:40,564 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:41,724 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:42,868 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:44,014 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:45,182 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:46,341 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:47,495 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:48,660 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n",
      "2025-11-03 14:32:49,802 INFO dune_client.api.base waiting for query execution 01K95K59GW0GEYDSWTYJXTZXEY to complete: ExecutionState.EXECUTING\n"
     ]
    }
   ],
   "source": [
    "# Run Dune\n",
    "print('     dune runs')\n",
    "days_param = du.generate_query_parameter(input=trailing_days,field_name='trailing_days',dtype='number')\n",
    "dune_df = du.get_dune_data(query_id = 3046107, #https://dune.com/queries/3046107\n",
    "    name = table_name,\n",
    "    path = \"outputs\",\n",
    "    performance=\"medium\",\n",
    "    params = [days_param],\n",
    "    num_hours_to_rerun=4\n",
    ")\n",
    "dune_df['source'] = 'dune'\n",
    "dune_df['tx_block_time'] = pd.to_datetime(dune_df['tx_block_time']).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dune_df['value'] = dune_df['value'].astype(str)\n",
    "dune_df['chain_id'] = dune_df['chain_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect duplicates for BigQuery merge keys\n",
    "\n",
    "Use the cell below to find rows and key groups that duplicate the unique key `['tx_block_number', 'tx_hash', 'trace_address']`. It prints a summary, previews the largest duplicate groups, and writes CSVs to `outputs/` for deeper inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate keys and save details for review\n",
    "unique_keys = ['tx_block_number', 'tx_hash', 'trace_address']\n",
    "\n",
    "# Rows participating in duplicate keys\n",
    "duplicate_mask = dune_df.duplicated(subset=unique_keys, keep=False)\n",
    "duplicate_rows = dune_df.loc[duplicate_mask].copy()\n",
    "\n",
    "# Group sizes for duplicate key groups\n",
    "group_sizes = (\n",
    "    duplicate_rows\n",
    "    .groupby(unique_keys, dropna=False)\n",
    "    .size()\n",
    "    .reset_index(name='duplicate_count')\n",
    "    .sort_values('duplicate_count', ascending=False)\n",
    ")\n",
    "\n",
    "print(f\"Total rows: {len(dune_df):,}\")\n",
    "print(f\"Rows with duplicated keys: {len(duplicate_rows):,}\")\n",
    "print(f\"Number of duplicate key groups: {group_sizes.shape[0]:,}\")\n",
    "\n",
    "# Preview the largest duplicate groups\n",
    "try:\n",
    "    display(group_sizes.head(20))\n",
    "except NameError:\n",
    "    print(group_sizes.head(20))\n",
    "\n",
    "# Preview rows from the largest duplicate group (if any)\n",
    "if not group_sizes.empty:\n",
    "    first_group = group_sizes.iloc[0]\n",
    "    example_filter = (\n",
    "        (dune_df['tx_block_number'] == first_group['tx_block_number']) &\n",
    "        (dune_df['tx_hash'] == first_group['tx_hash']) &\n",
    "        (dune_df['trace_address'] == first_group['trace_address'])\n",
    "    )\n",
    "    example_rows = dune_df.loc[example_filter]\n",
    "    print(\"\\nExample rows from the largest duplicate group:\")\n",
    "    try:\n",
    "        display(example_rows)\n",
    "    except NameError:\n",
    "        print(example_rows)\n",
    "\n",
    "# Export CSVs for deeper inspection\n",
    "outputs_dir = 'outputs'\n",
    "os.makedirs(outputs_dir, exist_ok=True)\n",
    "group_sizes.to_csv(os.path.join(outputs_dir, 'op_collective_revenue_duplicate_groups.csv'), index=False)\n",
    "duplicate_rows.to_csv(os.path.join(outputs_dir, 'op_collective_revenue_duplicate_rows.csv'), index=False)\n",
    "print(f\"Saved: {os.path.join(outputs_dir, 'op_collective_revenue_duplicate_groups.csv')}\")\n",
    "print(f\"Saved: {os.path.join(outputs_dir, 'op_collective_revenue_duplicate_rows.csv')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 14:32:51,604 INFO root Attempt 1: Using service account credentials\n",
      "2025-11-03 14:32:51,604 WARNING root Service account init failed on attempt 1: [Errno 2] No such file or directory: '/Users/michaelsilberling/Documents/bq/oplabs-tools-data-0842f34ceb69.json'\n",
      "2025-11-03 14:32:51,605 INFO root Attempt 1: Using ADC/OIDC login\n",
      "2025-11-03 14:32:53,351 INFO root Attempt 1: Using service account credentials\n",
      "2025-11-03 14:32:53,352 WARNING root Service account init failed on attempt 1: [Errno 2] No such file or directory: '/Users/michaelsilberling/Documents/bq/oplabs-tools-data-0842f34ceb69.json'\n",
      "2025-11-03 14:32:53,352 INFO root Attempt 1: Using ADC/OIDC login\n",
      "2025-11-03 14:32:54,421 INFO root Attempt 1: Using service account credentials\n",
      "2025-11-03 14:32:54,422 WARNING root Service account init failed on attempt 1: [Errno 2] No such file or directory: '/Users/michaelsilberling/Documents/bq/oplabs-tools-data-0842f34ceb69.json'\n",
      "2025-11-03 14:32:54,422 INFO root Attempt 1: Using ADC/OIDC login\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'op_collective_revenue_transactions_staging' deleted successfully from dataset 'api_table_uploads'.\n",
      "Start Writing api_table_uploads.op_collective_revenue_transactions_staging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1 loaded successfully to api_table_uploads.op_collective_revenue_transactions_staging\n",
      "All data loaded successfully to api_table_uploads.op_collective_revenue_transactions_staging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 14:33:00,639 ERROR google_bq_utils Error during append_and_upsert_df_to_bq_table on attempt 1: 400 GET https://bigquery.googleapis.com/bigquery/v2/projects/oplabs-tools-data/queries/7651df20-4f54-45c8-8095-3394a7828d8d?maxResults=0&location=US&prettyPrint=false: UPDATE/MERGE must match at most one source row for each target row\n",
      "\n",
      "Location: US\n",
      "Job ID: 7651df20-4f54-45c8-8095-3394a7828d8d\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting before retry...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 14:33:01,644 INFO root Attempt 1: Using service account credentials\n",
      "2025-11-03 14:33:01,645 WARNING root Service account init failed on attempt 1: [Errno 2] No such file or directory: '/Users/michaelsilberling/Documents/bq/oplabs-tools-data-0842f34ceb69.json'\n",
      "2025-11-03 14:33:01,645 INFO root Attempt 1: Using ADC/OIDC login\n",
      "2025-11-03 14:33:02,871 INFO root Attempt 1: Using service account credentials\n",
      "2025-11-03 14:33:02,872 WARNING root Service account init failed on attempt 1: [Errno 2] No such file or directory: '/Users/michaelsilberling/Documents/bq/oplabs-tools-data-0842f34ceb69.json'\n",
      "2025-11-03 14:33:02,872 INFO root Attempt 1: Using ADC/OIDC login\n",
      "2025-11-03 14:33:03,854 INFO root Attempt 1: Using service account credentials\n",
      "2025-11-03 14:33:03,855 WARNING root Service account init failed on attempt 1: [Errno 2] No such file or directory: '/Users/michaelsilberling/Documents/bq/oplabs-tools-data-0842f34ceb69.json'\n",
      "2025-11-03 14:33:03,859 INFO root Attempt 1: Using ADC/OIDC login\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'op_collective_revenue_transactions_staging' deleted successfully from dataset 'api_table_uploads'.\n",
      "Start Writing api_table_uploads.op_collective_revenue_transactions_staging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1 loaded successfully to api_table_uploads.op_collective_revenue_transactions_staging\n",
      "All data loaded successfully to api_table_uploads.op_collective_revenue_transactions_staging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 14:33:09,617 ERROR google_bq_utils Error during append_and_upsert_df_to_bq_table on attempt 2: 400 GET https://bigquery.googleapis.com/bigquery/v2/projects/oplabs-tools-data/queries/73684fa6-0cc6-4a02-a5a5-c50bddf4003a?maxResults=0&location=US&prettyPrint=false: UPDATE/MERGE must match at most one source row for each target row\n",
      "\n",
      "Location: US\n",
      "Job ID: 73684fa6-0cc6-4a02-a5a5-c50bddf4003a\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting before retry...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 14:33:11,623 INFO root Attempt 1: Using service account credentials\n",
      "2025-11-03 14:33:11,624 WARNING root Service account init failed on attempt 1: [Errno 2] No such file or directory: '/Users/michaelsilberling/Documents/bq/oplabs-tools-data-0842f34ceb69.json'\n",
      "2025-11-03 14:33:11,625 INFO root Attempt 1: Using ADC/OIDC login\n",
      "2025-11-03 14:33:12,894 INFO root Attempt 1: Using service account credentials\n",
      "2025-11-03 14:33:12,895 WARNING root Service account init failed on attempt 1: [Errno 2] No such file or directory: '/Users/michaelsilberling/Documents/bq/oplabs-tools-data-0842f34ceb69.json'\n",
      "2025-11-03 14:33:12,895 INFO root Attempt 1: Using ADC/OIDC login\n",
      "2025-11-03 14:33:13,857 INFO root Attempt 1: Using service account credentials\n",
      "2025-11-03 14:33:13,857 WARNING root Service account init failed on attempt 1: [Errno 2] No such file or directory: '/Users/michaelsilberling/Documents/bq/oplabs-tools-data-0842f34ceb69.json'\n",
      "2025-11-03 14:33:13,858 INFO root Attempt 1: Using ADC/OIDC login\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'op_collective_revenue_transactions_staging' deleted successfully from dataset 'api_table_uploads'.\n",
      "Start Writing api_table_uploads.op_collective_revenue_transactions_staging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/1 loaded successfully to api_table_uploads.op_collective_revenue_transactions_staging\n",
      "All data loaded successfully to api_table_uploads.op_collective_revenue_transactions_staging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 14:33:18,615 ERROR google_bq_utils Error during append_and_upsert_df_to_bq_table on attempt 3: 400 GET https://bigquery.googleapis.com/bigquery/v2/projects/oplabs-tools-data/queries/a6fbcc7c-fdc9-4ce1-8320-9154bea893ad?maxResults=0&location=US&prettyPrint=false: UPDATE/MERGE must match at most one source row for each target row\n",
      "\n",
      "Location: US\n",
      "Job ID: a6fbcc7c-fdc9-4ce1-8320-9154bea893ad\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on attempt 3: 400 GET https://bigquery.googleapis.com/bigquery/v2/projects/oplabs-tools-data/queries/a6fbcc7c-fdc9-4ce1-8320-9154bea893ad?maxResults=0&location=US&prettyPrint=false: UPDATE/MERGE must match at most one source row for each target row\n",
      "\n",
      "Location: US\n",
      "Job ID: a6fbcc7c-fdc9-4ce1-8320-9154bea893ad\n",
      "\n",
      "All attempts failed\n"
     ]
    },
    {
     "ename": "BadRequest",
     "evalue": "400 GET https://bigquery.googleapis.com/bigquery/v2/projects/oplabs-tools-data/queries/a6fbcc7c-fdc9-4ce1-8320-9154bea893ad?maxResults=0&location=US&prettyPrint=false: UPDATE/MERGE must match at most one source row for each target row\n\nLocation: US\nJob ID: a6fbcc7c-fdc9-4ce1-8320-9154bea893ad\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequest\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#BQ Upload\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mbqu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend_and_upsert_df_to_bq_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdune_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_keys\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtx_block_number\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtx_hash\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrace_address\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/op-analytics/op_collective_economics/opcollective_feesplit/../../helper_functions/google_bq_utils.py:465\u001b[39m, in \u001b[36mappend_and_upsert_df_to_bq_table\u001b[39m\u001b[34m(df, table_id, dataset_id, project_id, unique_keys, max_retries)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;66;03m# Execute the merge query\u001b[39;00m\n\u001b[32m    464\u001b[39m query_job = client.query(merge_query)\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[43mquery_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAppend and upsert to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m completed successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    469\u001b[39m \u001b[38;5;66;03m# Clean up staging table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/cloud/bigquery/job/query.py:1706\u001b[39m, in \u001b[36mQueryJob.result\u001b[39m\u001b[34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[39m\n\u001b[32m   1701\u001b[39m     remaining_timeout = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1704\u001b[39m     \u001b[38;5;66;03m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[39;00m\n\u001b[32m   1705\u001b[39m     \u001b[38;5;66;03m# long-running API, don't delay the next request at all.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1706\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_job_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1707\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1709\u001b[39m     \u001b[38;5;66;03m# Use a monotonic clock since we don't actually care about\u001b[39;00m\n\u001b[32m   1710\u001b[39m     \u001b[38;5;66;03m# daylight savings or similar, just the elapsed time.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/cloud/bigquery/job/query.py:1675\u001b[39m, in \u001b[36mQueryJob.result.<locals>.is_job_done\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1669\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1671\u001b[39m \u001b[38;5;66;03m# Call jobs.getQueryResults with max results set to 0 just to\u001b[39;00m\n\u001b[32m   1672\u001b[39m \u001b[38;5;66;03m# wait for the query to finish. Unlike most methods,\u001b[39;00m\n\u001b[32m   1673\u001b[39m \u001b[38;5;66;03m# jobs.getQueryResults hangs as long as it can to ensure we\u001b[39;00m\n\u001b[32m   1674\u001b[39m \u001b[38;5;66;03m# know when the query has finished as soon as possible.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1675\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reload_query_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mreload_query_results_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1677\u001b[39m \u001b[38;5;66;03m# Even if the query is finished now according to\u001b[39;00m\n\u001b[32m   1678\u001b[39m \u001b[38;5;66;03m# jobs.getQueryResults, we'll want to reload the job status if\u001b[39;00m\n\u001b[32m   1679\u001b[39m \u001b[38;5;66;03m# it's not already DONE.\u001b[39;00m\n\u001b[32m   1680\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/cloud/bigquery/job/query.py:1467\u001b[39m, in \u001b[36mQueryJob._reload_query_results\u001b[39m\u001b[34m(self, retry, timeout, page_size, start_index)\u001b[39m\n\u001b[32m   1464\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transport_timeout, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m)):\n\u001b[32m   1465\u001b[39m         transport_timeout = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1467\u001b[39m \u001b[38;5;28mself\u001b[39m._query_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_query_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1472\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1473\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransport_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1476\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/cloud/bigquery/client.py:2113\u001b[39m, in \u001b[36mClient._get_query_results\u001b[39m\u001b[34m(self, job_id, retry, project, timeout_ms, location, timeout, page_size, start_index)\u001b[39m\n\u001b[32m   2109\u001b[39m \u001b[38;5;66;03m# This call is typically made in a polling loop that checks whether the\u001b[39;00m\n\u001b[32m   2110\u001b[39m \u001b[38;5;66;03m# job is complete (from QueryJob.done(), called ultimately from\u001b[39;00m\n\u001b[32m   2111\u001b[39m \u001b[38;5;66;03m# QueryJob.result()). So we don't need to poll here.\u001b[39;00m\n\u001b[32m   2112\u001b[39m span_attributes = {\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m: path}\n\u001b[32m-> \u001b[39m\u001b[32m2113\u001b[39m resource = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspan_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBigQuery.getQueryResults\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspan_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspan_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2121\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _QueryResults.from_api_repr(resource)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/cloud/bigquery/client.py:861\u001b[39m, in \u001b[36mClient._call_api\u001b[39m\u001b[34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[39m\n\u001b[32m    857\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m span_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    858\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m create_span(\n\u001b[32m    859\u001b[39m         name=span_name, attributes=span_attributes, client=\u001b[38;5;28mself\u001b[39m, job_ref=job_ref\n\u001b[32m    860\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m call()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.11/site-packages/google/cloud/_http/__init__.py:494\u001b[39m, in \u001b[36mJSONConnection.api_request\u001b[39m\u001b[34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[39m\n\u001b[32m    482\u001b[39m response = \u001b[38;5;28mself\u001b[39m._make_request(\n\u001b[32m    483\u001b[39m     method=method,\n\u001b[32m    484\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m     extra_api_info=extra_api_info,\n\u001b[32m    491\u001b[39m )\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m200\u001b[39m <= response.status_code < \u001b[32m300\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_http_response(response)\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m expect_json \u001b[38;5;129;01mand\u001b[39;00m response.content:\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n",
      "\u001b[31mBadRequest\u001b[39m: 400 GET https://bigquery.googleapis.com/bigquery/v2/projects/oplabs-tools-data/queries/a6fbcc7c-fdc9-4ce1-8320-9154bea893ad?maxResults=0&location=US&prettyPrint=false: UPDATE/MERGE must match at most one source row for each target row\n\nLocation: US\nJob ID: a6fbcc7c-fdc9-4ce1-8320-9154bea893ad\n"
     ]
    }
   ],
   "source": [
    "#BQ Upload\n",
    "bqu.append_and_upsert_df_to_bq_table(dune_df, table_name, unique_keys = ['tx_block_number','tx_hash','trace_address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
