version: 2.1

orbs:
  gcp-cli: circleci/gcp-cli@3.0.1
  utils: ethereum-optimism/circleci-utils@0.0.12

parameters:
  run_job:
    type: enum
    default: "select_job"
    enum:
      [
        "select_job",
        "run_daily_api_upload",
        "daily_l2_aggregate_later_loads",
        "daily_l2_chain_data_uploads",
        "deploy_github_pages",
        "run_market_data",
        "run_op_governance_uploads",
        "run_op_stack_chain_revenue",
        "run_op_stack_metadata_upload",
        "run-python-installation-and-dependency-check",
        "run_value_locked_flows_update",
      ]

commands:
  setup-python-env:
    steps:
      - run:
          name: Install uv
          command: |
            curl -LsSf https://astral.sh/uv/install.sh | sh
      - run:
          name: Set up Python
          command: uv python install
      - run:
          name: Install project dependencies
          command: uv sync --all-extras --dev

  setup-python-3-10:
    steps:
      - run:
          name: Setup Python 3.10
          command: |
            if [ ! -d "~/miniconda3" ]; then
                wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
                bash Miniconda3-latest-Linux-x86_64.sh -b
            fi
            export PATH=$HOME/miniconda3/bin:$PATH
            ~/miniconda3/bin/conda create -n py310 python=3.10 -y
            echo "Python 3.10 environment setup complete"

  checkout-dep-1:
    parameters:
      base_git_project:
        type: string
        default: "."
    steps:
      - run:
          name: Clone repository and checkout branch with depth 1
          command: |
            git clone --depth 1 "$CIRCLE_REPOSITORY_URL" --branch "$CIRCLE_BRANCH" << parameters.base_git_project >>
            echo "Cloned repository folder name: << parameters.base_git_project >>"
  setup-python-3-10-project-dependencies:
    parameters:
      base_git_project:
        type: string
        default: "."
    steps:
      - run:
          name: Install Python 3.10 dependencies
          command: |
            cd "<< parameters.base_git_project >>"
            source ~/miniconda3/bin/activate py310
            python --version
            python -m pip install pipenv
            python -m pipenv install

jobs:
  python-installation-and-dependency-check:
    machine:
      image: ubuntu-2004:current
    steps:
      - checkout-dep-1:
          base_git_project: "op-analytics"
      - setup-python-3-10
      - setup-python-3-10-project-dependencies:
          base_git_project: "op-analytics"
      - run:
          name: Show dependency graph
          command: |
            cd "op-analytics"
            source ~/miniconda3/bin/activate py310
            pipenv graph
      - run:
          name: Check for security vulnerabilities
          command: |
            cd "op-analytics"
            source ~/miniconda3/bin/activate py310
            pipenv check || echo "Pipenv check failed, but continuing workflow"
      - run:
          name: Show installed packages
          command: |
            cd "op-analytics"
            source ~/miniconda3/bin/activate py310
            pipenv run pip list
      - run:
          name: Check for dependency conflicts
          command: |
            cd "op-analytics"
            source ~/miniconda3/bin/activate py310
            echo "Checking for potential conflicts..."
            pipenv graph | grep -E '\w+==.+\s+\w+==.+' || echo "No obvious conflicts found in dependency graph"

            echo "Packages with multiple versions:"
            pipenv graph | awk -F'==' '{print $1}' | sort | uniq -c | sort -nr | awk '$1 > 1 {print $0}'
      - run:
          name: Show environment information
          command: |
            cd "op-analytics"
            source ~/miniconda3/bin/activate py310
            pipenv --venv
            pipenv --where
            pipenv --py
      - run:
          name: List all installed packages with versions
          command: |
            cd "op-analytics"
            source ~/miniconda3/bin/activate py310
            pipenv run pip freeze

  setup-python-3-10-job:
    machine:
      image: ubuntu-2004:current
    steps:
      - checkout-dep-1:
          base_git_project: "op-analytics"
      - setup-python-3-10
      - setup-python-3-10-project-dependencies:
          base_git_project: "op-analytics"
      - persist_to_workspace:
          root: ~/
          paths:
            - miniconda3
            - .local

  convert-and-run-notebooks-to-py:
    description: "Convert and run notebooks to Python scripts"
    machine:
      image: ubuntu-2004:current
    parameters:
      notebook_paths:
        type: string
    steps:
      - attach_workspace:
          at: ~/
      - utils/gcp-oidc-authenticate:
          project_id: GCP_PROJECT_ID
          service_account_email: GCP_SERVICE_ACCOUNT_EMAIL
          workload_identity_pool_id: GCP_WIP_ID
          workload_identity_pool_provider_id: GCP_WIP_PROVIDER_ID
      - checkout-dep-1:
          base_git_project: "op-analytics"
      - run:
          name: Generate and run Python scripts
          command: |
            export PATH=~/miniconda3/bin:$PATH
            source ~/miniconda3/bin/activate py310

            cd "op-analytics"

            notebook_path="<< parameters.notebook_paths >>.ipynb"
            py_script_path="<< parameters.notebook_paths >>.py"
            directory=$(dirname "$notebook_path")
            cd $directory

            echo "Converting $notebook_path to $py_script_path"
            python -m pipenv run jupyter nbconvert --to python $(basename ${notebook_path})

            echo "Run $py_script_path"
            python -m pipenv run python $(basename ${py_script_path})

  convert-and-run-notebooks-to-py-and-push:
    description: "Convert and run notebooks to Python scripts and push changes to GitHub"
    parameters:
      notebook-paths:
        type: string
      commit-message:
        type: string
      push:
        type: string
        default: always
      skip-ci:
        type: boolean
        default: false
    machine:
      image: ubuntu-2004:current
    steps:
      - utils/gcp-oidc-authenticate:
          project_id: GCP_PROJECT_ID
          service_account_email: GCP_SERVICE_ACCOUNT_EMAIL
          workload_identity_pool_id: GCP_WIP_ID
          workload_identity_pool_provider_id: GCP_WIP_PROVIDER_ID
      - checkout-dep-1:
          base_git_project: "op-analytics"
      - setup-python-3-10
      - setup-python-3-10-project-dependencies:
          base_git_project: "op-analytics"
      - run:
          name: Generate and run Python scripts
          command: |
            export PATH=~/miniconda3/bin:$PATH
            source ~/miniconda3/bin/activate py310

            cd "op-analytics"

            echo '<< parameters.notebook-paths >>' | jq -c '.[]' | while read -r notebook; do
              dir=$(echo $notebook | jq -r '.dir')
              notebook=$(echo $notebook | jq -r '.notebook')

              cd "$dir"

              echo "Converting ${notebook}.ipynb to ${notebook}.py"
              python -m pipenv run jupyter nbconvert --to python ${notebook}.ipynb

              echo "Run ${notebook}.py"
              python -m pipenv run python ${notebook}.py

              cd - >/dev/null
            done

      - utils/get-github-access-token:
          condition: << parameters.push >>
      - utils/github-commit-and-push-changes:
          condition: << parameters.push >>
          commit-message: "<< parameters.commit-message >>"
          skip-ci: << parameters.skip-ci >>
          folder: "op-analytics"

  deploy-github-pages:
    machine:
      image: ubuntu-2004:current
    steps:
      - checkout-dep-1
      - setup-python-env
      - run:
          name: Build Static Content
          command: make html
      # Copy directories to serve
      - run:
          name: Copy Directories To Serve
          command: |
            #this copies everything from the build directory to the docs directory
            make html-copies
            tmp_dir="/tmp/docs"
            mkdir -p $tmp_dir
            cp -r docs/* $tmp_dir
            touch $tmp_dir/.nojekyll

      - utils/get-github-access-token
      - utils/github-pages-deploy:
          force-push: true
          src-pages-dir: /tmp/docs

  daily-uploads-other-tasks:
    machine:
      image: ubuntu-2004:current
    steps:
      - checkout-dep-1
      - setup-python-env
      - run:
          name: L2Beat
          command: uv run opdata pulls l2beat
          environment:
            OPLABS_RUNTIME: "gha"
            OPLABS_ENV: "prod"

workflows:
  version: 2
  run-daily-api-uploads:
    when:
      or:
        - equal: [<< pipeline.parameters.run_job >>, "run_daily_api_upload"]
        - equal: [<< pipeline.schedule.name >>, "run_daily_api_upload"]
    jobs:
      - daily-uploads-other-tasks:
          context: op-analytics
          filters:
            branches:
              only: main

  daily-l2-aggregate-later-loads:
    when:
      or:
        - equal:
            [<< pipeline.schedule.name >>, "daily_l2_aggregate_later_loads"]
        - equal:
            [
              << pipeline.parameters.run_job >>,
              "daily_l2_aggregate_later_loads",
            ]
    jobs:
      - convert-and-run-notebooks-to-py-and-push:
          commit-message: "GH Action Update - Agg Downloads"
          notebook-paths: >-
            [
              {"dir": "op_chains_tracking", "notebook": "dune_opchain_uploads"},
              {"dir": "other_chains_tracking", "notebook": "get_filtered_deployers"},
              {"dir": "other_chains_tracking", "notebook": "total_chain_data_to_uploads"}
            ]
          context:
            - op-analytics-legacy
            - circleci-repo-op-analytics
            - op-analytics-gcp

  run_market_data:
    when:
      or:
        - equal: [<< pipeline.schedule.name >>, "run_market_data"]
        - equal: [<< pipeline.parameters.run_job >>, "run_market_data"]
    jobs:
      - convert-and-run-notebooks-to-py-and-push:
          commit-message: "GH Action Update - Update Market Data"
          notebook-paths: >-
            [
              {"dir": "reference_data/market_data", "notebook": "get_market_data"}
            ]
          context:
            - op-analytics-legacy
            - circleci-repo-op-analytics
            - op-analytics-gcp

  run_op_governance_uploads:
    when:
      or:
        - equal: [<< pipeline.schedule.name >>, "run_op_governance_uploads"]
        - equal:
            [<< pipeline.parameters.run_job >>, "run_op_governance_uploads"]
    jobs:
      - convert-and-run-notebooks-to-py-and-push:
          commit-message: "GH Action Update - Gov Uploads"
          notebook-paths: >-
            [
              {"dir": "op_governance_data", "notebook": "op_circulating_supply"},
              {"dir": "op_governance_data", "notebook": "op_votable_supply"}
            ]
          context:
            - op-analytics-legacy
            - circleci-repo-op-analytics
            - op-analytics-gcp

  run_op_stack_chain_revenue:
    when:
      or:
        - equal: [<< pipeline.schedule.name >>, "run_op_stack_chain_revenue"]
        - equal:
            [<< pipeline.parameters.run_job >>, "run_op_stack_chain_revenue"]
    jobs:
      - convert-and-run-notebooks-to-py-and-push:
          push: never
          commit-message: ""
          notebook-paths: >-
            [
              {"dir": "op_chains_tracking", "notebook": "l2_revenue_tracking"}
            ]
          context:
            - op-analytics-legacy
            - circleci-repo-op-analytics
            - op-analytics-gcp

  run_op_stack_metadata_upload:
    when:
      or:
        - equal: [<< pipeline.schedule.name >>, "run_op_stack_metadata_upload"]
        - equal:
            [<< pipeline.parameters.run_job >>, "run_op_stack_metadata_upload"]
    jobs:
      - convert-and-run-notebooks-to-py-and-push:
          commit-message: "GH Action Update - Generate Chain Metadata, Matviews and Views"
          notebook-paths: >-
            [
              {"dir": "op_chains_tracking/inputs", "notebook": "clean_chain_metadata_and_upload"},
              {"dir": "op_chains_tracking/clickhouse_builds", "notebook": "create_matviews"},
              {"dir": "op_chains_tracking/clickhouse_builds", "notebook": "create_superchain_data_view"}
            ]
          context:
            - op-analytics-legacy
            - circleci-repo-op-analytics
            - op-analytics-gcp

  run_value_locked_flows_update:
    # when:
    #   or:
    #     - equal: [<< pipeline.schedule.name >>, "run_value_locked_flows_update"]
    #     - equal:
    #         [<< pipeline.parameters.run_job >>, "run_value_locked_flows_update"]
    jobs:
      - convert-and-run-notebooks-to-py-and-push:
          commit-message: "GH Action Update - Value Flows"
          notebook-paths: >-
            [
              {"dir": "value_locked_flows", "notebook": "total_app_net_flows_async"}
            ]
          context:
            - op-analytics-legacy
            - circleci-repo-op-analytics
            - op-analytics-gcp

  daily_l2_chain_data_uploads:
    when:
      or:
        - equal: [<< pipeline.schedule.name >>, "daily_l2_chain_data_uploads"]
        - equal:
            [<< pipeline.parameters.run_job >>, "daily_l2_chain_data_uploads"]
    jobs:
      - setup-python-3-10-job
      - convert-and-run-notebooks-to-py:
          requires:
            - setup-python-3-10-job
          context:
            - op-analytics-legacy
            - circleci-repo-op-analytics
            - op-analytics-gcp
          matrix:
            parameters:
              notebook_paths:
                - other_chains_tracking/chain_tvl_trends
                - other_chains_tracking/pull_l2_activity
                - other_chains_tracking/get_ethereum_chain_list
                - other_chains_tracking/get_dune_evms_info
                - other_chains_tracking/get_qualified_txs
                - other_chains_tracking/get_all_txs
                - other_chains_tracking/get_contract_labels
                - op_chains_tracking/get_superchain_token_list
                - op_chains_tracking/ch_gs_uploads
                - op_chains_tracking/dune_op_stack_chains_l1_data
                - op_collective_economics/opcollective_feesplit/op_collective_revenue
                - op_collective_economics/get_op_collective_net_revenue_balances

  github-pages:
    when:
      or:
        - equal: [<< pipeline.parameters.run_job >>, "deploy_github_pages"]
        - equal: [<< pipeline.schedule.name >>, "deploy_github_pages"]
        - and:
            - equal: [<< pipeline.git.branch >>, "main"]
            - not:
                equal: [<< pipeline.trigger_source >>, "api"]
    jobs:
      - deploy-github-pages:
          context:
            - circleci-repo-op-analytics
            - op-analytics
          filters:
            branches:
              only: main

  run-python-installation-and-dependency-check:
    when:
      or:
        - equal:
            [
              << pipeline.parameters.run_job >>,
              "run-python-installation-and-dependency-check",
            ]
        - and:
            - equal: [<< pipeline.git.branch >>, "main"]
            - not:
                equal: [<< pipeline.trigger_source >>, "api"]
    jobs:
      - python-installation-and-dependency-check
