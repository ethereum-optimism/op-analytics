{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook subgraph_utils.ipynb to python\n",
      "[NbConvertApp] Writing 18564 bytes to subgraph_utils.py\n"
     ]
    }
   ],
   "source": [
    "# ! jupyter nbconvert --to python subgraph_utils.ipynb\n",
    "# This notebook seres for testing functions that we can then export to a .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subgrounds.subgrounds import Subgrounds\n",
    "from subgrounds.pagination import ShallowStrategy, LegacyStrategy\n",
    "import pandas as pd\n",
    "import requests as r\n",
    "import defillama_utils as dfl\n",
    "\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:71.0) Gecko/20100101 Firefox/71.0'}\n",
    "sgs = pd.DataFrame(\n",
    "        [\n",
    "                 ['l2dao-velodrome','https://api.thegraph.com/subgraphs/name/messari/velodrome-optimism','']\n",
    "                ,['synthetix-curve','https://api.thegraph.com/subgraphs/name/convex-community/volume-optimism','']\n",
    "                ,['uniswap','https://api.thegraph.com/subgraphs/name/ianlapham/optimism-post-regenesis', '']\n",
    "        ]\n",
    "        ,columns = ['dfl_id','subgraph_url','query']\n",
    ")\n",
    "sg = Subgrounds()\n",
    "# curve_op = sg.load_subgraph(\"https://api.thegraph.com/subgraphs/name/messari/velodrome-optimism\")\n",
    "# display(sgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sg(tg_api):\n",
    "        csg = sg.load_subgraph(tg_api)\n",
    "        return csg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velodrome_pool_tvl(pid, min_ts = 0, max_ts = 99999999999999):\n",
    "        velo = create_sg('https://api.thegraph.com/subgraphs/name/messari/velodrome-optimism')\n",
    "        q1 = velo.Query.liquidityPoolDailySnapshots(\n",
    "        orderDirection='desc',\n",
    "        first=max_ts*max_ts, #arbitrarily large number so we pull everything\n",
    "                where=[\n",
    "                velo.Query.liquidityPoolDailySnapshot.pool == pid,\n",
    "                velo.Query.liquidityPoolDailySnapshot.timestamp > min_ts,\n",
    "                velo.Query.liquidityPoolDailySnapshot.timestamp <= max_ts,\n",
    "                ]\n",
    "        )\n",
    "        velo_tvl = sg.query_df([\n",
    "                q1.id,\n",
    "                q1.pool.id,\n",
    "                q1.timestamp,\n",
    "                q1.pool.inputTokens.id,\n",
    "                q1.pool.inputTokens.symbol,\n",
    "                \n",
    "                q1.totalValueLockedUSD\n",
    "                ]\n",
    "                , pagination_strategy=ShallowStrategy)\n",
    "        velo_wts = sg.query_df([\n",
    "                q1.id,\n",
    "                q1.pool.id,\n",
    "                q1.timestamp,\n",
    "                q1.inputTokenWeights,\n",
    "                ]\n",
    "                , pagination_strategy=ShallowStrategy)\n",
    "        velo_reserves = sg.query_df([\n",
    "                q1.id,\n",
    "                q1.pool.id,\n",
    "                q1.timestamp,\n",
    "                q1.inputTokenBalances,\n",
    "                ]\n",
    "                , pagination_strategy=ShallowStrategy)\n",
    "        \n",
    "        df_array = [velo_tvl, velo_wts, velo_reserves]\n",
    "\n",
    "        for df in df_array:\n",
    "                df.columns = df.columns.str.replace('liquidityPoolDailySnapshots_', '')\n",
    "                df['id_rank'] = df.groupby(['id']).cumcount()+1\n",
    "\n",
    "        velo_tvl = velo_tvl.merge(velo_wts, on =['id','id_rank','pool_id','timestamp'])\n",
    "        velo_tvl = velo_tvl.merge(velo_reserves, on =['id','id_rank','pool_id','timestamp'])\n",
    "\n",
    "        velo_tvl['timestamp_dt'] = pd.to_datetime(velo_tvl['timestamp'],unit='s')\n",
    "        velo_tvl['timestamp_day'] = pd.to_datetime(velo_tvl['timestamp'],unit='s').dt.floor('d')\n",
    "\n",
    "        velo_tvl['inputTokenBalances'] = velo_tvl['inputTokenBalances'] / (10 ** 18)\n",
    "        velo_tvl['inputToken_tvl'] = velo_tvl['totalValueLockedUSD'] * ( velo_tvl['inputTokenWeights'] / 100 )\n",
    "        # velo_tvl['inputToken_price'] = velo_tvl['inputToken_tvl'] / velo_tvl['inputTokenBalances']\n",
    "\n",
    "        #Standardize Columns\n",
    "        # date\ttoken\ttoken_value\tusd_value\tprotocol\n",
    "        velo_tvl['protocol'] = 'Velodrome'\n",
    "        velo_tvl = velo_tvl[['timestamp_day','pool_inputTokens_symbol','inputTokenBalances','inputToken_tvl','protocol']]\n",
    "        velo_tvl = velo_tvl.rename(columns={\n",
    "                'timestamp_day':'date',\n",
    "                'pool_inputTokens_symbol':'token',\n",
    "                'inputTokenBalances':'token_value',\n",
    "                'inputToken_tvl':'usd_value'\n",
    "        })\n",
    "\n",
    "        return velo_tvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_uniswap_pool_tvl(pid, min_ts = 0, max_ts = 99999999999999):\n",
    "#     uni = create_sg('https://api.thegraph.com/subgraphs/name/ianlapham/optimism-post-regenesis')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_curve_pool_tvl(pid, min_ts = 0, max_ts = 99999999999999):\n",
    "        curve = create_sg('https://api.thegraph.com/subgraphs/name/convex-community/volume-optimism')\n",
    "        q1 = curve.Query.dailyPoolSnapshots(\n",
    "        orderBy=curve.Query.dailyPoolSnapshot.timestamp,\n",
    "        orderDirection='desc',\n",
    "        first=max_ts*max_ts, #arbitrarily large number so we pull everything\n",
    "                where=[\n",
    "                curve.Query.dailyPoolSnapshot.pool == pid,\n",
    "                curve.Query.dailyPoolSnapshot.timestamp > min_ts,\n",
    "                curve.Query.dailyPoolSnapshot.timestamp <= max_ts,\n",
    "                ]\n",
    "        )\n",
    "        curve_tvl = sg.query_df([\n",
    "                q1.id,\n",
    "                q1.pool.address,\n",
    "                q1.pool.name,\n",
    "                q1.pool.symbol,\n",
    "                q1.timestamp,\n",
    "                # q1.tvl,\n",
    "                # q1.adminFeesUSD,\n",
    "                # q1.lpFeesUSD,\n",
    "                q1.pool.coinNames,\n",
    "                # q1.normalizedReserves,\n",
    "                # q1.reservesUSD,\n",
    "                ]\n",
    "                , pagination_strategy=ShallowStrategy)\n",
    "        curve_reserves_normal = sg.query_df([\n",
    "                q1.id,\n",
    "                q1.pool.address,\n",
    "                q1.timestamp,\n",
    "                q1.normalizedReserves,\n",
    "                # q1.pool.coinNames,\n",
    "                \n",
    "                # q1.reservesUSD\n",
    "                ]\n",
    "                , pagination_strategy=ShallowStrategy)\n",
    "        curve_reserves_usd = sg.query_df([\n",
    "                q1.id,\n",
    "                q1.pool.address,\n",
    "                q1.timestamp,\n",
    "                q1.reservesUSD\n",
    "                ]\n",
    "                , pagination_strategy=ShallowStrategy)\n",
    "\n",
    "        df_array = [curve_tvl, curve_reserves_normal, curve_reserves_usd]\n",
    "\n",
    "        for df in df_array:\n",
    "                df.columns = df.columns.str.replace('dailyPoolSnapshots_', '')\n",
    "                df['id_rank'] = df.groupby(['id']).cumcount()+1\n",
    "\n",
    "        curve_tvl = curve_tvl.merge(curve_reserves_normal, on =['id','id_rank','pool_address','timestamp'])\n",
    "        curve_tvl = curve_tvl.merge(curve_reserves_usd, on =['id','id_rank','pool_address','timestamp'])\n",
    "\n",
    "        curve_tvl['normalizedReserves'] = curve_tvl['normalizedReserves'] / ( 10 ** 18 ) #decimal adjust\n",
    "        # curve_tvl['reservePrice'] = curve_tvl['reservesUSD'] / curve_tvl['normalizedReserves'] \n",
    "        curve_tvl['timestamp_dt'] = pd.to_datetime(curve_tvl['timestamp'],unit='s')\n",
    "\n",
    "        #Standardize Columns\n",
    "        # date\ttoken\ttoken_value\tusd_value\tprotocol\n",
    "        curve_tvl['protocol'] = 'Curve'\n",
    "        curve_tvl = curve_tvl[['timestamp_dt','pool_coinNames','normalizedReserves','reservesUSD','protocol']]\n",
    "        curve_tvl = curve_tvl.rename(columns={\n",
    "                'timestamp_dt':'date',\n",
    "                'pool_coinNames':'token',\n",
    "                'normalizedReserves':'token_value',\n",
    "                'reservesUSD':'usd_value'\n",
    "        })\n",
    "\n",
    "        return curve_tvl\n",
    "\n",
    "def get_curve_pool_tvl_and_volume(chain, min_tvl = 10000, min_ts = 0, max_ts = 99999999999999):\n",
    "        # Playground: https://thegraph.com/hosted-service/subgraph/convex-community/volume-optimism\n",
    "        curve = create_sg('https://api.thegraph.com/subgraphs/name/convex-community/volume-' + str.lower(chain))\n",
    "        q1 = curve.Query.dailyPoolSnapshots(\n",
    "        orderBy=curve.Query.dailyPoolSnapshot.tvl,\n",
    "        orderDirection='desc',\n",
    "        first=max_ts*max_ts, #arbitrarily large number so we pull everything\n",
    "                where=[\n",
    "                curve.Query.dailyPoolSnapshot.timestamp > min_ts,\n",
    "                curve.Query.dailyPoolSnapshot.timestamp <= max_ts,\n",
    "                curve.Query.dailyPoolSnapshot.tvl >= min_tvl,\n",
    "                ]\n",
    "        )\n",
    "        curve_tvl = sg.query_df([\n",
    "                q1.id,\n",
    "                q1.timestamp,\n",
    "                q1.pool,\n",
    "                # q1.pool.name,\n",
    "                # q1.pool.symbol,\n",
    "                # q1.pool.coins,\n",
    "                # q1.pool.coins,\n",
    "                q1.pool.coinNames,\n",
    "                q1.tvl,\n",
    "                q1.lpFeesUSD,\n",
    "                q1.adminFeesUSD,\n",
    "                q1.totalDailyFeesUSD,\n",
    "                q1.fee\n",
    "                ]\n",
    "                , pagination_strategy=ShallowStrategy)\n",
    "        curve_tvl.columns = curve_tvl.columns.str.replace('dailyPoolSnapshots_', '')\n",
    "        # print(curve_tvl.columns)\n",
    "        curve_tvl['id_rank'] = curve_tvl.groupby(['id']).cumcount()+1\n",
    "        \n",
    "\n",
    "        grp = curve_tvl.groupby(['timestamp','pool_address','pool_name','pool_symbol','pool_lpToken','pool_isV2',\\\n",
    "                                 'pool_assetType','pool_poolType','tvl'\\\n",
    "                                 ,'lpFeesUSD', 'adminFeesUSD'\\\n",
    "                                        ,'totalDailyFeesUSD','fee'\n",
    "                                        ]).\\\n",
    "                                agg({'pool_coinNames':lambda x: list(x.unique())}\n",
    "                                     )\n",
    "        grp.reset_index(inplace=True)\n",
    "\n",
    "        # Assume Fees / fee rate = original volume\n",
    "        grp['daily_trade_voume_usd'] = grp['totalDailyFeesUSD'] / grp['fee'] \n",
    "\n",
    "        #Map asset type\n",
    "        mappings = {0: \"USD\", 1: \"ETH\", 2: \"BTC\", 3: \"Other\", 4: \"Crypto\"}\n",
    "        grp['pool_assetType_mapped'] = grp['pool_assetType'].map(mappings)\n",
    "        grp['dt'] = pd.to_datetime(grp['timestamp'], unit = 's')\n",
    "        grp['chain'] = chain\n",
    "\n",
    "        # convert the arrays to strings, sort the strings, and convert back to arrays\n",
    "        grp['pool_coinNames'] = grp['pool_coinNames'].apply(lambda x: sorted(x))\n",
    "        grp['pool_coinNames'] = grp['pool_coinNames'].apply(lambda x: [i if i != '0xeeee' else 'ETH' for i in x])\n",
    "        grp['pool_coinNames'] = grp['pool_coinNames'].apply(lambda x: ','.join(map(str, x)))\n",
    "\n",
    "        #cols to include\n",
    "        cols = ['dt','chain','pool_address','pool_lpToken','pool_name','pool_symbol','pool_coinNames','pool_assetType_mapped','pool_poolType'\\\n",
    "                        ,'tvl','daily_trade_voume_usd','totalDailyFeesUSD','fee']\n",
    "        grp = grp[cols]\n",
    "\n",
    "        grp = grp.sort_values(by=['dt','daily_trade_voume_usd'],ascending=[False,False])\n",
    "\n",
    "\n",
    "        return grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_messari_format_pool_tvl(slug, pool_id, chain = 'optimism', min_ts = 0, max_ts = 99999999999999):\n",
    "        msr_dfs = []\n",
    "        # print(slug)\n",
    "        sg_query = create_sg('https://api.thegraph.com/subgraphs/name/messari/' + slug + '-' + chain).Query\n",
    "        # Get Query\n",
    "        pool_info = sg_query.liquidityPools(\n",
    "        # orderBy=sg_query.liquidityPools.timestamp,\n",
    "        # orderDirection='desc',\n",
    "        first=10000,#max_ts*max_ts, #arbitrarily large number so we pull everything\n",
    "                where=[\n",
    "                # sg_query.liquidityPools.timestamp > min_ts,\n",
    "                # sg_query.liquidityPools.timestamp <= max_ts,\n",
    "                sg_query.liquidityPools.id == pool_id\n",
    "                ]\n",
    "        )\n",
    "        snapshots = sg_query.liquidityPoolDailySnapshots(\n",
    "        orderBy= sg_query.liquidityPoolDailySnapshots.timestamp,\n",
    "        orderDirection='desc',\n",
    "        first=10000,#max_ts*max_ts, #arbitrarily large number so we pull everything\n",
    "                where=[\n",
    "                sg_query.liquidityPoolDailySnapshots.timestamp > min_ts,\n",
    "                sg_query.liquidityPoolDailySnapshots.timestamp <= max_ts,\n",
    "                sg_query.liquidityPoolDailySnapshots.pool == pool_id\n",
    "                ]\n",
    "        )\n",
    "        # Pull Fields\n",
    "        pool_lst = sg.query_df([\n",
    "                pool_info.id,\n",
    "                pool_info.inputTokens.id,\n",
    "                pool_info.inputTokens.symbol,\n",
    "                pool_info.inputTokens.decimals,\n",
    "                pool_info.inputTokens.lastPriceUSD\n",
    "        ]\n",
    "        , pagination_strategy=ShallowStrategy)\n",
    "        #Snapshots\n",
    "        snap_lst = sg.query_df([\n",
    "                snapshots.id,\n",
    "                snapshots.timestamp,\n",
    "                #input tokens\n",
    "                snapshots.inputTokenBalances,\n",
    "                # q1.inputTokenBalances ,\n",
    "                # #pull all pool info\n",
    "                # q1.pool.name,\n",
    "                # q1.pool.symbol,\n",
    "                # q1.pool.inputTokens.id\n",
    "        ])\n",
    "        # print(msr_daily)\n",
    "        # msr_daily = pd.concat(snap_lst)\n",
    "        #fix up column names\n",
    "        \n",
    "        snap_lst.columns = snap_lst.columns.str.replace('liquidityPoolDailySnapshots_', '')\n",
    "        snap_lst = snap_lst.rename(columns={'id':'pool_date_id'})\n",
    "        \n",
    "        pool_lst.columns = pool_lst.columns.str.replace('liquidityPools_', '')\n",
    "        pool_lst = pool_lst.rename(columns={'id':'pool_id'})\n",
    "\n",
    "        # GET TOKEN PRICES FROM LLAMA\n",
    "        token_list = pool_lst['inputTokens_id'].drop_duplicates().to_list()\n",
    "\n",
    "        prices = dfl.get_historical_defillama_prices(token_list, chain, min_ts)\n",
    "        prices = prices.rename(columns={'token_address':'inputTokens_id'})\n",
    "\n",
    "        pool_lst = pool_lst[['pool_id','inputTokens_id','inputTokens_lastPriceUSD','inputTokens_symbol','inputTokens_decimals']]\n",
    "\n",
    "\n",
    "        snap_lst['token_order'] = snap_lst.groupby('pool_date_id')['pool_date_id'].cumcount() + 1\n",
    "        pool_lst['token_order'] = pool_lst.groupby('pool_id')['pool_id'].cumcount() + 1\n",
    "\n",
    "        snap_lst['dt'] = pd.to_datetime(snap_lst['timestamp'], unit='s').dt.date\n",
    "\n",
    "        data_df = snap_lst.merge(pool_lst,on='token_order',how='left')\n",
    "        data_df = data_df.merge(prices,on=['inputTokens_id','dt'],how='left')\n",
    "        data_df['token_price'] = data_df['price'].combine_first(data_df['inputTokens_lastPriceUSD']) #prefer defillama's 'price'\n",
    "\n",
    "        data_df['token_balance'] = data_df['inputTokenBalances'] / 10**data_df['inputTokens_decimals']\n",
    "        data_df['usd_balance'] = data_df['token_balance'] * data_df['token_price']\n",
    "\n",
    "        data_df = data_df.rename(columns={\n",
    "                'dt':'date',\n",
    "                'symbol':'token',\n",
    "                'token_balance':'token_value',\n",
    "                'usd_balance':'usd_value'\n",
    "        })\n",
    "\n",
    "        data_df = data_df[['date','token','token_value','usd_value']]\n",
    "\n",
    "        return data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hop_pool_tvl(pid, min_ts = 0, max_ts = 99999999999999):\n",
    "    prot_str = 'hop-protocol'\n",
    "    hop = dfl.get_single_tvl(prot_str, ['Optimism'])\n",
    "    hop = hop[(hop['token'] == pid) & (~hop['token_value'].isna())]\n",
    "    hop = hop[['date','token','token_value','usd_value','protocol']]\n",
    "    hop['protocol'] = 'Hop' #rename to match func\n",
    "    hop.reset_index(inplace=True,drop=True)\n",
    "    return hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, this is not in TVL tracking format - maybe we split this to a new file ~eventually\n",
    "def get_messari_sg_pool_snapshots(slug, chains = ['optimism'], min_ts = 0, max_ts = 99999999999999):\n",
    "        msr_dfs = []\n",
    "        print(slug)\n",
    "        for c in chains:\n",
    "                print(c)\n",
    "                try:\n",
    "                        # Set Chain\n",
    "                        curve = create_sg('https://api.thegraph.com/subgraphs/name/messari/' + slug + '-' + c)\n",
    "                        # Get Query\n",
    "                        q1 = curve.Query.liquidityPoolDailySnapshots(\n",
    "                        orderBy=curve.Query.liquidityPoolDailySnapshot.timestamp,\n",
    "                        orderDirection='desc',\n",
    "                        first=100000,#max_ts*max_ts, #arbitrarily large number so we pull everything\n",
    "                                where=[\n",
    "                                curve.Query.liquidityPoolDailySnapshot.timestamp > min_ts,\n",
    "                                curve.Query.liquidityPoolDailySnapshot.timestamp <= max_ts,\n",
    "                                curve.Query.liquidityPoolDailySnapshot.pool == '0xfb6fe7802ba9290ef8b00ca16af4bc26eb663a28'\n",
    "                                ]\n",
    "                        )\n",
    "                        # Pull Fields\n",
    "                        msr_list = sg.query_df([\n",
    "                                q1.id,\n",
    "                                q1.timestamp,\n",
    "                                q1.totalValueLockedUSD,\n",
    "                                q1.dailyVolumeUSD,\n",
    "                                q1.rewardTokenEmissionsUSD,\n",
    "                                #protocol\n",
    "                                q1.protocol.id,\n",
    "                                q1.protocol.name,\n",
    "                                q1.protocol.slug,\n",
    "                                q1.protocol.network,\n",
    "                                #pool\n",
    "                                q1.pool.id,\n",
    "                                q1.pool.name,\n",
    "                                q1.pool.symbol,\n",
    "                                q1.pool.inputTokens.id\n",
    "                        ]\n",
    "                        , pagination_strategy=ShallowStrategy)\n",
    "                        msr_df = pd.concat(msr_list)\n",
    "                except:\n",
    "                        msr_df = pd.DataFrame()\n",
    "                        continue\n",
    "                msr_dfs.append(msr_df)\n",
    "        \n",
    "        #combine all chains\n",
    "        msr_daily = pd.concat(msr_dfs)\n",
    "\n",
    "        #fix up column names\n",
    "\n",
    "        msr_daily.columns = msr_daily.columns.str.replace('liquidityPoolDailySnapshots_', '')\n",
    "        \n",
    "        col_list = msr_daily.columns.to_list()\n",
    "        print(col_list)\n",
    "        col_list.remove('pool_inputTokens_id') # we want to group by everything else \n",
    "        \n",
    "        msr_daily = msr_daily.fillna(0)\n",
    "\n",
    "        msr_daily = msr_daily.groupby(col_list).agg({'pool_inputTokens_id':lambda x: list(x)})\n",
    "        msr_daily.reset_index(inplace=True)\n",
    "\n",
    "        msr_daily['timestamp'] = pd.to_datetime(msr_daily['timestamp'],unit='s')\n",
    "        msr_daily['date'] = msr_daily['timestamp'].dt.floor('d')\n",
    "        msr_daily['id_rank'] = msr_daily.groupby(['id']).cumcount()+1\n",
    "        msr_daily = msr_daily[msr_daily['pool_id'] != 0] #weird....\n",
    "        # display(msr_daily[msr_daily['id']=='0x445fe580ef8d70ff569ab36e80c647af338db351-19258'])\n",
    "        return pd.DataFrame(msr_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = get_curve_pool_tvl('0x061b87122ed14b9526a813209c8a59a633257bab')\n",
    "# vdf = get_velodrome_pool_tvl('0xfc77e39de40e54f820e313039207dc850e4c9e60')\n",
    "# get_hop_pool_tvl('SNX')\n",
    "# display(vdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.\n",
    "# df = get_curve_pool_tvl_and_volume('optimism')\n",
    "# print(df.columns)\n",
    "# display(df.head())\n",
    "# msr = get_messari_sg_pool_snapshots('curve-finance',['polygon'])\n",
    "# display(msr)\n",
    "\n",
    "# print(msr['dailyVolumeUSD'].mean())\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_messari_format_pool_tvl('beethoven-x', '0xb1c9ac57594e9b1ec0f3787d9f6744ef4cb0a024')\n",
    "# df = get_messari_format_pool_tvl('beethoven-x', '0x4fd63966879300cafafbb35d157dc5229278ed23')\n",
    "# df = get_curve_pool_tvl_and_volume('optimism')\n",
    "# display(df)\n",
    "# messari - beethoven-x - 0xb1c9ac57594e9b1ec0f3787d9f6744ef4cb0a024\n",
    "# display(df)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! jupyter nbconvert --to python subgraph_utils.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d537a1638226190f579d6fbb68604c1b09ebc740a69df557abedb49ad78e592"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
