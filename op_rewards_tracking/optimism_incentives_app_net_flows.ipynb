{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9104321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas\n",
    "# ! pip install requests\n",
    "# ! pip install plotly\n",
    "# ! pip install datetime\n",
    "# ! pip install os\n",
    "# ! pip freeze = requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffefb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests as r\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('../helper_functions')\n",
    "import subgraph_utils as subg\n",
    "import defillama_utils as dfl\n",
    "import pandas_utils as pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1015ba20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/michaelsilberling/Documents/GitHub/op-analytics/op_rewards_tracking\n"
     ]
    }
   ],
   "source": [
    "pwd = os.getcwd()\n",
    "# Verify that our path is right\n",
    "if 'op_rewards_tracking' in pwd:\n",
    "    prepend = ''\n",
    "else:\n",
    "    prepend = 'op_rewards_tracking/'\n",
    "print(pwd)\n",
    "# if TVL by token is not available, do we fallback on raw TVL (sensitive to token prices)?\n",
    "do_fallback_on_raw_tvl = True\n",
    "str_fallback_indicator = '' #Dont append any indicator yet, since it screws up joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94d6ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protocol Incentive Start Dates\n",
    "# Eventually, move this to its own file / csv\n",
    "protocols = pd.read_csv('inputs/' + 'op_incentive_protocols.csv')\n",
    "#evaluate arrays as array\n",
    "protocols['contracts'] = protocols['contracts'].apply(pu.str_to_list)\n",
    "# display(protocols)\n",
    "\n",
    "protocol_name_mapping = pd.DataFrame(\n",
    "    [\n",
    "        ['aave-v3','aave'],\n",
    "        ['beefy','beefy finance'],\n",
    "        ['revert-compoundor','revert finance'],\n",
    "        ['cbridge','celer'],\n",
    "        ['pickle','pickle finance'],\n",
    "        ['stargate','stargate finance'],\n",
    "        ['sushi-trident','sushi']\n",
    "        \n",
    "    ]\n",
    "    ,columns=['slug','app_name']\n",
    ")\n",
    "\n",
    "date_cols = ['start_date', 'end_date']\n",
    "for d in date_cols:\n",
    "    protocols[d] = pd.to_datetime( protocols[d] )\n",
    "\n",
    "protocols = protocols.merge(protocol_name_mapping,on='slug', how = 'left')\n",
    "\n",
    "# For subgraphs\n",
    "protocols['protocol'] = protocols['slug']\n",
    "protocols['app_name'] = protocols['app_name'].combine_first(protocols['slug'])\n",
    "protocols['id_format'] = protocols['slug'].str.replace('-',' ').str.title()\n",
    "protocols['program_name'] = np.where( ( (protocols['name'] == '') )\n",
    "                                    , protocols['id_format']\n",
    "                                    , protocols['id_format'] + ' - ' + protocols['name']\n",
    "                                    )\n",
    "protocols['top_level_name'] = np.where( protocols['name'] == ''\n",
    "                                    , protocols['id_format']\n",
    "                                    , protocols['name']\n",
    "                                    )\n",
    "# protocols['program_name'] = np.where( protocols['name'] == '', protocols['id_format'], protocols['name'])\n",
    "\n",
    "protocols = protocols.sort_values(by='start_date', ascending=True)\n",
    "                    \n",
    "# display(protocols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "177ccca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull Data\n",
    "dfl_protocols = protocols[protocols['data_source'] == 'defillama'].copy()\n",
    "\n",
    "#drop og protocol column to avoid collisions\n",
    "dfl_protocols = dfl_protocols.drop('protocol', axis=1)\n",
    "\n",
    "dfl_slugs = dfl_protocols[['slug']].drop_duplicates()\n",
    "# display(dfl_slugs)\n",
    "df_dfl = dfl.get_range(dfl_slugs[['slug']],['Optimism'], fallback_on_raw_tvl= do_fallback_on_raw_tvl)\n",
    "\n",
    "df_dfl['is_raw_tvl'] = np.where(df_dfl['slug'].str.endswith('*'), 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3399236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Columns\n",
    "# df_dfl['app_name'] = df_dfl['app_name'].combine_first(df_dfl['protocol'])\n",
    "\n",
    "# df_dfl['id_format'] = df_dfl['slug'].str.replace('-',' ').str.title()\n",
    "\n",
    "# df_dfl['app_name'] = df_dfl['app_name'].str.replace('-',' ').str.title()\n",
    "\n",
    "\n",
    "df_dfl = df_dfl.merge(dfl_protocols, on ='slug')\n",
    "# display(df_dfl)\n",
    "\n",
    "# df_dfl['protocol'] = df_dfl['slug']#.combine_first(df_dfl['slug_y'])\n",
    "# display(df_dfl)\n",
    "df_dfl['name'] = df_dfl['name_y'].combine_first(df_dfl['name_x']) + \\\n",
    "                        np.where(df_dfl['protocol'].str.endswith('*'), '*','') #IF Raw TVL, pull this in\n",
    "# display(df_dfl)\n",
    "df_dfl['top_level_name'] = df_dfl['top_level_name'] + np.where(df_dfl['protocol'].str.endswith('*'), '*','') #IF Raw TVL, pull this in\n",
    "\n",
    "df_dfl['program_name'] = df_dfl['program_name'] + np.where(df_dfl['protocol'].str.endswith('*'), '*','') #IF Raw TVL, pull this in\n",
    "\n",
    "df_dfl = df_dfl[['date', 'token', 'token_value', 'usd_value', 'protocol', 'start_date','end_date','program_name','app_name','top_level_name']]\n",
    "\n",
    "# display(df_dfl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d797655",
   "metadata": {},
   "outputs": [],
   "source": [
    "subg_protocols = protocols[protocols['data_source'].str.contains('pool-')].copy()\n",
    "subg_protocols['og_app_name'] = subg_protocols['app_name']\n",
    "subg_protocols['og_protocol'] = subg_protocols['slug']\n",
    "subg_protocols['og_top_level_name'] = subg_protocols['top_level_name']\n",
    "subg_protocols['df_source'] = subg_protocols['data_source'].str.split('-').str[-1]\n",
    "# display(subg_protocols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8904e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfs_sub = []\n",
    "for index, program in subg_protocols.iterrows():\n",
    "        min_tsmp = int( pd.to_datetime(program['start_date']).timestamp() )\n",
    "        min_tsmp = min_tsmp - 1000 #add some buffer\n",
    "        source_slug = program['source_slug']\n",
    "        df_source = program['df_source']\n",
    "        for c in program['contracts']:\n",
    "                # print(df_source + ' - ' +source_slug + ' - ' + c)\n",
    "                # messari generalized\n",
    "                if df_source == 'messari':\n",
    "                        sdf = subg.get_messari_format_pool_tvl(source_slug, c.lower(), min_ts = min_tsmp)\n",
    "                # subgraph specific\n",
    "                elif df_source == 'curve':\n",
    "                        sdf = subg.get_curve_pool_tvl(c.lower(), min_ts = min_tsmp)\n",
    "                elif df_source == 'velodrome':\n",
    "                        sdf = subg.get_velodrome_pool_tvl(c.lower(), min_ts = min_tsmp)\n",
    "                elif df_source == 'hop':\n",
    "                        sdf = subg.get_hop_pool_tvl(c, min_ts = min_tsmp)\n",
    "                        \n",
    "                sdf['start_date'] = program['start_date']\n",
    "                sdf['end_date'] = program['end_date']\n",
    "                sdf['program_name'] = program['program_name']\n",
    "                sdf['protocol'] = program['og_protocol']\n",
    "                sdf['app_name'] = program['og_app_name']\n",
    "                sdf['top_level_name'] = program['og_top_level_name']\n",
    "\n",
    "                sdf['token_value'] = sdf['token_value'].fillna(0)\n",
    "                sdf['usd_value'] = sdf['usd_value'].fillna(0)\n",
    "                dfs_sub.append(sdf)\n",
    "df_df_sub = pd.concat(dfs_sub)\n",
    "# display(df_df_sub[df_df_sub['program_name'].str.contains('Velo')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21088526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_df_sub.sort_values(by='date'))\n",
    "# display(df_dfl[df_dfl['protocol']=='defiedge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f224a707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/by/kltjc8yd0yz_7_wrtrzhrm9m0000gn/T/ipykernel_22626/751772777.py:9: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  df_df_comb['date'] = pd.to_datetime(df_df_comb['date'])\n",
      "/var/folders/by/kltjc8yd0yz_7_wrtrzhrm9m0000gn/T/ipykernel_22626/751772777.py:9: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  df_df_comb['date'] = pd.to_datetime(df_df_comb['date'])\n",
      "/var/folders/by/kltjc8yd0yz_7_wrtrzhrm9m0000gn/T/ipykernel_22626/751772777.py:38: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_df = df_df.groupby(['date','token','protocol','start_date','end_date_30','program_name','app_name','top_level_name']).sum().reset_index()\n"
     ]
    }
   ],
   "source": [
    "df_df_comb = pd.concat([df_dfl, df_df_sub])\n",
    "#remove * from protocol\n",
    "df_df_comb['protocol'] = df_df_comb['protocol'].str[:-1].where(df_df_comb['protocol'].str[-1] == '*', df_df_comb['protocol'])\n",
    "\n",
    "\n",
    "# display(df_df_comb)\n",
    "df_df_comb['start_date'] = pd.to_datetime(df_df_comb['start_date'])\n",
    "df_df_comb['end_date'] = pd.to_datetime(df_df_comb['end_date'])\n",
    "df_df_comb['date'] = pd.to_datetime(df_df_comb['date'])\n",
    "# display(df_df_comb)\n",
    "\n",
    "# Make sure datatypes are clean\n",
    "df_df_comb['token_value'] = df_df_comb['token_value'].astype('float64')\n",
    "df_df_comb['usd_value'] = df_df_comb['usd_value'].astype('float64')\n",
    "\n",
    "#create an extra day to handle for tokens dropping to 0\n",
    "#this is a temp fix - longer term also: Get max of a token x date and do date + 1 = 0 (i.e. weth to eth flips)\n",
    "# find intermediate gaps. Call it a 0 flow in the in-between dates (i.e. pooltogether)\n",
    "df_df_shift = df_df_comb.copy()\n",
    "df_df_shift['date'] = df_df_shift['date'] + timedelta(days=1)\n",
    "df_df_shift['token_value'] = 0\n",
    "df_df_shift['usd_value'] = 0\n",
    "#merge back in\n",
    "df_df = pd.concat([df_df_comb,df_df_shift])\n",
    "df_df = df_df[df_df['date'] <= pd.to_datetime(\"today\") ]\n",
    "\n",
    "\n",
    "\n",
    "# Group - Exclude End Date since this is often null and overwritting could be weird, especially if we actually know an end date\n",
    "df_df['start_date'] = df_df['start_date'].fillna( pd.to_datetime(\"today\").floor('d') )\n",
    "#Generate End Date Column\n",
    "df_df['end_date_30'] = df_df['end_date'].fillna(pd.to_datetime(\"today\")).dt.floor('d') + timedelta(days = 30)\n",
    "\n",
    "# display(\n",
    "#         df_df[(df_df['protocol']=='velodrome')] \n",
    "#         )\n",
    "\n",
    "df_df = df_df.groupby(['date','token','protocol','start_date','end_date_30','program_name','app_name','top_level_name']).sum().reset_index()\n",
    "\n",
    "# display(df_df)\n",
    "# display(\n",
    "#         df_df[(df_df['protocol']=='pooltogether')] \n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f9eb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = df_df.copy()#merge(cg_df, on=['date','token'],how='inner')\n",
    "\n",
    "# data_df = data_df[data_df['token_value'] > 0] #Exclude this, so we can read flows\n",
    "\n",
    "data_df.sort_values(by='date',inplace=True)\n",
    "# data_df['token_value'] = data_df['token_value'].replace(0, np.nan) #keep zeroes\n",
    "data_df['price_usd'] = data_df['usd_value']/data_df['token_value']\n",
    "\n",
    "data_df['rank_desc'] = data_df.groupby(['protocol', 'program_name', 'token'])['date'].\\\n",
    "                            rank(method='dense',ascending=False).astype(int)\n",
    "\n",
    "data_df.sort_values(by='date',inplace=True)\n",
    "\n",
    "last_df = data_df[data_df['rank_desc'] == 1]\n",
    "last_df = last_df.rename(columns={'price_usd':'last_price_usd'})\n",
    "last_df = last_df[['token','protocol','program_name','last_price_usd']]\n",
    "# display(last_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97ea7ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.merge(last_df, on=['token','protocol','program_name'], how='left')\n",
    "\n",
    "data_df['last_token_value'] = data_df.groupby(['token','protocol', 'program_name'])['token_value'].shift(1)\n",
    "\n",
    "data_df['last_price_usd'] = data_df.groupby(['token','protocol', 'program_name'])['price_usd'].shift(1)\n",
    "\n",
    "# If first instnace of token, make sure there's no price diff\n",
    "data_df['last_price_usd'] = data_df[['last_price_usd', 'price_usd']].bfill(axis=1).iloc[:, 0]\n",
    "#Forward fill if token drops off\n",
    "data_df['price_usd'] = data_df[['price_usd','last_price_usd']].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "data_df['last_token_value'] = data_df['last_token_value'].fillna(0)\n",
    "\n",
    "data_df['net_token_flow'] = data_df['token_value'] - data_df['last_token_value']\n",
    "data_df['net_price_change'] = data_df['price_usd'] - data_df['last_price_usd']\n",
    "\n",
    "data_df['net_dollar_flow'] = data_df['net_token_flow'] * data_df['price_usd']\n",
    "data_df['last_price_net_dollar_flow'] = data_df['net_token_flow'] * data_df['last_price_usd']\n",
    "\n",
    "data_df['net_price_stock_change'] = data_df['last_token_value'] * data_df['net_price_change']\n",
    "\n",
    "\n",
    "# display(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59e56fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter before start date\n",
    "data_df = data_df[data_df['date']>= data_df['start_date']]\n",
    "# filter lte end date + 30\n",
    "data_df = data_df[data_df['date']<= data_df['end_date_30']]\n",
    "data_df.drop('end_date_30', axis=1, inplace=True)\n",
    "\n",
    "if not os.path.exists(prepend + \"csv_outputs\"):\n",
    "        os.mkdir(prepend + \"csv_outputs\")\n",
    "data_df.to_csv(prepend + 'csv_outputs/' + 'tvl_flows_by_token.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1ae7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df[data_df['protocol']=='perpetual-protocol'].sort_values(by='date')\n",
    "# data_df.fillna(0)\n",
    "# data_df.sample(5)\n",
    "# data_df[(data_df['protocol'] == 'pooltogether') & (data_df['date'] >= '2022-10-06') & (data_df['date'] <= '2022-10-12')].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59f3f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "netdf_df = data_df[['date','protocol','program_name','net_dollar_flow','net_price_stock_change','last_price_net_dollar_flow','usd_value','app_name','top_level_name']]\n",
    "\n",
    "netdf_df = netdf_df.groupby(['date','protocol','program_name','app_name','top_level_name']).sum(['net_dollar_flow','net_price_stock_change','last_price_net_dollar_flow','usd_value'])\n",
    "\n",
    "# reset & get program data\n",
    "netdf_df.reset_index(inplace=True)\n",
    "\n",
    "netdf_df['tvl_change'] = netdf_df['usd_value'] - netdf_df.groupby(['protocol', 'program_name','app_name'])['usd_value'].shift(1)\n",
    "netdf_df['error'] = netdf_df['tvl_change'] - (netdf_df['net_dollar_flow'] + netdf_df['net_price_stock_change'])\n",
    "\n",
    "cumul_cols = ['net_dollar_flow','last_price_net_dollar_flow','net_price_stock_change']\n",
    "for c in cumul_cols:\n",
    "        netdf_df['cumul_' + c] = netdf_df.groupby(['protocol', 'program_name'])[c].cumsum()\n",
    "        # netdf_df['cumul_last_price_net_dollar_flow'] = netdf_df.groupby(['protocol', 'program_name'])['last_price_net_dollar_flow'].cumsum()\n",
    "        # netdf_df['cumul_net_price_stock_change'] = netdf_df.groupby(['protocol', 'program_name'])['net_price_stock_change'].cumsum()\n",
    "\n",
    "\n",
    "# print(protocols.columns)\n",
    "# print(netdf_df.columns)\n",
    "\n",
    "# Bring Program info Back In\n",
    "join_cols = ['program_name','protocol','app_name','top_level_name']\n",
    "join_cols_join = [col + '_join' for col in join_cols]\n",
    "for c in join_cols:\n",
    "        netdf_df[c+'_join'] = netdf_df[c].str[:-1].where(netdf_df[c].str[-1] == '*', netdf_df[c])\n",
    "        protocols[c+'_join'] = protocols[c]\n",
    "\n",
    "protocol_cols = ['include_in_summary','op_source','start_date','end_date','num_op'] + join_cols_join\n",
    "\n",
    "netdf_df = netdf_df.merge(protocols[protocol_cols], on=join_cols_join)\n",
    "\n",
    "# for c in join_cols_join:\n",
    "#         old_col = c.replace(\"_join\", \"\")\n",
    "#         netdf_df[old_col] = netdf_df[c]\n",
    "\n",
    "#For Summary\n",
    "if_ended_cols = ['net_dollar_flow','last_price_net_dollar_flow']\n",
    "new_ended_cols = []\n",
    "for e in if_ended_cols:\n",
    "        netdf_df['cumul_' + e + '_if_ended'] = netdf_df[~netdf_df['end_date'].isna()].groupby(['protocol', 'program_name'])[e].cumsum()\n",
    "        new_ended_cols.append('cumul_' + e + '_if_ended')\n",
    "#\n",
    "# print(new_ended_cols)\n",
    "# display(netdf_df[netdf_df['protocol'] == 'revert-compoundor'])\n",
    "\n",
    "for d in date_cols:\n",
    "        netdf_df[d] = pd.to_datetime(netdf_df[d])\n",
    "\n",
    "# check info at program end\n",
    "# display(program_end_df)\n",
    "# display(netdf_df[netdf_df['protocol'] == 'velodrome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ba8ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cols = ['cumul_net_dollar_flow','cumul_last_price_net_dollar_flow','cumul_net_price_stock_change','num_op']\n",
    "\n",
    "netdf_df['program_rank_desc'] = netdf_df.groupby(['protocol', 'program_name'])['date'].\\\n",
    "                            rank(method='dense',ascending=False).astype(int)\n",
    "\n",
    "# for sc in summary_cols:\n",
    "#         netdf_df[sc] = netdf_df[sc].astype('int64')\n",
    "summary_cols = summary_cols + new_ended_cols\n",
    "# print(summary_cols)\n",
    "program_end_df = netdf_df[\n",
    "        (pd.to_datetime(netdf_df['date']) == pd.to_datetime(netdf_df['end_date']) ) # is at end date\n",
    "        | (netdf_df['program_rank_desc'] ==1)# or is latest date\n",
    "                        ].groupby(['protocol', 'program_name','app_name']).sum(numeric_only=True)\n",
    "program_end_df.reset_index(inplace=True)\n",
    "# display(program_end_df)\n",
    "\n",
    "# display(program_end_df)\n",
    "for s in summary_cols:\n",
    "        s_new = s+'_at_program_end'\n",
    "        program_end_df = program_end_df.rename(columns={s:s_new})\n",
    "        netdf_df = netdf_df.merge(program_end_df[['protocol','program_name',s_new]], on=['protocol','program_name'], how = 'left')\n",
    "\n",
    "# netdf_df['cumul_net_dollar_flow_at_program_end'] = netdf_df[is_program_end].groupby(['protocol', 'program_name']).sum(['cumul_net_dollar_flow'])\n",
    "# netdf_df['cumul_last_price_net_dollar_flow_at_program_end'] = netdf_df[netdf_df['date'] == netdf_df['end_date']]['last_price_net_dollar_flow'].groupby(['protocol', 'program_name']).cumsum()\n",
    "# netdf_df['cumul_net_price_stock_change_at_program_end'] = netdf_df[netdf_df['date'] == netdf_df['end_date']]['net_price_stock_change'].groupby(['protocol', 'program_name']).cumsum()\n",
    "\n",
    "# netdf_df.loc[ netdf_df['end_date'] == pd.to_datetime(\"2000-01-01\"), 'end_date' ] == pd.to_datetime(\"1900-01-01\")\n",
    "\n",
    "# np.where( netdf_df['end_date'] <= pd.to_datetime(\"2000-01-01\") , pd.NaT , netdf_df['end_date'] )\n",
    "# display(netdf_df[netdf_df['protocol'] == 'hundred-finance'].sort_values(by='program_rank_desc'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb142e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# netdf_df[(netdf_df['date'] >= '2022-10-06') & (netdf_df['date'] <= '2022-10-12')].tail(10)\n",
    "# netdf_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e3721d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "during_str = 'During Program'\n",
    "post_str = 'Post-Program'\n",
    "\n",
    "netdf_df['period'] = np.where(\n",
    "        netdf_df['date'] > netdf_df['end_date'], post_str, during_str\n",
    "        )\n",
    "if not os.path.exists(prepend + \"csv_outputs\"):\n",
    "        os.mkdir(prepend + \"csv_outputs\")\n",
    "netdf_df.to_csv(prepend + 'csv_outputs/op_summer_daily_stats.csv', index=False)\n",
    "\n",
    "#SORT FOR CHARTS\n",
    "netdf_df = netdf_df.sort_values(by=['top_level_name','program_name','app_name'], ascending=[True,True,True])\n",
    "# display(netdf_df.head())\n",
    "# print(netdf_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c7d9ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/by/kltjc8yd0yz_7_wrtrzhrm9m0000gn/T/ipykernel_22626/2447625266.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  latest_data_df['date'] = latest_data_df['date'].dt.date\n",
      "/var/folders/by/kltjc8yd0yz_7_wrtrzhrm9m0000gn/T/ipykernel_22626/2447625266.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  latest_data_df['days_since_program_end'] = \\\n"
     ]
    }
   ],
   "source": [
    "latest_data_df = netdf_df[netdf_df['program_rank_desc'] == 1]\n",
    "latest_data_df['date'] = latest_data_df['date'].dt.date\n",
    "# latest_data_df['days_since_program_end'] \n",
    "# latest_data_df.loc[latest_data_df['end_date'] != '', 'days_since_program_end'] = \\\n",
    "#         pd.to_datetime(latest_data_df['end_date']) \\\n",
    "#         - pd.to_datetime(latest_data_df['date'])\n",
    "\n",
    "latest_data_df['days_since_program_end'] = \\\n",
    "        np.where(latest_data_df['end_date'] != '',\n",
    "        pd.to_datetime(latest_data_df['end_date']) \\\n",
    "        - pd.to_datetime(latest_data_df['date']) \\\n",
    "        , \\\n",
    "        pd.to_datetime(latest_data_df['date']) \\\n",
    "        - pd.to_datetime(latest_data_df['start_date']) \\\n",
    "        )\n",
    "latest_data_df = latest_data_df.sort_values(by='start_date', ascending=False)\n",
    "# display(latest_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5af93f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/by/kltjc8yd0yz_7_wrtrzhrm9m0000gn/T/ipykernel_22626/2610320600.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  season_summary_s0_no_perp['op_source'] = 'Gov Fund - Phase 0 (Excl. Perp)'\n"
     ]
    }
   ],
   "source": [
    "# Generate agg summary df\n",
    "season_summary_pds = latest_data_df[latest_data_df['include_in_summary'] == 1].copy()\n",
    "\n",
    "season_summary_s0_no_perp = season_summary_pds[(season_summary_pds['op_source'] == 'Gov Fund - Phase 0') \\\n",
    "                                                & (season_summary_pds['protocol'] != 'perpetual-protocol')]\n",
    "\n",
    "season_summary_s0_no_perp['op_source'] = 'Gov Fund - Phase 0 (Excl. Perp)'\n",
    "\n",
    "season_summary_raw = pd.concat([season_summary_pds, season_summary_s0_no_perp])\n",
    "\n",
    "season_summary_completed_raw = season_summary_pds[season_summary_pds['end_date'] < pd.to_datetime(\"today\")] #only ended summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe4a0773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/by/kltjc8yd0yz_7_wrtrzhrm9m0000gn/T/ipykernel_22626/2825843958.py:2: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  season_summary = season_summary_raw.groupby('op_source').sum()\n",
      "/var/folders/by/kltjc8yd0yz_7_wrtrzhrm9m0000gn/T/ipykernel_22626/2825843958.py:8: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  season_summary_total = pd.DataFrame(season_summary_total_raw.groupby('op_source').sum())\n",
      "/var/folders/by/kltjc8yd0yz_7_wrtrzhrm9m0000gn/T/ipykernel_22626/2825843958.py:17: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  season_summary_completed = season_summary_completed_raw.groupby('op_source').sum()\n",
      "/var/folders/by/kltjc8yd0yz_7_wrtrzhrm9m0000gn/T/ipykernel_22626/2825843958.py:23: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  season_summary_completed_total = pd.DataFrame(season_summary_completed_total_raw.groupby('op_source').sum())\n"
     ]
    }
   ],
   "source": [
    "# SEASON SUMMARY\n",
    "season_summary = season_summary_raw.groupby('op_source').sum()\n",
    "# display(season_summary.head())\n",
    "season_summary.reset_index()\n",
    "# create a row with total values\n",
    "season_summary_total_raw = season_summary_raw.copy()\n",
    "season_summary_total_raw['op_source'] = '- TOTAL -'\n",
    "season_summary_total = pd.DataFrame(season_summary_total_raw.groupby('op_source').sum())\n",
    "\n",
    "# concatenate the aggregated grouped data with the total row\n",
    "season_summary = pd.concat([season_summary, season_summary_total])\n",
    "season_summary.reset_index(inplace=True)\n",
    "# season_summary.head()\n",
    "\n",
    "# SEASON SUMMARY IF COMPLETED - loops were weird, so doing it this way\n",
    "\n",
    "season_summary_completed = season_summary_completed_raw.groupby('op_source').sum()\n",
    "# display(season_summary.head())\n",
    "season_summary_completed.reset_index()\n",
    "# create a row with total values\n",
    "season_summary_completed_total_raw = season_summary_completed_raw.copy()\n",
    "season_summary_completed_total_raw['op_source'] = '- TOTAL -'\n",
    "season_summary_completed_total = pd.DataFrame(season_summary_completed_total_raw.groupby('op_source').sum())\n",
    "\n",
    "# concatenate the aggregated grouped data with the total row\n",
    "season_summary_completed = pd.concat([season_summary_completed, season_summary_completed_total])\n",
    "season_summary_completed.reset_index(inplace=True)\n",
    "# season_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acdfaaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(latest_data_df.columns)\n",
    "# print(season_summary.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b36e2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [latest_data_df, season_summary, season_summary_completed]\n",
    "latest_data_df.name = 'op_summer_latest'\n",
    "season_summary.name = 'season_summary'\n",
    "season_summary_completed.name = 'season_summary_completed'\n",
    "\n",
    "for df in df_list:\n",
    "        # Fix 0 columns\n",
    "        for col in df.columns:\n",
    "                if \"_at_program_end\" in col:\n",
    "                        df[col] = df[col].astype(float)\n",
    "                        df[col] = np.where(df[col] == 0, np.NaN, df[col])\n",
    "\n",
    "        df['cumul_flows_per_op_at_program_end'] = df['cumul_net_dollar_flow_at_program_end'] / df['num_op_at_program_end']\n",
    "        \n",
    "        df['cumul_flows_per_op_latest'] = df['cumul_net_dollar_flow'] / df['num_op']\n",
    "\n",
    "        df['last_price_net_dollar_flows_per_op_at_program_end'] = df['cumul_last_price_net_dollar_flow_at_program_end'] / df['num_op_at_program_end']\n",
    "        df['last_price_net_dollar_flows_per_op_latest'] = df['cumul_last_price_net_dollar_flow'] / df['num_op']\n",
    "\n",
    "        df['flows_retention'] = \\\n",
    "                        df['cumul_net_dollar_flow_if_ended'] / df['cumul_net_dollar_flow_at_program_end'] \\\n",
    "                        * np.where(df['cumul_net_dollar_flow'] < 0, -1, 1)\n",
    "        df['last_price_net_dollar_flows_retention'] = \\\n",
    "                        df['cumul_last_price_net_dollar_flow_if_ended'] / df['cumul_last_price_net_dollar_flow_at_program_end'] \\\n",
    "                        * np.where(df['cumul_last_price_net_dollar_flow'] < 0, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74469ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    # display(df)\n",
    "    #get df name\n",
    "    col_list = [\n",
    "        'date','include_in_summary','program_name','app_name','num_op','period','op_source','start_date','end_date'\n",
    "        ,'cumul_net_dollar_flow_at_program_end'\n",
    "        ,'cumul_net_dollar_flow'\n",
    "        ,'cumul_flows_per_op_at_program_end','cumul_last_price_net_dollar_flow_at_program_end'\n",
    "        ,'cumul_flows_per_op_latest', 'cumul_last_price_net_dollar_flow'\n",
    "        , 'last_price_net_dollar_flows_per_op_at_program_end','last_price_net_dollar_flows_per_op_latest'\n",
    "        ,'flows_retention', 'last_price_net_dollar_flows_retention'\n",
    "    ]\n",
    "    summary_exclude_list = ['date','program_name','app_name','period','start_date','end_date']\n",
    "    sort_cols = ['Start','# OP']\n",
    "\n",
    "    if df.name == 'op_summer_latest':\n",
    "        html_name = df.name + '_stats'\n",
    "        sort_order = [False, False]\n",
    "    elif 'season_summary' in df.name:\n",
    "        html_name = df.name + '_stats'\n",
    "        sort_cols = ['Source','# OP']\n",
    "        sort_order = [False, True] # so totals goes to bottom\n",
    "        col_list = [x for x in col_list if x not in summary_exclude_list]\n",
    "    else:\n",
    "        html_name = 'other'\n",
    "\n",
    "    df_format = df.copy()\n",
    "    new_cols = df_format.columns\n",
    "    drop_cols = ['net_dollar_flow',\n",
    "        'net_price_stock_change', 'last_price_net_dollar_flow', 'usd_value',\n",
    "        'tvl_change', 'error'\n",
    "        ]\n",
    "    new_cols = new_cols.drop(drop_cols)\n",
    "    # print(new_cols)\n",
    "    df_format = df_format[new_cols]\n",
    "\n",
    "    # df_format['num_op'] = df_format['num_op'].apply(lambda x: '{0:,.0f}'.format(x) if not pd.isna(x) else x )\n",
    "    # df_format['flows_retention'] = df_format['flows_retention'].apply(lambda x: '{:,.1%}'.format(x) if not pd.isna(x) else x )\n",
    "    # df_format['last_price_net_dollar_flows_retention'] = df_format['last_price_net_dollar_flows_retention'].apply(lambda x: '{:,.1%}'.format(x) if not pd.isna(x) else x )\n",
    "\n",
    "    df_format = df_format[col_list]\n",
    "    df_format = df_format.reset_index(drop=True)\n",
    "\n",
    "    if not os.path.exists(prepend + \"csv_outputs\"):\n",
    "        os.mkdir(prepend + \"csv_outputs\")\n",
    "    if not os.path.exists(prepend + \"img_outputs\"):\n",
    "        os.mkdir(prepend + \"img_outputs\")\n",
    "        os.mkdir(prepend + \"img_outputs/overall\")\n",
    "        os.mkdir(prepend + \"img_outputs/overall/png\")\n",
    "        os.mkdir(prepend + \"img_outputs/overall/svg\")\n",
    "        os.mkdir(prepend + \"img_outputs/overall/html\")\n",
    "    df_format.to_csv(prepend + 'csv_outputs/' + html_name + '.csv', index=False)\n",
    "\n",
    "    format_cols = [\n",
    "        'cumul_flows_per_op_at_program_end','cumul_flows_per_op_latest','last_price_net_dollar_flows_per_op_at_program_end','last_price_net_dollar_flows_per_op_latest']\n",
    "    format_mil_cols = [\n",
    "        'cumul_net_dollar_flow', 'cumul_last_price_net_dollar_flow',\n",
    "        'cumul_net_dollar_flow_at_program_end',\n",
    "        'cumul_last_price_net_dollar_flow_at_program_end'\n",
    "    ]\n",
    "    # for f in format_cols:\n",
    "        # df_format[f] = df_format[f].apply(lambda x: '${0:,.2f}'.format(x) if not pd.isna(x) else x )\n",
    "        # df_format[f] = df_format[f].apply(lambda x: round(x,1) if not pd.isna(x) else x )\n",
    "    # for fm in format_mil_cols:\n",
    "    #     df_format[fm] = df_format[fm].apply(lambda x: '${0:,.2f}M'.format(x/1e6) if not pd.isna(x) else x )\n",
    "\n",
    "\n",
    "    df_format = df_format.rename(columns={\n",
    "        'date':'Date', 'program_name':'Program', 'num_op': '# OP'\n",
    "        ,'period': 'Period','op_source': 'Source','start_date':'Start','end_date':'End'\n",
    "        ,'cumul_net_dollar_flow_at_program_end':'Net Flows (at End Date)'\n",
    "        ,'cumul_net_dollar_flow':'Net Flows (End + 30)'\n",
    "        ,'cumul_flows_per_op_at_program_end': 'Net Flows per OP (at End Date)'\n",
    "        ,'cumul_flows_per_op_latest': 'Net Flows per OP (End + 30)'\n",
    "        ##\n",
    "        ,'cumul_last_price_net_dollar_flow_at_program_end':'Net Flows @ Current Prices (at End Date)'\n",
    "        ,'cumul_last_price_net_dollar_flow':'Net Flows @ Current Prices (End + 30)'\n",
    "        ,'last_price_net_dollar_flows_per_op_at_program_end': 'Net Flows per OP @ Current Prices (at End Date)'\n",
    "        ,'last_price_net_dollar_flows_per_op_latest': 'Net Flows per OP @ Current Prices (End + 30)'\n",
    "        ,'flows_retention' : 'Net Flows Retained'\n",
    "        ,'last_price_net_dollar_flows_retention' : 'Net Flows Retained @ Current Prices'\n",
    "    })\n",
    "\n",
    "    df_col_list = list(df_format.columns)\n",
    "    df_col_list.remove('include_in_summary')\n",
    "\n",
    "    format_mil_cols_clean = [x for x in df_col_list\n",
    "                             if ('Flows' in x) & ('Retained' not in x)]\n",
    "    # print(format_mil_cols_clean)\n",
    "    format_pct_cols_clean = [x for x in df_col_list\n",
    "                             if 'Retained' in x]\n",
    "\n",
    "    format_op_cols_clean = ['# OP']\n",
    "    # [\n",
    "    #     '# OP','Net Flows (at End Date)',\n",
    "    #     'Net Flows (End + 30)', 'Net Flows @ Current Prices (End + 30)',\n",
    "    #     'Net Flows @ Current Prices (at End Date)',\n",
    "    #     'Net Flows @ Current Prices (at End Date)'\n",
    "    # ]\n",
    "    df_format = df_format.fillna('')\n",
    "    df_format = df_format.reset_index(drop=True)\n",
    "    df_format = df_format.sort_values(by=sort_cols, ascending = sort_order)\n",
    "\n",
    "    # df_format.to_html(\n",
    "    #     prepend + \"img_outputs/app/\" + html_name + \".html\",\n",
    "    #     classes='table table-stripped')\n",
    "    # display(df_format[format_mil_cols_clean])\n",
    "    # fig_tbl = px.table(df_format[df_col_list], sortable=True)\n",
    "    # fig_tbl.show()\n",
    "    \n",
    "    #chatgpt goat?\n",
    "    header = dict(values=df_col_list, fill_color='darkgray', align='center')#, sort_action='native')\n",
    "\n",
    "    # format the numbers in mil_columns and store the result in a list of lists\n",
    "    values = [[pu.format_num(x,'$') if col in format_mil_cols_clean else \n",
    "           pu.format_num(x) if col in format_op_cols_clean else \n",
    "           pu.format_pct(x) if col in format_pct_cols_clean else x \n",
    "           for x in df_format[col]] for col in df_col_list]\n",
    "\n",
    "    cells = dict(values=values, fill_color=['white', 'lightgray'] * (len(df_format)//2+1), align='right')#, line_break=True)\n",
    "\n",
    "    data = [go.Table(header=header, cells=cells)]\n",
    "\n",
    "    layout = go.Layout(title='TVL & Flows Stats')#, width='100%')\n",
    "\n",
    "    fig_tbl = go.Figure(data=data, layout=layout)\n",
    "    # fig_tbl.show()\n",
    "    # pd_html = pu.generate_html(df_format[df_col_list])\n",
    "    # pd_html = pu.DataTable(df_format[df_col_list]).data\n",
    "\n",
    "    # print(type(pd_html))\n",
    "    # open(prepend + \"img_outputs/app/html/\" + html_name + \".html\", \"w\").write(pd_html)\n",
    "\n",
    "    if not os.path.exists(prepend+'img_outputs/app'):\n",
    "        os.mkdir(prepend+'img_outputs/app')\n",
    "        os.mkdir(prepend+'img_outputs/app/png')\n",
    "        os.mkdir(prepend+'img_outputs/app/svg')\n",
    "        os.mkdir(prepend+'img_outputs/app/html')\n",
    "\n",
    "    fig_tbl.write_html(prepend+'img_outputs/app/html/'+html_name+'.html', include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ee270a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for Charts\n",
    "\n",
    "netdf_df = netdf_df[netdf_df['date'] <= pd.to_datetime(\"today\").floor('d')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "546e13b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelsilberling/opt/anaconda3/envs/new-env/lib/python3.10/site-packages/kaleido/scopes/base.py:188: DeprecationWarning:\n",
      "\n",
      "setDaemon() is deprecated, set the daemon attribute instead\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'img_outputs/svg/daily_ndf.svg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m fig\u001b[39m.\u001b[39mupdate_layout(\n\u001b[1;32m      9\u001b[0m     legend_title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mApp Name\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m fig\u001b[39m.\u001b[39mupdate_layout(yaxis_tickprefix \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m$\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m fig\u001b[39m.\u001b[39;49mwrite_image(prepend \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mimg_outputs/svg/daily_ndf.svg\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     13\u001b[0m fig\u001b[39m.\u001b[39mwrite_image(prepend \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mimg_outputs/png/daily_ndf.png\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m fig\u001b[39m.\u001b[39mwrite_html(prepend \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mimg_outputs/daily_ndf.html\u001b[39m\u001b[39m\"\u001b[39m, include_plotlyjs\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcdn\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new-env/lib/python3.10/site-packages/plotly/basedatatypes.py:3821\u001b[0m, in \u001b[0;36mBaseFigure.write_image\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3761\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3762\u001b[0m \u001b[39mConvert a figure to a static image and write it to a file or writeable\u001b[39;00m\n\u001b[1;32m   3763\u001b[0m \u001b[39mobject\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3817\u001b[0m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3818\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3819\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n\u001b[0;32m-> 3821\u001b[0m \u001b[39mreturn\u001b[39;00m pio\u001b[39m.\u001b[39;49mwrite_image(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new-env/lib/python3.10/site-packages/plotly/io/_kaleido.py:297\u001b[0m, in \u001b[0;36mwrite_image\u001b[0;34m(fig, file, format, scale, width, height, validate, engine)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    288\u001b[0m \u001b[39m            \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39mThe 'file' argument '{file}' is not a string, pathlib.Path object, or file descriptor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[1;32m    293\u001b[0m         )\n\u001b[1;32m    294\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m         \u001b[39m# We previously succeeded in interpreting `file` as a pathlib object.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m         \u001b[39m# Now we can use `write_bytes()`.\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m         path\u001b[39m.\u001b[39;49mwrite_bytes(img_data)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new-env/lib/python3.10/pathlib.py:1143\u001b[0m, in \u001b[0;36mPath.write_bytes\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[39m# type-check for the buffer interface before truncating the file\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m view \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(data)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopen(mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m   1144\u001b[0m     \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39mwrite(view)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new-env/lib/python3.10/pathlib.py:1119\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1118\u001b[0m     encoding \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1119\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m, mode, buffering, encoding, errors,\n\u001b[1;32m   1120\u001b[0m                            newline)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'img_outputs/svg/daily_ndf.svg'"
     ]
    }
   ],
   "source": [
    "fig = px.line(netdf_df, x=\"date\", y=\"net_dollar_flow\", color=\"program_name\", \\\n",
    "             title=\"Daily Liquidity Flows Since Program Announcement\",\\\n",
    "            labels={\n",
    "                     \"date\": \"Day\",\n",
    "                     \"net_dollar_flow\": \"Net Liquidity Flows (USD)\"\n",
    "                 }\n",
    "            )\n",
    "fig.update_layout(\n",
    "    legend_title=\"App Name\"\n",
    ")\n",
    "fig.update_layout(yaxis_tickprefix = '$')\n",
    "fig.write_image(prepend + \"img_outputs/overall/svg/daily_ndf.svg\")\n",
    "fig.write_image(prepend + \"img_outputs/overall/png/daily_ndf.png\")\n",
    "fig.write_html(prepend + \"img_outputs/overall/daily_ndf.html\", include_plotlyjs='cdn')\n",
    "\n",
    "\n",
    "\n",
    "cumul_fig = go.Figure()\n",
    "proto_names = netdf_df['program_name'].drop_duplicates()\n",
    "# print(proto_names)\n",
    "for p in proto_names:\n",
    "    cumul_fig.add_trace(go.Scatter(x=netdf_df[netdf_df['program_name'] == p]['date'] \\\n",
    "                                   , y=netdf_df[netdf_df['program_name'] == p]['cumul_net_dollar_flow'] \\\n",
    "                                    ,name = p\\\n",
    "                                  ,fill='tozeroy')) # fill down to xaxis\n",
    "\n",
    "cumul_fig.update_layout(yaxis_tickprefix = '$')\n",
    "cumul_fig.update_layout(\n",
    "    title=\"Cumulative Net Liquidity Flows Since Program Announcement<br><sup>For Ended Programs, we show continue to show flows through 30 days after program end. | * Shows raw TVL change, rather than flows</sup>\",\n",
    "    xaxis_title=\"Day\",\n",
    "    yaxis_title=\"Cumulative Net Liquidity Flows (USD)\",\n",
    "    legend_title=\"App Name\",\n",
    "#     color_discrete_map=px.colors.qualitative.G10\n",
    ")\n",
    "cumul_fig.write_image(prepend + \"img_outputs/overall/svg/cumul_ndf.svg\") #prepend + \n",
    "cumul_fig.write_image(prepend + \"img_outputs/overall/png/cumul_ndf.png\") #prepend + \n",
    "cumul_fig.write_html(prepend + \"img_outputs/overall/cumul_ndf.html\", include_plotlyjs='cdn')\n",
    "\n",
    "\n",
    "fig_last = go.Figure()\n",
    "proto_names = netdf_df['program_name'].drop_duplicates()\n",
    "# print(proto_names)\n",
    "for p in proto_names:\n",
    "    fig_last.add_trace(go.Scatter(x=netdf_df[netdf_df['program_name'] == p]['date'] \\\n",
    "                                   , y=netdf_df[netdf_df['program_name'] == p]['cumul_last_price_net_dollar_flow'] \\\n",
    "                                    ,name = p\\\n",
    "                                  ,fill='tozeroy')) # fill down to xaxis\n",
    "\n",
    "fig_last.update_layout(yaxis_tickprefix = '$')\n",
    "fig_last.update_layout(\n",
    "    title=\"Cumulative Net Flows since Program Announcement (At Most Recent Token Price)<br><sup>For Ended Programs, we show continue to show flows through 30 days after program end. | * Shows raw TVL change, rather than flows</sup>\",\n",
    "    xaxis_title=\"Day\",\n",
    "    yaxis_title=\"Cumulative Net Flows (USD) - At Most Recent Price\",\n",
    "    legend_title=\"App Name\",\n",
    "#     color_discrete_map=px.colors.qualitative.G10\n",
    ")\n",
    "fig_last.write_image(prepend + \"img_outputs/overall/svg/cumul_ndf_last_price.svg\")\n",
    "fig_last.write_image(prepend + \"img_outputs/overall/png/cumul_ndf_last_price.png\")\n",
    "fig_last.write_html(prepend + \"img_outputs/overall/cumul_ndf_last_price.html\", include_plotlyjs='cdn')\n",
    "# cumul_fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b294a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program-Specific Charts\n",
    "\n",
    "value_list = ['cumul_net_dollar_flow','cumul_last_price_net_dollar_flow']\n",
    "\n",
    "for val in value_list:\n",
    "  if val == 'cumul_last_price_net_dollar_flow':\n",
    "    postpend = \" - At Last Price\"\n",
    "    folder_path = \"/last_price\"\n",
    "  else:\n",
    "    postpend = \"\"\n",
    "    folder_path = \"\"\n",
    "  proto_names = netdf_df['program_name'].drop_duplicates()\n",
    "  # print(proto_names)\n",
    "  for p in proto_names:\n",
    "      cumul_fig_app = go.Figure()\n",
    "      p_df = netdf_df[netdf_df['program_name'] == p]\n",
    "      # cumul_fig_app = px.area(p_df, x=\"date\", y=\"cumul_net_dollar_flow\", color=\"period\")\n",
    "      \n",
    "      during_df = p_df[p_df['period'] == during_str]\n",
    "      cumul_fig_app.add_trace(go.Scatter(x= during_df['date'] \\\n",
    "                                    , y= during_df[val] \\\n",
    "                                      , name = during_str \\\n",
    "                                    ,fill='tozeroy')) # fill down to xaxis\n",
    "      \n",
    "      post_df = p_df[p_df['period'] == post_str]\n",
    "      cumul_fig_app.add_trace(go.Scatter(x= post_df['date'] \\\n",
    "                                    , y= post_df[val] \\\n",
    "                                      , name = post_str \\\n",
    "                                    ,fill='tozeroy')) # fill down to xaxis\n",
    "\n",
    "      cumul_fig_app.update_layout(yaxis_tickprefix = '$')\n",
    "      cumul_fig_app.update_layout(\n",
    "          title=p + \"<br><sup>Cumulative Net Flows since Program Announcement, Until Program End + 30 Days\" + postpend + \"</sup>\",\n",
    "          xaxis_title=\"Day\",\n",
    "          yaxis_title=\"Cumulative Net Flows (USD)\",\n",
    "          legend_title=\"Period\",\n",
    "      #     color_discrete_map=px.colors.qualitative.G10\n",
    "      )\n",
    "      \n",
    "      if not os.path.exists(prepend + \"img_outputs/app\" + folder_path):\n",
    "        os.mkdir(prepend + \"img_outputs/app\" + folder_path)\n",
    "      if not os.path.exists(prepend + \"img_outputs/app\" + folder_path + \"/svg\"):\n",
    "        os.mkdir(prepend + \"img_outputs/app\" + folder_path + \"/svg\")\n",
    "      if not os.path.exists(prepend + \"img_outputs/app\" + folder_path + \"/png\"):\n",
    "        os.mkdir(prepend + \"img_outputs/app/\" + folder_path + \"/png\")\n",
    "      \n",
    "      p_file = p\n",
    "      p_file = p_file.replace(' ','_')\n",
    "      p_file = p_file.replace(':','')\n",
    "      p_file = p_file.replace('/','-')\n",
    "      cumul_fig_app.write_image(prepend + \"img_outputs/app\" + folder_path + \"/svg/cumul_ndf_\" + p_file + \".svg\") #prepend + \n",
    "      cumul_fig_app.write_image(prepend + \"img_outputs/app\" + folder_path + \"/png/cumul_ndf_\" + p_file + \".png\") #prepend + \n",
    "      cumul_fig_app.write_html(prepend + \"img_outputs/app\" + folder_path + \"/cumul_ndf_\" + p_file + \".html\", include_plotlyjs='cdn')\n",
    "      # cumul_fig_app.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_last.show()\n",
    "print(\"yay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! jupyter nbconvert --to python optimism_incentives_app_net_flows.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d537a1638226190f579d6fbb68604c1b09ebc740a69df557abedb49ad78e592"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
