{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9104321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS WIP / EXPERIMENTAL\n",
    "\n",
    "# ! pip install pandas\n",
    "# ! pip install requests\n",
    "# ! pip install plotly\n",
    "# ! pip install datetime\n",
    "# ! pip install os\n",
    "# ! pip freeze = requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffefb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests as r\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "from IPython.display import display  # So that display is recognized in .py files\n",
    "\n",
    "sys.path.append(\"../helper_functions\")\n",
    "import subgraph_utils as subg\n",
    "import defillama_utils as dfl\n",
    "import pandas_utils as pu\n",
    "import duneapi_utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1015ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = os.getcwd()\n",
    "# Verify that our path is right\n",
    "if \"op_rewards_tracking\" in pwd:\n",
    "    prepend = \"\"\n",
    "else:\n",
    "    prepend = \"op_rewards_tracking/\"\n",
    "print(pwd)\n",
    "# if TVL by token is not available, do we fallback on raw TVL (sensitive to token prices)?\n",
    "do_fallback_on_raw_tvl = True\n",
    "str_fallback_indicator = \"\"  # Dont append any indicator yet, since it screws up joins\n",
    "\n",
    "# protocols info\n",
    "protocol_cols = [\n",
    "    \"include_in_summary\",\n",
    "    \"op_source\",\n",
    "    \"start_date\",\n",
    "    \"end_date\",\n",
    "    \"num_op\",\n",
    "    \"num_op_override\",\n",
    "]\n",
    "join_cols = [\n",
    "    \"program_name\",\n",
    "    \"protocol\",\n",
    "    \"app_name\",\n",
    "    \"top_level_name\",\n",
    "    \"parent_protocol\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protocol Incentive Start Dates\n",
    "# Eventually, move this to its own file / csv\n",
    "protocols = pd.read_csv(\"inputs/\" + \"op_incentive_protocols_for_tvl.csv\")\n",
    "# evaluate arrays as array\n",
    "protocols[\"contracts\"] = protocols[\"contracts\"].apply(pu.str_to_list)\n",
    "protocols[\"is_external_dex_bridge_pool\"] = protocols[\n",
    "    \"is_external_dex_bridge_pool\"\n",
    "].fillna(0)\n",
    "# display(protocols)\n",
    "\n",
    "# If we need to map Defillama slugs to our unified App Name (Dune-Based) - Not case sensitive\n",
    "protocol_name_mapping = pd.DataFrame(\n",
    "    [\n",
    "        [\"aave-v3\", \"aave\"],\n",
    "        [\"beefy\", \"beefy finance\"],\n",
    "        [\"revert-compoundor\", \"revert finance\"],\n",
    "        [\"cbridge\", \"celer\"],\n",
    "        [\"pickle\", \"pickle finance\"],\n",
    "        [\"stargate\", \"stargate finance\"],\n",
    "        [\"sushi-trident\", \"sushi\"],\n",
    "        [\"yearn-finance\", \"yearn\"],\n",
    "    ],\n",
    "    columns=[\"slug\", \"app_name\"],\n",
    ")\n",
    "\n",
    "date_cols = [\"start_date\", \"end_date\"]\n",
    "for d in date_cols:\n",
    "    protocols[d] = pd.to_datetime(protocols[d])\n",
    "\n",
    "protocols = protocols.merge(protocol_name_mapping, on=\"slug\", how=\"left\")\n",
    "\n",
    "# For subgraphs\n",
    "protocols[\"protocol\"] = protocols[\"slug\"]\n",
    "protocols[\"app_name\"] = (\n",
    "    (protocols[\"app_name\"].combine_first(protocols[\"slug\"]))\n",
    "    .str.replace(\"-\", \" \")\n",
    "    .str.title()\n",
    ")\n",
    "\n",
    "protocols[\"parent_protocol\"] = (\n",
    "    protocols[\"parent_protocol\"].combine_first(protocols[\"app_name\"])\n",
    ").str.capitalize()\n",
    "\n",
    "protocols[\"id_format\"] = protocols[\n",
    "    \"parent_protocol\"\n",
    "]  # protocols['slug'].str.replace('-',' ').str.title()\n",
    "protocols[\"program_name\"] = np.where(\n",
    "    ((protocols[\"name\"].isna())),\n",
    "    protocols[\"slug\"].str.replace(\"-\", \" \").str.title(),\n",
    "    protocols[\"slug\"].str.replace(\"-\", \" \").str.title() + \" - \" + protocols[\"name\"],\n",
    ")\n",
    "protocols[\"top_level_name\"] = np.where(\n",
    "    protocols[\"name\"].isna(), protocols[\"id_format\"], protocols[\"name\"]\n",
    ")\n",
    "# protocols['program_name'] = np.where( protocols['name'] == '', protocols['id_format'], protocols['name'])\n",
    "\n",
    "protocols = protocols.sort_values(by=\"start_date\", ascending=True)\n",
    "\n",
    "# display(protocols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a35549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Protocols to Dune\n",
    "du.write_dune_api_from_pandas(\n",
    "    protocols,\n",
    "    \"op_rewards_tvl_flow_protocols_input\",\n",
    "    \"Table containing input parameters for OP Rewards TVL Flows\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177ccca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull Data\n",
    "dfl_protocols = protocols[protocols[\"data_source\"] == \"defillama\"].copy()\n",
    "\n",
    "# drop og protocol column to avoid collisions\n",
    "dfl_protocols = dfl_protocols.drop(\"protocol\", axis=1)\n",
    "\n",
    "dfl_slugs = dfl_protocols[[\"slug\"]].drop_duplicates()\n",
    "# display(dfl_slugs)\n",
    "df_dfl = dfl.get_range(\n",
    "    dfl_slugs[[\"slug\"]], [\"Optimism\"], fallback_on_raw_tvl=do_fallback_on_raw_tvl\n",
    ")\n",
    "\n",
    "df_dfl[\"is_raw_tvl\"] = np.where(df_dfl[\"slug\"].str.endswith(\"*\"), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff9db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_dfl[df_dfl['protocol'].str.contains('arrakis')])\n",
    "# display(df_dfl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3399236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Columns\n",
    "# df_dfl['app_name'] = df_dfl['app_name'].combine_first(df_dfl['protocol'])\n",
    "\n",
    "# df_dfl['id_format'] = df_dfl['slug'].str.replace('-',' ').str.title()\n",
    "\n",
    "# df_dfl['app_name'] = df_dfl['app_name'].str.replace('-',' ').str.title()\n",
    "\n",
    "\n",
    "df_dfl = df_dfl.merge(dfl_protocols, on=\"slug\")\n",
    "# display(df_dfl)\n",
    "\n",
    "# df_dfl['protocol'] = df_dfl['slug']#.combine_first(df_dfl['slug_y'])\n",
    "df_dfl[\"parent_protocol\"] = df_dfl[\"parent_protocol_y\"].combine_first(\n",
    "    df_dfl[\"parent_protocol_x\"]\n",
    ")\n",
    "# display(df_dfl)\n",
    "df_dfl[\"name\"] = df_dfl[\"name_y\"].combine_first(df_dfl[\"name_x\"]) + np.where(\n",
    "    df_dfl[\"protocol\"].str.endswith(\"*\"), \"*\", \"\"\n",
    ")  # IF Raw TVL, pull this in\n",
    "# display(df_dfl)\n",
    "df_dfl[\"top_level_name\"] = df_dfl[\"top_level_name\"] + np.where(\n",
    "    df_dfl[\"protocol\"].str.endswith(\"*\"), \"*\", \"\"\n",
    ")  # IF Raw TVL, pull this in\n",
    "\n",
    "df_dfl[\"program_name\"] = df_dfl[\"program_name\"] + np.where(\n",
    "    df_dfl[\"protocol\"].str.endswith(\"*\"), \"*\", \"\"\n",
    ")  # IF Raw TVL, pull this in\n",
    "\n",
    "# display(df_dfl)\n",
    "\n",
    "df_dfl = df_dfl[\n",
    "    [\n",
    "        \"date\",\n",
    "        \"token\",\n",
    "        \"token_value\",\n",
    "        \"usd_value\",\n",
    "        \"protocol\",\n",
    "        \"start_date\",\n",
    "        \"end_date\",\n",
    "        \"program_name\",\n",
    "        \"app_name\",\n",
    "        \"top_level_name\",\n",
    "        \"parent_protocol\",\n",
    "        \"include_in_summary\",\n",
    "        \"is_external_dex_bridge_pool\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_dfl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d797655",
   "metadata": {},
   "outputs": [],
   "source": [
    "subg_protocols = protocols[protocols[\"data_source\"].str.contains(\"pool-\")].copy()\n",
    "subg_protocols[\"og_app_name\"] = subg_protocols[\"app_name\"]\n",
    "subg_protocols[\"og_protocol\"] = subg_protocols[\"slug\"]\n",
    "subg_protocols[\"og_top_level_name\"] = subg_protocols[\"top_level_name\"]\n",
    "subg_protocols[\"df_source\"] = subg_protocols[\"data_source\"].str.split(\"-\").str[-1]\n",
    "subg_protocols[\"og_include_in_summary\"] = subg_protocols[\"include_in_summary\"]\n",
    "subg_protocols[\"og_is_external_dex_bridge_pool\"] = subg_protocols[\n",
    "    \"is_external_dex_bridge_pool\"\n",
    "]\n",
    "subg_protocols[\"og_parent_protocol\"] = protocols[\"parent_protocol\"]\n",
    "# display(subg_protocols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_dfl)\n",
    "dfs_sub = []\n",
    "for index, program in subg_protocols.iterrows():\n",
    "    min_tsmp = int(pd.to_datetime(program[\"start_date\"]).timestamp())\n",
    "    min_tsmp = min_tsmp - 1000  # add some buffer\n",
    "    source_slug = program[\"source_slug\"]\n",
    "    df_source = program[\"df_source\"]\n",
    "    for c in program[\"contracts\"]:\n",
    "        # print(df_source + ' - ' +source_slug + ' - ' + c)\n",
    "        # messari generalized\n",
    "        if df_source == \"messari\":\n",
    "            # print(df_source)\n",
    "            sdf = subg.get_messari_format_pool_tvl(\n",
    "                source_slug, c.lower(), min_ts=min_tsmp\n",
    "            )\n",
    "        # subgraph specific\n",
    "        elif df_source == \"curve\":\n",
    "            # print(df_source)\n",
    "            sdf = subg.get_curve_pool_tvl(c.lower(), min_ts=min_tsmp)\n",
    "        elif df_source == \"velodrome\":\n",
    "            # print(df_source)\n",
    "            sdf = subg.get_velodrome_pool_tvl(c.lower(), min_ts=min_tsmp)\n",
    "        elif df_source == \"hop\":\n",
    "            # print(df_source)\n",
    "            sdf = subg.get_hop_pool_tvl(c, min_ts=min_tsmp)\n",
    "\n",
    "        sdf[\"start_date\"] = program[\"start_date\"]\n",
    "        sdf[\"end_date\"] = program[\"end_date\"]\n",
    "        sdf[\"program_name\"] = program[\"program_name\"]\n",
    "        sdf[\"protocol\"] = program[\"og_protocol\"]\n",
    "        sdf[\"app_name\"] = program[\"og_app_name\"]\n",
    "        sdf[\"top_level_name\"] = program[\"og_top_level_name\"]\n",
    "        sdf[\"parent_protocol\"] = program[\"og_parent_protocol\"]\n",
    "        sdf[\"include_in_summary\"] = program[\"og_include_in_summary\"]\n",
    "        sdf[\"is_external_dex_bridge_pool\"] = program[\"og_is_external_dex_bridge_pool\"]\n",
    "\n",
    "        sdf[\"token_value\"] = sdf[\"token_value\"].fillna(0)\n",
    "        sdf[\"usd_value\"] = sdf[\"usd_value\"].fillna(0)\n",
    "        dfs_sub.append(sdf)\n",
    "df_df_sub = pd.concat(dfs_sub)\n",
    "# display(df_df_sub[df_df_sub['program_name'].str.contains('Velo')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21088526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_df_sub.sort_values(by='date'))\n",
    "# display(df_dfl[df_dfl['protocol']=='defiedge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_df_comb = pd.concat([df_dfl, df_df_sub])\n",
    "# remove * from protocol\n",
    "df_df_comb[\"protocol\"] = (\n",
    "    df_df_comb[\"protocol\"]\n",
    "    .str[:-1]\n",
    "    .where(df_df_comb[\"protocol\"].str[-1] == \"*\", df_df_comb[\"protocol\"])\n",
    ")\n",
    "\n",
    "\n",
    "# display(df_df_comb)\n",
    "df_df_comb[\"start_date\"] = pd.to_datetime(df_df_comb[\"start_date\"])\n",
    "df_df_comb[\"end_date\"] = pd.to_datetime(df_df_comb[\"end_date\"])\n",
    "df_df_comb[\"date\"] = pd.to_datetime(df_df_comb[\"date\"])\n",
    "# display(df_df_comb)\n",
    "\n",
    "# display(df_df_comb[df_df_comb['usd_value'] ==''])\n",
    "# Make sure datatypes are clean\n",
    "df_df_comb[\"token_value\"] = df_df_comb[\"token_value\"].astype(\"float64\")\n",
    "df_df_comb[\"usd_value\"] = df_df_comb[\"usd_value\"].astype(\"float64\")\n",
    "\n",
    "# create an extra day to handle for tokens dropping to 0\n",
    "# this is a temp fix - longer term also: Get max of a token x date and do date + 1 = 0 (i.e. weth to eth flips)\n",
    "# find intermediate gaps. Call it a 0 flow in the in-between dates (i.e. pooltogether)\n",
    "df_df_shift = df_df_comb.copy()\n",
    "df_df_shift[\"date\"] = df_df_shift[\"date\"] + timedelta(days=1)\n",
    "df_df_shift[\"token_value\"] = 0\n",
    "df_df_shift[\"usd_value\"] = 0\n",
    "# merge back in\n",
    "df_df = pd.concat([df_df_comb, df_df_shift])\n",
    "df_df = df_df[df_df[\"date\"] <= pd.to_datetime(\"today\")]\n",
    "\n",
    "\n",
    "# Group - Exclude End Date since this is often null and overwritting could be weird, especially if we actually know an end date\n",
    "df_df[\"start_date\"] = df_df[\"start_date\"].fillna(pd.to_datetime(\"today\").floor(\"d\"))\n",
    "# Generate End Date Column\n",
    "df_df[\"end_date_30\"] = df_df[\"end_date\"].fillna(pd.to_datetime(\"today\")).dt.floor(\n",
    "    \"d\"\n",
    ") + timedelta(days=30)\n",
    "\n",
    "# display(\n",
    "#         df_df[(df_df['protocol']=='velodrome')]\n",
    "#         )\n",
    "\n",
    "df_df = (\n",
    "    df_df.groupby(\n",
    "        [\n",
    "            \"date\",\n",
    "            \"token\",\n",
    "            \"protocol\",\n",
    "            \"start_date\",\n",
    "            \"end_date_30\",\n",
    "            \"program_name\",\n",
    "            \"app_name\",\n",
    "            \"top_level_name\",\n",
    "            \"parent_protocol\",\n",
    "            \"include_in_summary\",\n",
    "            \"is_external_dex_bridge_pool\",\n",
    "        ]\n",
    "    )\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# display(\n",
    "#         df_df[(df_df['protocol']=='pooltogether')]\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06393b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(\n",
    "#         df_df[(df_df['protocol']=='velodrome')]\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9eb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = df_df.copy()  # merge(cg_df, on=['date','token'],how='inner')\n",
    "\n",
    "# data_df = data_df[data_df['token_value'] > 0] #Exclude this, so we can read flows\n",
    "\n",
    "data_df.sort_values(by=\"date\", inplace=True)\n",
    "# data_df['token_value'] = data_df['token_value'].replace(0, np.nan) #keep zeroes\n",
    "data_df[\"price_usd\"] = data_df[\"usd_value\"] / data_df[\"token_value\"]\n",
    "\n",
    "data_df[\"rank_desc\"] = (\n",
    "    data_df.groupby([\"protocol\", \"program_name\", \"token\"])[\"date\"]\n",
    "    .rank(method=\"dense\", ascending=False)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "data_df.sort_values(by=\"date\", inplace=True)\n",
    "\n",
    "last_df = data_df[data_df[\"rank_desc\"] == 1]\n",
    "last_df = last_df.rename(columns={\"price_usd\": \"last_price_usd\"})\n",
    "last_df = last_df[[\"token\", \"protocol\", \"program_name\", \"last_price_usd\"]]\n",
    "# display(last_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea7ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.merge(last_df, on=[\"token\", \"protocol\", \"program_name\"], how=\"left\")\n",
    "\n",
    "data_df[\"last_token_value\"] = data_df.groupby([\"token\", \"protocol\", \"program_name\"])[\n",
    "    \"token_value\"\n",
    "].shift(1)\n",
    "\n",
    "data_df[\"last_price_usd\"] = data_df.groupby([\"token\", \"protocol\", \"program_name\"])[\n",
    "    \"price_usd\"\n",
    "].shift(1)\n",
    "\n",
    "# If first instnace of token, make sure there's no price diff\n",
    "data_df[\"last_price_usd\"] = (\n",
    "    data_df[[\"last_price_usd\", \"price_usd\"]].bfill(axis=1).iloc[:, 0]\n",
    ")\n",
    "# Forward fill if token drops off\n",
    "data_df[\"price_usd\"] = data_df[[\"price_usd\", \"last_price_usd\"]].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "data_df[\"last_token_value\"] = data_df[\"last_token_value\"].fillna(0)\n",
    "\n",
    "data_df[\"net_token_flow\"] = data_df[\"token_value\"] - data_df[\"last_token_value\"]\n",
    "data_df[\"net_price_change\"] = data_df[\"price_usd\"] - data_df[\"last_price_usd\"]\n",
    "\n",
    "data_df[\"net_dollar_flow\"] = data_df[\"net_token_flow\"] * data_df[\"price_usd\"]\n",
    "data_df[\"last_price_net_dollar_flow\"] = (\n",
    "    data_df[\"net_token_flow\"] * data_df[\"last_price_usd\"]\n",
    ")\n",
    "\n",
    "data_df[\"net_price_stock_change\"] = (\n",
    "    data_df[\"last_token_value\"] * data_df[\"net_price_change\"]\n",
    ")\n",
    "\n",
    "\n",
    "# display(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e56fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter before start date\n",
    "data_df = data_df[data_df[\"date\"] >= data_df[\"start_date\"]]\n",
    "# filter lte end date + 30\n",
    "data_df = data_df[data_df[\"date\"] <= data_df[\"end_date_30\"]]\n",
    "data_df.drop(\"end_date_30\", axis=1, inplace=True)\n",
    "\n",
    "if not os.path.exists(prepend + \"csv_outputs\"):\n",
    "    os.mkdir(prepend + \"csv_outputs\")\n",
    "data_df.to_csv(prepend + \"csv_outputs/\" + \"tvl_flows_by_token.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df[data_df['protocol']=='perpetual-protocol'].sort_values(by='date')\n",
    "# data_df.fillna(0)\n",
    "# data_df.sample(5)\n",
    "# data_df[(data_df['protocol'] == 'pooltogether') & (data_df['date'] >= '2022-10-06') & (data_df['date'] <= '2022-10-12')].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd4df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae14e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# Add rows for unified program views\n",
    "# ---\n",
    "# Remove * from mappings\n",
    "data_df[\"top_level_name_map\"] = (\n",
    "    data_df[\"top_level_name\"]\n",
    "    .str[:-1]\n",
    "    .where(data_df[\"top_level_name\"].str[-1] == \"*\", data_df[\"top_level_name\"])\n",
    ")\n",
    "data_df[\"program_name_map\"] = (\n",
    "    data_df[\"program_name\"]\n",
    "    .str[:-1]\n",
    "    .where(data_df[\"program_name\"].str[-1] == \"*\", data_df[\"program_name\"])\n",
    ")\n",
    "\n",
    "tst = data_df[data_df[\"include_in_summary\"] == 1]\n",
    "tst = (\n",
    "    tst.groupby([\"top_level_name_map\"])\n",
    "    .agg({\"program_name_map\": \"nunique\"})\n",
    "    .reset_index()\n",
    ")\n",
    "tst = tst[tst[\"program_name_map\"] > 1]\n",
    "# display(tst)\n",
    "\n",
    "unified_prg = data_df[\n",
    "    (data_df[\"top_level_name_map\"].isin(tst[\"top_level_name_map\"]))\n",
    "    & (data_df[\"include_in_summary\"] == 1)\n",
    "]\n",
    "unified_prot = protocols[\n",
    "    (protocols[\"name\"].isin(tst[\"top_level_name_map\"]))\n",
    "    & (protocols[\"include_in_summary\"] == 1)\n",
    "][protocol_cols + join_cols + [\"name\"]]\n",
    "unified_prg[\"include_in_summary\"] = 0\n",
    "unified_prot[\"include_in_summary\"] = 0\n",
    "# # set the values of the columns to the dictionary value using loc\n",
    "cols_override = [\n",
    "    \"protocol\",\n",
    "    \"program_name\",\n",
    "    \"app_name\",\n",
    "    \"top_level_name\",\n",
    "    \"parent_protocol\",\n",
    "]\n",
    "for i in cols_override:\n",
    "    unified_prg[i] = unified_prg[\"top_level_name_map\"]\n",
    "    unified_prot[i] = unified_prot[\"name\"]\n",
    "\n",
    "unified_prot = unified_prot.groupby(\n",
    "    join_cols + [\"include_in_summary\", \"op_source\"]\n",
    ").agg(\n",
    "    {\"start_date\": \"min\", \"end_date\": \"max\", \"num_op\": \"sum\", \"num_op_override\": \"sum\"}\n",
    ")\n",
    "unified_prot = unified_prot.reset_index()\n",
    "# display(unified_prot)\n",
    "unified_prot = unified_prot[protocol_cols + join_cols]\n",
    "\n",
    "# align program info\n",
    "unified_prg[\"start_date\"] = unified_prg.groupby(\"program_name\")[\"start_date\"].transform(\n",
    "    \"min\"\n",
    ")\n",
    "# unified_prg['end_date'] = unified_prg.groupby('program_name')['end_date'].transform('max')\n",
    "# unified_prg['end_date_30'] = unified_prg.groupby('program_name')['end_date_30'].transform('max')\n",
    "\n",
    "# Add to protocol info\n",
    "\n",
    "# display(unified_prg)\n",
    "\n",
    "# APPEND BACK IN\n",
    "\n",
    "data_df = pd.concat([data_df, unified_prg])\n",
    "protocols = pd.concat(\n",
    "    [protocols[protocol_cols + join_cols], unified_prot[protocol_cols + join_cols]]\n",
    ")\n",
    "\n",
    "# display(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ec0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "netdf_df = data_df[\n",
    "    [\n",
    "        \"date\",\n",
    "        \"protocol\",\n",
    "        \"program_name\",\n",
    "        \"net_dollar_flow\",\n",
    "        \"net_price_stock_change\",\n",
    "        \"last_price_net_dollar_flow\",\n",
    "        \"usd_value\",\n",
    "        \"app_name\",\n",
    "        \"top_level_name\",\n",
    "        \"parent_protocol\",\n",
    "        \"is_external_dex_bridge_pool\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "netdf_df = netdf_df.groupby(\n",
    "    [\n",
    "        \"date\",\n",
    "        \"protocol\",\n",
    "        \"program_name\",\n",
    "        \"app_name\",\n",
    "        \"top_level_name\",\n",
    "        \"parent_protocol\",\n",
    "        \"is_external_dex_bridge_pool\",\n",
    "    ]\n",
    ").sum(\n",
    "    [\n",
    "        \"net_dollar_flow\",\n",
    "        \"net_price_stock_change\",\n",
    "        \"last_price_net_dollar_flow\",\n",
    "        \"usd_value\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# reset & get program data\n",
    "netdf_df.reset_index(inplace=True)\n",
    "\n",
    "netdf_df[\"tvl_change\"] = netdf_df[\"usd_value\"] - netdf_df.groupby(\n",
    "    [\"protocol\", \"program_name\", \"app_name\"]\n",
    ")[\"usd_value\"].shift(1)\n",
    "netdf_df[\"error\"] = netdf_df[\"tvl_change\"] - (\n",
    "    netdf_df[\"net_dollar_flow\"] + netdf_df[\"net_price_stock_change\"]\n",
    ")\n",
    "\n",
    "cumul_cols = [\"net_dollar_flow\", \"last_price_net_dollar_flow\", \"net_price_stock_change\"]\n",
    "for c in cumul_cols:\n",
    "    netdf_df[\"cumul_\" + c] = netdf_df.groupby([\"protocol\", \"program_name\"])[c].cumsum()\n",
    "    # netdf_df['cumul_last_price_net_dollar_flow'] = netdf_df.groupby(['protocol', 'program_name'])['last_price_net_dollar_flow'].cumsum()\n",
    "    # netdf_df['cumul_net_price_stock_change'] = netdf_df.groupby(['protocol', 'program_name'])['net_price_stock_change'].cumsum()\n",
    "\n",
    "\n",
    "# print(protocols.columns)\n",
    "# print(netdf_df.columns)\n",
    "\n",
    "# Bring Program info Back In\n",
    "\n",
    "join_cols_join = [col + \"_join\" for col in join_cols]\n",
    "for c in join_cols:\n",
    "    netdf_df[c + \"_join\"] = (\n",
    "        netdf_df[c].str[:-1].where(netdf_df[c].str[-1] == \"*\", netdf_df[c])\n",
    "    )\n",
    "    protocols[c + \"_join\"] = protocols[c]\n",
    "\n",
    "protocol_cols = protocol_cols + join_cols_join\n",
    "\n",
    "netdf_df = netdf_df.merge(protocols[protocol_cols], on=join_cols_join, how=\"left\")\n",
    "\n",
    "# for c in join_cols_join:\n",
    "#         old_col = c.replace(\"_join\", \"\")\n",
    "#         netdf_df[old_col] = netdf_df[c]\n",
    "\n",
    "# For Summary\n",
    "if_ended_cols = [\"net_dollar_flow\", \"last_price_net_dollar_flow\"]\n",
    "new_ended_cols = []\n",
    "for e in if_ended_cols:\n",
    "    netdf_df[\"cumul_\" + e + \"_if_ended\"] = (\n",
    "        netdf_df[~netdf_df[\"end_date\"].isna()]\n",
    "        .groupby([\"protocol\", \"program_name\"])[e]\n",
    "        .cumsum()\n",
    "    )\n",
    "    new_ended_cols.append(\"cumul_\" + e + \"_if_ended\")\n",
    "#\n",
    "# print(new_ended_cols)\n",
    "# display(netdf_df[netdf_df['protocol'] == 'revert-compoundor'])\n",
    "\n",
    "for d in date_cols:\n",
    "    netdf_df[d] = pd.to_datetime(netdf_df[d])\n",
    "\n",
    "# check info at program end\n",
    "# display(program_end_df)\n",
    "# display(netdf_df[netdf_df['protocol'] == 'velodrome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(netdf_df[netdf_df['protocol'].str.contains('velodr')].sort_values(by='date',ascending=True).iloc[:, :15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cols = [\n",
    "    \"cumul_net_dollar_flow\",\n",
    "    \"cumul_last_price_net_dollar_flow\",\n",
    "    \"cumul_net_price_stock_change\",\n",
    "    \"num_op_override\",\n",
    "]\n",
    "\n",
    "netdf_df[\"program_rank_desc\"] = (\n",
    "    netdf_df.groupby([\"protocol\", \"program_name\"])[\"date\"]\n",
    "    .rank(method=\"dense\", ascending=False)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# for sc in summary_cols:\n",
    "#         netdf_df[sc] = netdf_df[sc].astype('int64')\n",
    "summary_cols = summary_cols + new_ended_cols\n",
    "# print(summary_cols)\n",
    "program_end_df = (\n",
    "    netdf_df[\n",
    "        (\n",
    "            pd.to_datetime(netdf_df[\"date\"]) == pd.to_datetime(netdf_df[\"end_date\"])\n",
    "        )  # is at end date\n",
    "        | (netdf_df[\"program_rank_desc\"] == 1)  # or is latest date\n",
    "    ]\n",
    "    .groupby([\"protocol\", \"program_name\", \"app_name\", \"parent_protocol\"])\n",
    "    .sum(numeric_only=True)\n",
    ")\n",
    "program_end_df.reset_index(inplace=True)\n",
    "# display(program_end_df)\n",
    "\n",
    "# display(program_end_df)\n",
    "for s in summary_cols:\n",
    "    s_new = s + \"_at_program_end\"\n",
    "    program_end_df = program_end_df.rename(columns={s: s_new})\n",
    "    netdf_df = netdf_df.merge(\n",
    "        program_end_df[[\"protocol\", \"program_name\", s_new]],\n",
    "        on=[\"protocol\", \"program_name\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "# netdf_df['cumul_net_dollar_flow_at_program_end'] = netdf_df[is_program_end].groupby(['protocol', 'program_name']).sum(['cumul_net_dollar_flow'])\n",
    "# netdf_df['cumul_last_price_net_dollar_flow_at_program_end'] = netdf_df[netdf_df['date'] == netdf_df['end_date']]['last_price_net_dollar_flow'].groupby(['protocol', 'program_name']).cumsum()\n",
    "# netdf_df['cumul_net_price_stock_change_at_program_end'] = netdf_df[netdf_df['date'] == netdf_df['end_date']]['net_price_stock_change'].groupby(['protocol', 'program_name']).cumsum()\n",
    "\n",
    "# netdf_df.loc[ netdf_df['end_date'] == pd.to_datetime(\"2000-01-01\"), 'end_date' ] == pd.to_datetime(\"1900-01-01\")\n",
    "\n",
    "# np.where( netdf_df['end_date'] <= pd.to_datetime(\"2000-01-01\") , pd.NaT , netdf_df['end_date'] )\n",
    "# display(netdf_df[netdf_df['protocol'] == 'hundred-finance'].sort_values(by='program_rank_desc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb142e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# netdf_df[(netdf_df['date'] >= '2022-10-06') & (netdf_df['date'] <= '2022-10-12')].tail(10)\n",
    "# netdf_df[netdf_df['protocol'].str.contains('velodr')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3721d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "during_str = \"During Program\"\n",
    "post_str = \"Post-Program\"\n",
    "\n",
    "netdf_df[\"period\"] = np.where(\n",
    "    netdf_df[\"date\"] > netdf_df[\"end_date\"], post_str, during_str\n",
    ")\n",
    "if not os.path.exists(prepend + \"csv_outputs\"):\n",
    "    os.mkdir(prepend + \"csv_outputs\")\n",
    "netdf_df.to_csv(prepend + \"csv_outputs/op_summer_daily_stats.csv\", index=False)\n",
    "\n",
    "# SORT FOR CHARTS\n",
    "netdf_df = netdf_df.sort_values(\n",
    "    by=[\"top_level_name\", \"program_name\", \"app_name\", \"parent_protocol\"],\n",
    "    ascending=[True, True, True, True],\n",
    ")\n",
    "# display(netdf_df.head())\n",
    "# print(netdf_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7d9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_data_df = netdf_df[netdf_df[\"program_rank_desc\"] == 1]\n",
    "latest_data_df[\"date\"] = latest_data_df[\"date\"].dt.date\n",
    "# latest_data_df['days_since_program_end']\n",
    "# latest_data_df.loc[latest_data_df['end_date'] != '', 'days_since_program_end'] = \\\n",
    "#         pd.to_datetime(latest_data_df['end_date']) \\\n",
    "#         - pd.to_datetime(latest_data_df['date'])\n",
    "\n",
    "latest_data_df[\"days_since_program_end\"] = np.where(\n",
    "    latest_data_df[\"end_date\"] != \"\",\n",
    "    pd.to_datetime(latest_data_df[\"end_date\"]) - pd.to_datetime(latest_data_df[\"date\"]),\n",
    "    pd.to_datetime(latest_data_df[\"date\"])\n",
    "    - pd.to_datetime(latest_data_df[\"start_date\"]),\n",
    ")\n",
    "latest_data_df = latest_data_df.sort_values(by=\"start_date\", ascending=False)\n",
    "# display(latest_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af93f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate agg summary df\n",
    "season_summary_pds = latest_data_df[latest_data_df[\"include_in_summary\"] == 1].copy()\n",
    "\n",
    "season_summary_s0_no_perp = season_summary_pds[\n",
    "    (season_summary_pds[\"op_source\"] == \"Gov Fund - Phase 0\")\n",
    "    & (season_summary_pds[\"protocol\"] != \"perpetual-protocol\")\n",
    "]\n",
    "\n",
    "season_summary_s0_no_perp[\"op_source\"] = \"Gov Fund - Phase 0 (Excl. Perp)\"\n",
    "\n",
    "season_summary_raw = pd.concat([season_summary_pds, season_summary_s0_no_perp])\n",
    "\n",
    "season_summary_completed_raw = season_summary_pds[\n",
    "    season_summary_pds[\"end_date\"] < pd.to_datetime(\"today\")\n",
    "]  # only ended summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEASON SUMMARY\n",
    "season_summary = season_summary_raw.groupby(\"op_source\").sum()\n",
    "# display(season_summary.head())\n",
    "season_summary.reset_index()\n",
    "# create a row with total values\n",
    "season_summary_total_raw = season_summary_raw.copy()\n",
    "season_summary_total_raw[\"op_source\"] = \"- TOTAL -\"\n",
    "season_summary_total = pd.DataFrame(season_summary_total_raw.groupby(\"op_source\").sum())\n",
    "\n",
    "# concatenate the aggregated grouped data with the total row\n",
    "season_summary = pd.concat([season_summary, season_summary_total])\n",
    "season_summary.reset_index(inplace=True)\n",
    "# season_summary.head()\n",
    "\n",
    "# SEASON SUMMARY IF COMPLETED - loops were weird, so doing it this way\n",
    "\n",
    "season_summary_completed = season_summary_completed_raw.groupby(\"op_source\").sum()\n",
    "# display(season_summary.head())\n",
    "season_summary_completed.reset_index()\n",
    "# create a row with total values\n",
    "season_summary_completed_total_raw = season_summary_completed_raw.copy()\n",
    "season_summary_completed_total_raw[\"op_source\"] = \"- TOTAL -\"\n",
    "season_summary_completed_total = pd.DataFrame(\n",
    "    season_summary_completed_total_raw.groupby(\"op_source\").sum()\n",
    ")\n",
    "\n",
    "# concatenate the aggregated grouped data with the total row\n",
    "season_summary_completed = pd.concat(\n",
    "    [season_summary_completed, season_summary_completed_total]\n",
    ")\n",
    "season_summary_completed.reset_index(inplace=True)\n",
    "# season_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdfaaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(latest_data_df.columns)\n",
    "# print(season_summary.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [latest_data_df, season_summary, season_summary_completed]\n",
    "latest_data_df.name = \"op_summer_latest\"\n",
    "season_summary.name = \"season_summary\"\n",
    "season_summary_completed.name = \"season_summary_completed\"\n",
    "\n",
    "for df in df_list:\n",
    "    # Fix 0 columns\n",
    "    for col in df.columns:\n",
    "        if \"_at_program_end\" in col:\n",
    "            df[col] = df[col].astype(float)\n",
    "            df[col] = np.where(df[col] == 0, np.NaN, df[col])\n",
    "\n",
    "    # Per OP Metrics Migrated to combined deployment measures\n",
    "    # df['cumul_flows_per_op_at_program_end'] = df['cumul_net_dollar_flow_at_program_end'] / df['num_op_at_program_end']\n",
    "\n",
    "    # df['cumul_flows_per_op_latest'] = df['cumul_net_dollar_flow'] / df['num_op']\n",
    "\n",
    "    # df['last_price_net_dollar_flows_per_op_at_program_end'] = df['cumul_last_price_net_dollar_flow_at_program_end'] / df['num_op_at_program_end']\n",
    "    # df['last_price_net_dollar_flows_per_op_latest'] = df['cumul_last_price_net_dollar_flow'] / df['num_op']\n",
    "\n",
    "    df[\"flows_retention\"] = (\n",
    "        df[\"cumul_net_dollar_flow_if_ended\"]\n",
    "        / df[\"cumul_net_dollar_flow_at_program_end\"]\n",
    "        * np.where(df[\"cumul_net_dollar_flow\"] < 0, -1, 1)\n",
    "    )\n",
    "    df[\"last_price_net_dollar_flows_retention\"] = (\n",
    "        df[\"cumul_last_price_net_dollar_flow_if_ended\"]\n",
    "        / df[\"cumul_last_price_net_dollar_flow_at_program_end\"]\n",
    "        * np.where(df[\"cumul_last_price_net_dollar_flow\"] < 0, -1, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74469ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    # display(df)\n",
    "    # get df name\n",
    "    col_list = [\n",
    "        \"date\",\n",
    "        \"include_in_summary\",\n",
    "        \"top_level_name\",\n",
    "        \"parent_protocol\",\n",
    "        \"is_external_dex_bridge_pool\",\n",
    "        \"program_name\",\n",
    "        \"app_name\",\n",
    "        \"num_op\",\n",
    "        \"num_op_override\",\n",
    "        \"period\",\n",
    "        \"op_source\",\n",
    "        \"start_date\",\n",
    "        \"end_date\",\n",
    "        \"cumul_net_dollar_flow_at_program_end\",\n",
    "        \"cumul_net_dollar_flow\"\n",
    "        # ,'cumul_flows_per_op_at_program_end'\n",
    "        ,\n",
    "        \"cumul_last_price_net_dollar_flow_at_program_end\"\n",
    "        # ,'cumul_flows_per_op_latest'\n",
    "        ,\n",
    "        \"cumul_last_price_net_dollar_flow\"\n",
    "        # , 'last_price_net_dollar_flows_per_op_at_program_end'\n",
    "        # ,'last_price_net_dollar_flows_per_op_latest'\n",
    "        ,\n",
    "        \"flows_retention\",\n",
    "        \"last_price_net_dollar_flows_retention\",\n",
    "    ]\n",
    "    summary_exclude_list = [\n",
    "        \"date\",\n",
    "        \"top_level_name\",\n",
    "        \"program_name\",\n",
    "        \"app_name\",\n",
    "        \"period\",\n",
    "        \"start_date\",\n",
    "        \"end_date\",\n",
    "        \"parent_protocol\",\n",
    "        \"is_external_dex_bridge_pool\",\n",
    "    ]\n",
    "    sort_cols = [\"Start\", \"# OP\"]\n",
    "\n",
    "    if df.name == \"op_summer_latest\":\n",
    "        html_name = df.name + \"_stats\"\n",
    "        sort_order = [False, False]\n",
    "    elif \"season_summary\" in df.name:\n",
    "        html_name = df.name + \"_stats\"\n",
    "        sort_cols = [\"Source\", \"# OP\"]\n",
    "        sort_order = [False, True]  # so totals goes to bottom\n",
    "        col_list = [x for x in col_list if x not in summary_exclude_list]\n",
    "    else:\n",
    "        html_name = \"other\"\n",
    "\n",
    "    df_format = df.copy()\n",
    "    new_cols = df_format.columns\n",
    "    drop_cols = [\n",
    "        \"net_dollar_flow\",\n",
    "        \"net_price_stock_change\",\n",
    "        \"last_price_net_dollar_flow\",\n",
    "        \"usd_value\",\n",
    "        \"tvl_change\",\n",
    "        \"error\",\n",
    "    ]\n",
    "    new_cols = new_cols.drop(drop_cols)\n",
    "    # print(new_cols)\n",
    "    df_format = df_format[new_cols]\n",
    "\n",
    "    # df_format['num_op'] = df_format['num_op'].apply(lambda x: '{0:,.0f}'.format(x) if not pd.isna(x) else x )\n",
    "    # df_format['flows_retention'] = df_format['flows_retention'].apply(lambda x: '{:,.1%}'.format(x) if not pd.isna(x) else x )\n",
    "    # df_format['last_price_net_dollar_flows_retention'] = df_format['last_price_net_dollar_flows_retention'].apply(lambda x: '{:,.1%}'.format(x) if not pd.isna(x) else x )\n",
    "\n",
    "    df_format = df_format[col_list]\n",
    "    df_format = df_format.reset_index(drop=True)\n",
    "\n",
    "    if not os.path.exists(prepend + \"csv_outputs\"):\n",
    "        os.mkdir(prepend + \"csv_outputs\")\n",
    "    if not os.path.exists(prepend + \"img_outputs\"):\n",
    "        os.mkdir(prepend + \"img_outputs\")\n",
    "        os.mkdir(prepend + \"img_outputs/overall\")\n",
    "        os.mkdir(prepend + \"img_outputs/overall/png\")\n",
    "        os.mkdir(prepend + \"img_outputs/overall/svg\")\n",
    "        os.mkdir(prepend + \"img_outputs/overall/html\")\n",
    "    df_format.to_csv(prepend + \"csv_outputs/\" + html_name + \".csv\", index=False)\n",
    "    if html_name == \"op_summer_latest_stats\":\n",
    "        # Write Output to Dune\n",
    "        du.write_dune_api_from_pandas(\n",
    "            df_format,\n",
    "            \"op_summer_latest_stats\",\n",
    "            \"Table containing outputs for OP Rewards TVL Flows\",\n",
    "        )\n",
    "\n",
    "    format_cols = [\n",
    "        \"cumul_flows_per_op_at_program_end\",\n",
    "        \"cumul_flows_per_op_latest\",\n",
    "        \"last_price_net_dollar_flows_per_op_at_program_end\",\n",
    "        \"last_price_net_dollar_flows_per_op_latest\",\n",
    "    ]\n",
    "    format_mil_cols = [\n",
    "        \"cumul_net_dollar_flow\",\n",
    "        \"cumul_last_price_net_dollar_flow\",\n",
    "        \"cumul_net_dollar_flow_at_program_end\",\n",
    "        \"cumul_last_price_net_dollar_flow_at_program_end\",\n",
    "    ]\n",
    "    # for f in format_cols:\n",
    "    # df_format[f] = df_format[f].apply(lambda x: '${0:,.2f}'.format(x) if not pd.isna(x) else x )\n",
    "    # df_format[f] = df_format[f].apply(lambda x: round(x,1) if not pd.isna(x) else x )\n",
    "    # for fm in format_mil_cols:\n",
    "    #     df_format[fm] = df_format[fm].apply(lambda x: '${0:,.2f}M'.format(x/1e6) if not pd.isna(x) else x )\n",
    "\n",
    "    df_format = df_format.rename(\n",
    "        columns={\n",
    "            \"date\": \"Date\",\n",
    "            \"program_name\": \"Program\",\n",
    "            \"num_op\": \"# OP\",\n",
    "            \"period\": \"Period\",\n",
    "            \"op_source\": \"Source\",\n",
    "            \"start_date\": \"Start\",\n",
    "            \"end_date\": \"End\",\n",
    "            \"cumul_net_dollar_flow_at_program_end\": \"Net Flows (at End Date)\",\n",
    "            \"cumul_net_dollar_flow\": \"Net Flows (End + 30)\",\n",
    "            \"cumul_flows_per_op_at_program_end\": \"Net Flows per OP (at End Date)\",\n",
    "            \"cumul_flows_per_op_latest\": \"Net Flows per OP (End + 30)\"\n",
    "            ##\n",
    "            ,\n",
    "            \"cumul_last_price_net_dollar_flow_at_program_end\": \"Net Flows @ Current Prices (at End Date)\",\n",
    "            \"cumul_last_price_net_dollar_flow\": \"Net Flows @ Current Prices (End + 30)\",\n",
    "            \"last_price_net_dollar_flows_per_op_at_program_end\": \"Net Flows per OP @ Current Prices (at End Date)\",\n",
    "            \"last_price_net_dollar_flows_per_op_latest\": \"Net Flows per OP @ Current Prices (End + 30)\",\n",
    "            \"flows_retention\": \"Net Flows Retained\",\n",
    "            \"last_price_net_dollar_flows_retention\": \"Net Flows Retained @ Current Prices\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df_col_list = list(df_format.columns)\n",
    "    df_col_list.remove(\"include_in_summary\")\n",
    "\n",
    "    format_mil_cols_clean = [\n",
    "        x for x in df_col_list if (\"Flows\" in x) & (\"Retained\" not in x)\n",
    "    ]\n",
    "    # print(format_mil_cols_clean)\n",
    "    format_pct_cols_clean = [x for x in df_col_list if \"Retained\" in x]\n",
    "\n",
    "    format_op_cols_clean = [\"# OP\"]\n",
    "    # [\n",
    "    #     '# OP','Net Flows (at End Date)',\n",
    "    #     'Net Flows (End + 30)', 'Net Flows @ Current Prices (End + 30)',\n",
    "    #     'Net Flows @ Current Prices (at End Date)',\n",
    "    #     'Net Flows @ Current Prices (at End Date)'\n",
    "    # ]\n",
    "    df_format = df_format.fillna(\"\")\n",
    "    df_format = df_format.reset_index(drop=True)\n",
    "    df_format = df_format.sort_values(by=sort_cols, ascending=sort_order)\n",
    "\n",
    "    # df_format.to_html(\n",
    "    #     prepend + \"img_outputs/app/\" + html_name + \".html\",\n",
    "    #     classes='table table-stripped')\n",
    "    # display(df_format[format_mil_cols_clean])\n",
    "    # fig_tbl = px.table(df_format[df_col_list], sortable=True)\n",
    "    # fig_tbl.show()\n",
    "\n",
    "    # chatgpt goat?\n",
    "    header = dict(\n",
    "        values=df_col_list, fill_color=\"darkgray\", align=\"center\"\n",
    "    )  # , sort_action='native')\n",
    "\n",
    "    # format the numbers in mil_columns and store the result in a list of lists\n",
    "    values = [\n",
    "        [\n",
    "            pu.format_num(x, \"$\")\n",
    "            if col in format_mil_cols_clean\n",
    "            else pu.format_num(x)\n",
    "            if col in format_op_cols_clean\n",
    "            else pu.format_pct(x)\n",
    "            if col in format_pct_cols_clean\n",
    "            else x\n",
    "            for x in df_format[col]\n",
    "        ]\n",
    "        for col in df_col_list\n",
    "    ]\n",
    "\n",
    "    cells = dict(\n",
    "        values=values,\n",
    "        fill_color=[\"white\", \"lightgray\"] * (len(df_format) // 2 + 1),\n",
    "        align=\"right\",\n",
    "    )  # , line_break=True)\n",
    "\n",
    "    data = [go.Table(header=header, cells=cells)]\n",
    "\n",
    "    layout = go.Layout(title=\"TVL & Flows Stats\")  # , width='100%')\n",
    "\n",
    "    fig_tbl = go.Figure(data=data, layout=layout)\n",
    "    # fig_tbl.show()\n",
    "    # pd_html = pu.generate_html(df_format[df_col_list])\n",
    "    # pd_html = pu.DataTable(df_format[df_col_list]).data\n",
    "\n",
    "    # print(type(pd_html))\n",
    "    # open(prepend + \"img_outputs/app/html/\" + html_name + \".html\", \"w\").write(pd_html)\n",
    "\n",
    "    if not os.path.exists(prepend + \"img_outputs/app\"):\n",
    "        os.mkdir(prepend + \"img_outputs/app\")\n",
    "        os.mkdir(prepend + \"img_outputs/app/png\")\n",
    "        os.mkdir(prepend + \"img_outputs/app/svg\")\n",
    "        os.mkdir(prepend + \"img_outputs/app/html\")\n",
    "\n",
    "    fig_tbl.write_html(\n",
    "        prepend + \"img_outputs/app/html/\" + html_name + \".html\", include_plotlyjs=\"cdn\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee270a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Charts\n",
    "\n",
    "netdf_df = netdf_df[netdf_df[\"date\"] < pd.to_datetime(\"today\").floor(\"d\")]\n",
    "# netdf_df = netdf_df[netdf_df['include_in_summary'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e13b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    netdf_df,\n",
    "    x=\"date\",\n",
    "    y=\"net_dollar_flow\",\n",
    "    color=\"program_name\",\n",
    "    title=\"Daily Liquidity Flows Since Program Announcement\",\n",
    "    labels={\"date\": \"Day\", \"net_dollar_flow\": \"Net Liquidity Flows (USD)\"},\n",
    ")\n",
    "fig.update_layout(legend_title=\"App Name\")\n",
    "fig.update_layout(yaxis_tickprefix=\"$\")\n",
    "fig.write_image(prepend + \"img_outputs/overall/svg/daily_ndf.svg\")\n",
    "fig.write_image(prepend + \"img_outputs/overall/png/daily_ndf.png\")\n",
    "fig.write_html(prepend + \"img_outputs/overall/daily_ndf.html\", include_plotlyjs=\"cdn\")\n",
    "\n",
    "\n",
    "cumul_fig = go.Figure()\n",
    "proto_names = netdf_df[\"program_name\"].drop_duplicates()\n",
    "# print(proto_names)\n",
    "for p in proto_names:\n",
    "    cumul_fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=netdf_df[netdf_df[\"program_name\"] == p][\"date\"],\n",
    "            y=netdf_df[netdf_df[\"program_name\"] == p][\"cumul_net_dollar_flow\"],\n",
    "            name=p,\n",
    "            fill=\"tozeroy\",\n",
    "        )\n",
    "    )  # fill down to xaxis\n",
    "\n",
    "cumul_fig.update_layout(yaxis_tickprefix=\"$\")\n",
    "cumul_fig.update_layout(\n",
    "    title=\"Cumulative Net Liquidity Flows Since Program Announcement<br><sup>For Ended Programs, we show continue to show flows through 30 days after program end. | * Shows raw TVL change, rather than flows</sup>\",\n",
    "    xaxis_title=\"Day\",\n",
    "    yaxis_title=\"Cumulative Net Liquidity Flows (USD)\",\n",
    "    legend_title=\"App Name\",\n",
    "    #     color_discrete_map=px.colors.qualitative.G10\n",
    ")\n",
    "cumul_fig.write_image(prepend + \"img_outputs/overall/svg/cumul_ndf.svg\")  # prepend +\n",
    "cumul_fig.write_image(prepend + \"img_outputs/overall/png/cumul_ndf.png\")  # prepend +\n",
    "cumul_fig.write_html(\n",
    "    prepend + \"img_outputs/overall/cumul_ndf.html\", include_plotlyjs=\"cdn\"\n",
    ")\n",
    "\n",
    "\n",
    "fig_last = go.Figure()\n",
    "proto_names = netdf_df[\"program_name\"].drop_duplicates()\n",
    "# print(proto_names)\n",
    "for p in proto_names:\n",
    "    fig_last.add_trace(\n",
    "        go.Scatter(\n",
    "            x=netdf_df[netdf_df[\"program_name\"] == p][\"date\"],\n",
    "            y=netdf_df[netdf_df[\"program_name\"] == p][\n",
    "                \"cumul_last_price_net_dollar_flow\"\n",
    "            ],\n",
    "            name=p,\n",
    "            fill=\"tozeroy\",\n",
    "        )\n",
    "    )  # fill down to xaxis\n",
    "\n",
    "fig_last.update_layout(yaxis_tickprefix=\"$\")\n",
    "fig_last.update_layout(\n",
    "    title=\"Cumulative Net Flows since Program Announcement (At Most Recent Token Price)<br><sup>For Ended Programs, we show continue to show flows through 30 days after program end. | * Shows raw TVL change, rather than flows</sup>\",\n",
    "    xaxis_title=\"Day\",\n",
    "    yaxis_title=\"Cumulative Net Flows (USD) - At Most Recent Price\",\n",
    "    legend_title=\"App Name\",\n",
    "    #     color_discrete_map=px.colors.qualitative.G10\n",
    ")\n",
    "fig_last.write_image(prepend + \"img_outputs/overall/svg/cumul_ndf_last_price.svg\")\n",
    "fig_last.write_image(prepend + \"img_outputs/overall/png/cumul_ndf_last_price.png\")\n",
    "fig_last.write_html(\n",
    "    prepend + \"img_outputs/overall/cumul_ndf_last_price.html\", include_plotlyjs=\"cdn\"\n",
    ")\n",
    "# cumul_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b294a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program-Specific Charts\n",
    "\n",
    "value_list = [\"cumul_net_dollar_flow\", \"cumul_last_price_net_dollar_flow\"]\n",
    "\n",
    "for val in value_list:\n",
    "    if val == \"cumul_last_price_net_dollar_flow\":\n",
    "        postpend = \" - At Last Price\"\n",
    "        folder_path = \"/last_price\"\n",
    "    else:\n",
    "        postpend = \"\"\n",
    "        folder_path = \"/daily_price\"\n",
    "\n",
    "    # Clean Folders\n",
    "    for ftype in (\"\", \"/svg\", \"/png\"):\n",
    "        path_fld = prepend + \"img_outputs/app\" + folder_path + ftype\n",
    "        if os.path.exists(path_fld):\n",
    "            shutil.rmtree(path_fld)\n",
    "        os.mkdir(path_fld)\n",
    "\n",
    "    proto_names = netdf_df[\"program_name\"].drop_duplicates()\n",
    "    # print(proto_names)\n",
    "    for p in proto_names:\n",
    "        cumul_fig_app = go.Figure()\n",
    "        p_df = netdf_df[netdf_df[\"program_name\"] == p]\n",
    "        # cumul_fig_app = px.area(p_df, x=\"date\", y=\"cumul_net_dollar_flow\", color=\"period\")\n",
    "\n",
    "        during_df = p_df[p_df[\"period\"] == during_str]\n",
    "        cumul_fig_app.add_trace(\n",
    "            go.Scatter(\n",
    "                x=during_df[\"date\"], y=during_df[val], name=during_str, fill=\"tozeroy\"\n",
    "            )\n",
    "        )  # fill down to xaxis\n",
    "\n",
    "        post_df = p_df[p_df[\"period\"] == post_str]\n",
    "        cumul_fig_app.add_trace(\n",
    "            go.Scatter(x=post_df[\"date\"], y=post_df[val], name=post_str, fill=\"tozeroy\")\n",
    "        )  # fill down to xaxis\n",
    "\n",
    "        cumul_fig_app.update_layout(yaxis_tickprefix=\"$\")\n",
    "        cumul_fig_app.update_layout(\n",
    "            title=p\n",
    "            + \"<br><sup>Cumulative Net Flows since Program Announcement, Until Program End + 30 Days\"\n",
    "            + postpend\n",
    "            + \"</sup>\",\n",
    "            xaxis_title=\"Day\",\n",
    "            yaxis_title=\"Cumulative Net Flows (USD)\",\n",
    "            legend_title=\"Period\",\n",
    "            #     color_discrete_map=px.colors.qualitative.G10\n",
    "        )\n",
    "\n",
    "        p_file = p\n",
    "        p_file = p_file.replace(\" \", \"_\")\n",
    "        p_file = p_file.replace(\":\", \"\")\n",
    "        p_file = p_file.replace(\"/\", \"-\")\n",
    "        cumul_fig_app.write_image(\n",
    "            prepend\n",
    "            + \"img_outputs/app\"\n",
    "            + folder_path\n",
    "            + \"/svg/cumul_ndf_\"\n",
    "            + p_file\n",
    "            + \".svg\"\n",
    "        )  # prepend +\n",
    "        cumul_fig_app.write_image(\n",
    "            prepend\n",
    "            + \"img_outputs/app\"\n",
    "            + folder_path\n",
    "            + \"/png/cumul_ndf_\"\n",
    "            + p_file\n",
    "            + \".png\"\n",
    "        )  # prepend +\n",
    "        cumul_fig_app.write_html(\n",
    "            prepend\n",
    "            + \"img_outputs/app\"\n",
    "            + folder_path\n",
    "            + \"/cumul_ndf_\"\n",
    "            + p_file\n",
    "            + \".html\",\n",
    "            include_plotlyjs=\"cdn\",\n",
    "        )\n",
    "        # cumul_fig_app.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_last.show()\n",
    "print(\"yay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! jupyter nbconvert --to python optimism_incentives_app_net_flows.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "568c45baba86190bf65a5d0b3302bdb9b067c88c2eaacc48373a8c9b6714aa9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
