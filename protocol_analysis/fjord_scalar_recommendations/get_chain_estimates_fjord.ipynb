{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run configs\n",
    "schemas_to_select = [\n",
    "        'op', \n",
    "        # 'base',\n",
    "        # 'mode',\n",
    "        # 'fraxtal',\n",
    "        # 'zora'\n",
    "        ]  # Add more schemas as needed\n",
    "\n",
    "days_of_data = 28\n",
    "chunk_strategy = 'day' #'hour'\n",
    "\n",
    "end_date = '2024-05-30' # -1 if 'now'\n",
    "\n",
    "#FastLZ Regression Metrics\n",
    "# Specs - https://specs.optimism.io/fjord/exec-engine.html?search=#fjord-l1-cost-fee-changes-fastlz-estimator\n",
    "intercept = -42_585_600\n",
    "fastlzCoef = 836_500\n",
    "minTransactionSize = 100\n",
    "scaled_by = 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readme\n",
    "FastLZ needs Python version 3.9x or lower, make sure your environment is using a later python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from web3 import Web3\n",
    "from hexbytes import HexBytes\n",
    "import ast\n",
    "import rlp\n",
    "from rlp.sedes import Binary, big_endian_int, binary, List\n",
    "from eth_utils import to_bytes, to_hex, int_to_big_endian\n",
    "import fastlz\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "import time\n",
    "from dune_client.client import DuneClient\n",
    "from dune_client.types import QueryParameter\n",
    "dotenv.load_dotenv()\n",
    "sys.path.append(\"../../helper_functions\")\n",
    "import clickhouse_utils as ch\n",
    "import duneapi_utils as du\n",
    "import pandas_utils as pu\n",
    "sys.path.pop()\n",
    "\n",
    "client = ch.connect_to_clickhouse_db() #Default is OPLabs DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) #Supress internal fastlz warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read OP Stack Metadata\n",
    "csv_path = '../../op_chains_tracking/outputs/chain_metadata.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Filter the DataFrame based on the schemas_to_select list\n",
    "filtered_df = df[df['oplabs_db_schema'].isin(schemas_to_select)]\n",
    "\n",
    "# Select the required columns and convert to a list of dictionaries\n",
    "chain_mappings_list = filtered_df[['oplabs_db_schema', 'display_name', 'mainnet_chain_id']].rename(\n",
    "    columns={'oplabs_db_schema': 'schema_name', 'mainnet_chain_id': 'chain_id'}\n",
    ").to_dict(orient='records')\n",
    "\n",
    "# Print the resulting list of dictionaries\n",
    "# print(chain_mappings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test transaction receipt\n",
    "# from web3 import Web3\n",
    "# op_rpc = os.getenv(\"OP_PUBLIC_RPC\")\n",
    "# w3 = Web3(Web3.HTTPProvider(op_rpc))\n",
    "\n",
    "# tx_test = '0xcea81f2e836a37b38ba82afd37e6f66c02e348e7b89538aa232013d91edcb926'\n",
    "# tx = w3.eth.get_transaction(tx_test)\n",
    "# txr = w3.eth.get_transaction_receipt(tx_test)\n",
    "# # # txraw = w3.eth.get_raw_transaction(tx_test)\n",
    "# print(tx)\n",
    "# # print(txr)\n",
    "# # # print(txraw)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get L2 Txs from Clickhouse / Goldsky\n",
    "query_by_day = '''\n",
    "        SELECT @chain_id@ as chain_id, nonce, gas, gas_price,\n",
    "                to_address, value, input, block_timestamp, hash\n",
    "        FROM @chain_db_name@_transactions\n",
    "        WHERE gas_price > 0\n",
    "        -- 1 day chunk\n",
    "        AND block_timestamp < DATE_TRUNC('day',NOW()) - interval '@num@ @chunk_strategy@s'\n",
    "        AND block_timestamp >= DATE_TRUNC('day',NOW()) - (interval '@num@ @chunk_strategy@s') - (interval '1 @chunk_strategy@')\n",
    "        group by 1,2,3,4,5,6,7,8,9 --distincts\n",
    "        SETTINGS max_execution_time = 10000\n",
    "'''\n",
    "# AND hash = '0xcea81f2e836a37b38ba82afd37e6f66c02e348e7b89538aa232013d91edcb926'\n",
    "# AND block_number = 120731426\n",
    "\n",
    "# txs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process transactions and RLP encode\n",
    "#https://ethereum.org/en/developers/docs/transactions/\n",
    "\n",
    "# NOTE THE RLP ENCODING IS NOT 1:1 WITH ETHERSCAN YET (but it's ~close-ish)\n",
    "def process_and_encode_transaction(row):\n",
    "    try:\n",
    "        \n",
    "        # Process \"to\" field\n",
    "        to_field = row['to_address']\n",
    "        try:\n",
    "            if isinstance(to_field, str):\n",
    "                if to_field:\n",
    "                    to_field = to_field.decode('utf-8', errors='ignore')\n",
    "                    to_bytes = bytes.fromhex(to_field[2:])\n",
    "                else:\n",
    "                    to_bytes = b''  # Set to an empty bytes object if \"to\" address is null\n",
    "            elif isinstance(to_field, bytes):\n",
    "                if to_field.startswith(b'0x'):\n",
    "                    to_field = to_field.decode('utf-8', errors='ignore')\n",
    "                    to_bytes = bytes.fromhex(to_field[2:])\n",
    "                else:\n",
    "                    to_bytes = to_field\n",
    "            else:\n",
    "                raise ValueError(\"Invalid 'to_address' field type\")\n",
    "        except Exception as e:\n",
    "            print(f'Error in \"to_address\" field: {e}')\n",
    "            print(f'Problematic value: {to_field}')\n",
    "            return pd.Series([None, None])\n",
    "\n",
    "        # Prepare transaction parameters\n",
    "        try:\n",
    "            tx_params = {\n",
    "                'nonce': int_to_big_endian(int(row['nonce'])),\n",
    "                'gasPrice': int_to_big_endian(int(row['gas_price'])),\n",
    "                'gas': int_to_big_endian(int(row['gas'])),\n",
    "                'to': to_bytes,\n",
    "                'value': int_to_big_endian(int(row['value'])) if row['value'] != 0 else b'',  # Encode value as byte array if 0\n",
    "                'input': bytes.fromhex(row['input'][2:]),\n",
    "                'v': int_to_big_endian(int(row['v'])),  # Convert v to a bytes object\n",
    "                'r': bytes.fromhex(row['r'][2:]),\n",
    "                's': bytes.fromhex(row['s'][2:])\n",
    "            }\n",
    "        except:\n",
    "            print(row)\n",
    "\n",
    "        # # Print transaction parameters for debugging\n",
    "        # for key, value in tx_params.items():\n",
    "        #     print(f\"{key}: {value}, {type(value)}\")\n",
    "\n",
    "        # Prepare the transaction fields for RLP encoding\n",
    "        transaction = [\n",
    "            tx_params['nonce'],\n",
    "            tx_params['gasPrice'],\n",
    "            tx_params['gas'],\n",
    "            tx_params['to'],\n",
    "            tx_params['value'],\n",
    "            tx_params['input'],\n",
    "            tx_params['v'],\n",
    "            tx_params['r'],\n",
    "            tx_params['s']\n",
    "        ]\n",
    "\n",
    "        # Encode the entire transaction\n",
    "        encoded_tx = rlp.encode(transaction)\n",
    "        encoded_tx_hex = \"0x\" + encoded_tx.hex()\n",
    "        return pd.Series([encoded_tx_hex, len(encoded_tx)])\n",
    "\n",
    "    except (ValueError, TypeError, UnicodeDecodeError) as e:\n",
    "        print(\"Error:\", e)\n",
    "        print(\"Failed Transaction Info:\")\n",
    "        print(row)\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "# Function to compress transaction data\n",
    "def compress_transaction(encoded_transaction):\n",
    "\n",
    "    hex_string = encoded_transaction[2:]\n",
    "    # Convert the hexadecimal string to bytes\n",
    "    byte_string = bytes.fromhex(hex_string)\n",
    "    compressed_data = fastlz.compress(byte_string)\n",
    "\n",
    "    return compressed_data.hex(), len(compressed_data)\n",
    "# Define a function to apply to each row of the DataFrame\n",
    "def process_and_compress_transaction(row):\n",
    "    encoded_tx = row['encoded_transaction']\n",
    "    compressed_tx, len_tx = compress_transaction(encoded_tx)\n",
    "    return compressed_tx, len_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if chunk_strategy == 'day':\n",
    "        nums_of_data = days_of_data\n",
    "elif chunk_strategy == 'hour':\n",
    "        nums_of_data = days_of_data*24\n",
    "else:\n",
    "        print('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op : day 1/28\n",
      "        Query Done: Completed in 52.02 seconds\n",
      "        Encoding Done: Completed in 60.24 seconds\n",
      "        Compression Done: Completed in 13.13 seconds\n",
      "        Estimation Done: Completed in 4.71 seconds\n",
      "        Transactions Processed: 523.9k\n",
      "op : day 2/28\n",
      "        Query Done: Completed in 52.47 seconds\n",
      "        Encoding Done: Completed in 64.31 seconds\n",
      "        Compression Done: Completed in 16.97 seconds\n",
      "        Estimation Done: Completed in 9.26 seconds\n",
      "        Transactions Processed: 523.4k\n",
      "op : day 3/28\n",
      "        Query Done: Completed in 21.54 seconds\n",
      "        Encoding Done: Completed in 67.05 seconds\n",
      "        Compression Done: Completed in 18.82 seconds\n",
      "        Estimation Done: Completed in 11.45 seconds\n",
      "        Transactions Processed: 516.5k\n",
      "op : day 4/28\n",
      "        Query Done: Completed in 23.09 seconds\n",
      "        Encoding Done: Completed in 65.45 seconds\n",
      "        Compression Done: Completed in 18.27 seconds\n",
      "        Estimation Done: Completed in 11.60 seconds\n",
      "        Transactions Processed: 500.0k\n",
      "op : day 5/28\n",
      "        Query Done: Completed in 22.48 seconds\n",
      "        Encoding Done: Completed in 58.11 seconds\n",
      "        Compression Done: Completed in 15.79 seconds\n",
      "        Estimation Done: Completed in 11.68 seconds\n",
      "        Transactions Processed: 461.9k\n",
      "op : day 6/28\n",
      "        Query Done: Completed in 53.12 seconds\n",
      "        Encoding Done: Completed in 60.55 seconds\n",
      "        Compression Done: Completed in 16.75 seconds\n",
      "        Estimation Done: Completed in 9.36 seconds\n",
      "        Transactions Processed: 461.7k\n",
      "op : day 7/28\n",
      "        Query Done: Completed in 30.73 seconds\n",
      "        Encoding Done: Completed in 69.06 seconds\n",
      "        Compression Done: Completed in 17.63 seconds\n",
      "        Estimation Done: Completed in 12.41 seconds\n",
      "        Transactions Processed: 530.8k\n",
      "op : day 8/28\n",
      "        Query Done: Completed in 31.43 seconds\n",
      "        Encoding Done: Completed in 87.47 seconds\n",
      "        Compression Done: Completed in 21.67 seconds\n",
      "        Estimation Done: Completed in 13.98 seconds\n",
      "        Transactions Processed: 656.6k\n",
      "op : day 9/28\n",
      "        Query Done: Completed in 44.80 seconds\n",
      "        Encoding Done: Completed in 83.66 seconds\n",
      "        Compression Done: Completed in 23.28 seconds\n",
      "        Estimation Done: Completed in 17.16 seconds\n",
      "        Transactions Processed: 630.2k\n",
      "op : day 10/28\n",
      "        Query Done: Completed in 43.38 seconds\n",
      "        Encoding Done: Completed in 83.31 seconds\n",
      "        Compression Done: Completed in 20.95 seconds\n",
      "        Estimation Done: Completed in 14.65 seconds\n",
      "        Transactions Processed: 609.9k\n",
      "op : day 11/28\n",
      "        Query Done: Completed in 32.57 seconds\n",
      "        Encoding Done: Completed in 77.64 seconds\n",
      "        Compression Done: Completed in 20.18 seconds\n",
      "        Estimation Done: Completed in 12.88 seconds\n",
      "        Transactions Processed: 520.3k\n",
      "op : day 12/28\n",
      "        Query Done: Completed in 13.16 seconds\n",
      "        Encoding Done: Completed in 44.20 seconds\n",
      "        Compression Done: Completed in 9.54 seconds\n",
      "        Estimation Done: Completed in 3.54 seconds\n",
      "        Transactions Processed: 358.3k\n",
      "op : day 13/28\n",
      "        Query Done: Completed in 13.30 seconds\n",
      "        Encoding Done: Completed in 45.54 seconds\n",
      "        Compression Done: Completed in 9.54 seconds\n",
      "        Estimation Done: Completed in 3.35 seconds\n",
      "        Transactions Processed: 376.8k\n",
      "op : day 14/28\n",
      "        Query Done: Completed in 14.87 seconds\n",
      "        Encoding Done: Completed in 56.52 seconds\n",
      "        Compression Done: Completed in 12.09 seconds\n",
      "        Estimation Done: Completed in 4.57 seconds\n",
      "        Transactions Processed: 472.5k\n",
      "op : day 15/28\n",
      "        Query Done: Completed in 17.50 seconds\n",
      "        Encoding Done: Completed in 66.55 seconds\n",
      "        Compression Done: Completed in 16.57 seconds\n",
      "        Estimation Done: Completed in 7.55 seconds\n",
      "        Transactions Processed: 543.7k\n",
      "op : day 16/28\n",
      "        Query Done: Completed in 24.08 seconds\n",
      "        Encoding Done: Completed in 71.71 seconds\n",
      "        Compression Done: Completed in 16.90 seconds\n",
      "        Estimation Done: Completed in 7.68 seconds\n",
      "        Transactions Processed: 563.8k\n",
      "op : day 17/28\n",
      "        Query Done: Completed in 19.28 seconds\n",
      "        Encoding Done: Completed in 61.93 seconds\n",
      "        Compression Done: Completed in 16.27 seconds\n",
      "        Estimation Done: Completed in 9.80 seconds\n",
      "        Transactions Processed: 482.0k\n",
      "op : day 18/28\n",
      "        Query Done: Completed in 27.24 seconds\n",
      "        Encoding Done: Completed in 67.63 seconds\n",
      "        Compression Done: Completed in 17.58 seconds\n",
      "        Estimation Done: Completed in 12.90 seconds\n",
      "        Transactions Processed: 511.6k\n",
      "op : day 19/28\n",
      "        Query Done: Completed in 21.54 seconds\n",
      "        Encoding Done: Completed in 52.49 seconds\n",
      "        Compression Done: Completed in 14.70 seconds\n",
      "        Estimation Done: Completed in 8.73 seconds\n",
      "        Transactions Processed: 400.7k\n",
      "op : day 20/28\n",
      "        Query Done: Completed in 24.79 seconds\n",
      "        Encoding Done: Completed in 56.45 seconds\n",
      "        Compression Done: Completed in 15.07 seconds\n",
      "        Estimation Done: Completed in 11.44 seconds\n",
      "        Transactions Processed: 424.9k\n",
      "op : day 21/28\n",
      "        Query Done: Completed in 22.54 seconds\n",
      "        Encoding Done: Completed in 67.46 seconds\n",
      "        Compression Done: Completed in 17.86 seconds\n",
      "        Estimation Done: Completed in 12.82 seconds\n",
      "        Transactions Processed: 518.1k\n",
      "op : day 22/28\n",
      "        Query Done: Completed in 28.92 seconds\n",
      "        Encoding Done: Completed in 69.30 seconds\n",
      "        Compression Done: Completed in 17.23 seconds\n",
      "        Estimation Done: Completed in 12.27 seconds\n",
      "        Transactions Processed: 521.2k\n",
      "op : day 23/28\n",
      "        Query Done: Completed in 41.48 seconds\n",
      "        Encoding Done: Completed in 66.43 seconds\n",
      "        Compression Done: Completed in 18.56 seconds\n",
      "        Estimation Done: Completed in 13.52 seconds\n",
      "        Transactions Processed: 532.7k\n",
      "op : day 24/28\n",
      "        Query Done: Completed in 21.33 seconds\n",
      "        Encoding Done: Completed in 70.03 seconds\n",
      "        Compression Done: Completed in 18.59 seconds\n",
      "        Estimation Done: Completed in 13.01 seconds\n",
      "        Transactions Processed: 540.9k\n",
      "op : day 25/28\n",
      "        Query Done: Completed in 42.75 seconds\n",
      "        Encoding Done: Completed in 69.03 seconds\n",
      "        Compression Done: Completed in 18.07 seconds\n",
      "        Estimation Done: Completed in 12.84 seconds\n",
      "        Transactions Processed: 555.3k\n",
      "op : day 26/28\n",
      "        Query Done: Completed in 26.83 seconds\n",
      "        Encoding Done: Completed in 56.23 seconds\n",
      "        Compression Done: Completed in 11.99 seconds\n",
      "        Estimation Done: Completed in 4.76 seconds\n",
      "        Transactions Processed: 471.0k\n",
      "op : day 27/28\n",
      "        Query Done: Completed in 15.28 seconds\n",
      "        Encoding Done: Completed in 56.25 seconds\n",
      "        Compression Done: Completed in 11.89 seconds\n",
      "        Estimation Done: Completed in 4.23 seconds\n",
      "        Transactions Processed: 477.0k\n",
      "op : day 28/28\n",
      "        Query Done: Completed in 26.17 seconds\n",
      "        Encoding Done: Completed in 67.83 seconds\n",
      "        Compression Done: Completed in 16.66 seconds\n",
      "        Estimation Done: Completed in 7.68 seconds\n",
      "        Transactions Processed: 571.8k\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for chain in chain_mappings_list:\n",
    "        for num in range(0,nums_of_data):\n",
    "                result_df = None #Kill so we don't rerun\n",
    "                print(chain['schema_name'] + ' : '+chunk_strategy+' ' + str(num+1) + '/' + str(nums_of_data))\n",
    "                query_map = query_by_day\n",
    "\n",
    "                query_map = query_map.replace(\"@chain_db_name@\", chain['schema_name'])\n",
    "                query_map = query_map.replace(\"@chain_id@\", str(int(chain['chain_id'])))\n",
    "                query_map = query_map.replace(\"@num@\", str(num))\n",
    "                query_map = query_map.replace(\"@chunk_strategy@\", str(chunk_strategy))\n",
    "\n",
    "                if end_date != -1: #end date config\n",
    "                        query_map = query_map.replace(\"DATE_TRUNC('day',NOW())\", f\"(toDateTime('{end_date} 00:00:00') + interval '1 days')\")\n",
    "\n",
    "                # print(query_map)\n",
    "                query_start_time = time.time()\n",
    "                try:\n",
    "                        result_df = client.query_df(query_map)\n",
    "                except UnicodeDecodeError as e:\n",
    "                        print(f\"UnicodeDecodeError: {e}\")\n",
    "                        print(f\"Problematic byte sequence: {e.object[e.start:e.end]}\")\n",
    "\n",
    "                query_end_time = time.time()  # Record the start time\n",
    "                query_elapsed_time = query_end_time - query_start_time\n",
    "                print (f\"        Query Done: Completed in {query_elapsed_time:.2f} seconds\")\n",
    "\n",
    "                if result_df is None or result_df.empty:\n",
    "                        print(f\"No data found for {chain['schema_name']} on day {num + 1}\")\n",
    "                        continue  # Skip to the next day if there's no data\n",
    "                \n",
    "                # try:\n",
    "                # Add Dummy Signature and fields\n",
    "                result_df['access_list'] = '[]'\n",
    "                result_df['access_list'] = result_df['access_list'].apply(ast.literal_eval)\n",
    "                result_df['r'] = '0x6727a53c0972c55923242cea052dc4e1105d7b65c91c442e2741440965eac357'\n",
    "                result_df['s'] = '0x0a8e71aea623adb7b5562fb9a779634f3b84dad7be1e1f22caaa640db352a6ff'\n",
    "                result_df['v'] = '55'\n",
    "\n",
    "                # Assuming `txs_df` is your DataFrame\n",
    "                # print(result_df.apply(process_and_encode_transaction, axis=1, result_type='expand'))\n",
    "\n",
    "                result_df[['encoded_transaction', 'len_encoded_transaction']] = result_df.apply(process_and_encode_transaction, axis=1, result_type='expand')\n",
    "                enc_end_time = time.time()  # Record the start time\n",
    "                enc_elapsed_time = enc_end_time - query_end_time\n",
    "                print (f\"        Encoding Done: Completed in {enc_elapsed_time:.2f} seconds\")\n",
    "\n",
    "                # Apply compression to each transaction in the DataFrame\n",
    "                result_df[['compressed_transaction', 'compressed_transaction_length']] = result_df.apply(process_and_compress_transaction, axis=1, result_type='expand')\n",
    "                comp_end_time = time.time()\n",
    "                comp_elapsed_time = comp_end_time - enc_end_time\n",
    "                print (f\"        Compression Done: Completed in {comp_elapsed_time:.2f} seconds\")\n",
    "                \n",
    "                # Calculate estimated size for each row\n",
    "                result_df['estimatedSize_raw'] = result_df.apply(lambda row: (intercept + (row['compressed_transaction_length'] * fastlzCoef)) / scaled_by, axis=1)\n",
    "                # Calculate minimum value for 'estimatedSize' column\n",
    "                result_df['estimatedSize'] = result_df.apply(lambda row: max(minTransactionSize, row['estimatedSize_raw']), axis=1)\n",
    "                est_end_time = time.time()\n",
    "                est_elapsed_time = est_end_time - comp_end_time\n",
    "                print (f\"        Estimation Done: Completed in {est_elapsed_time:.2f} seconds\")\n",
    "\n",
    "                # Agg L2\n",
    "                # Convert block_timestamp to date (truncate to day)\n",
    "                result_df['block_date'] = pd.to_datetime(result_df['block_timestamp']).dt.date\n",
    "                result_df['block_date'] = pd.to_datetime(result_df['block_date']).dt.tz_localize(None)\n",
    "                grouped_df = result_df.groupby(['block_date', 'chain_id'])\n",
    "                # Define aggregation functions\n",
    "                agg_functions = {\n",
    "                        'len_encoded_transaction': ['sum', 'mean', 'count'],\n",
    "                        'compressed_transaction_length': ['sum', 'mean'],\n",
    "                        'estimatedSize': ['sum', 'mean']\n",
    "                }\n",
    "                # Perform aggregation\n",
    "                aggregated_df = grouped_df.agg(agg_functions).reset_index()\n",
    "                # Rename columns for clarity\n",
    "                aggregated_df.columns = ['block_date', 'chain_id', \n",
    "                                        'total_len_encoded_transaction', 'average_len_encoded_transaction', 'transaction_count',\n",
    "                                        'total_len_compressed_transaction','average_len_compressed_transaction',\n",
    "                                        'total_estimatedSize', 'average_estimatedSize']\n",
    "                formatted_value = pu.format_num(aggregated_df['transaction_count'][0])\n",
    "                print (f\"        Transactions Processed: {formatted_value}\")\n",
    "                try:\n",
    "                        aggregated_df['chain_name'] = chain['schema_name']\n",
    "                        dfs.append(aggregated_df)\n",
    "                except:\n",
    "                        print('nothing to append')\n",
    "                        continue\n",
    "\n",
    "aggregated_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opstack_metadata = pd.read_csv('../../op_chains_tracking/outputs/chain_metadata.csv')\n",
    "meta_columns = ['alignment', 'display_name', 'mainnet_chain_id','op_based_version','is_op_chain','oplabs_db_schema']\n",
    "opstack_metadata = opstack_metadata[meta_columns][~opstack_metadata['oplabs_db_schema'].isna()]\n",
    "\n",
    "opstack_metadata = opstack_metadata.rename(columns={'mainnet_chain_id':'chain_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/by/kltjc8yd0yz_7_wrtrzhrm9m0000gn/T/ipykernel_29430/4152098967.py:1: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation.\n",
      "  aggregated_df_map = aggregated_df.merge(opstack_metadata[['chain_id','display_name']], on = 'chain_id', how = 'left')\n"
     ]
    }
   ],
   "source": [
    "aggregated_df_map = aggregated_df.merge(opstack_metadata[['chain_id','display_name']], on = 'chain_id', how = 'left')\n",
    "\n",
    "# aggregated_df_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-08 09:14:10.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mduneapi_utils\u001b[0m:\u001b[36mget_dune_data\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mResults available at https://dune.com/queries/3807789?total_days=28&date_end=2024-05-30+00%3A00%3A00\u001b[0m\n",
      "2024-06-08 09:14:11,994 INFO dune_client.api.base executing 3807789 on medium cluster\n",
      "2024-06-08 09:14:12,285 INFO dune_client.api.base waiting for query execution 01HZVZRD7NVRMZ9VBW4A716NKZ to complete: ExecutionState.PENDING (queue position: 1)\n",
      "2024-06-08 09:14:13,434 INFO dune_client.api.base waiting for query execution 01HZVZRD7NVRMZ9VBW4A716NKZ to complete: ExecutionState.PENDING (queue position: 1)\n",
      "2024-06-08 09:14:14,584 INFO dune_client.api.base waiting for query execution 01HZVZRD7NVRMZ9VBW4A716NKZ to complete: ExecutionState.PENDING (queue position: 1)\n",
      "2024-06-08 09:14:15,740 INFO dune_client.api.base waiting for query execution 01HZVZRD7NVRMZ9VBW4A716NKZ to complete: ExecutionState.PENDING (queue position: 1)\n",
      "2024-06-08 09:14:16,917 INFO dune_client.api.base waiting for query execution 01HZVZRD7NVRMZ9VBW4A716NKZ to complete: ExecutionState.PENDING (queue position: 1)\n",
      "\u001b[32m2024-06-08 09:14:18.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mduneapi_utils\u001b[0m:\u001b[36mget_dune_data\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1m✨ Results saved as csv_outputs/my_query_results.csv, with 543 rows and 37 columns.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Pull aggregate L1 data\n",
    "query_id = 3807789\n",
    "\n",
    "# if query_id == None:\n",
    "#         dune_query = '''\n",
    "#         SELECT *\n",
    "#         FROM dune.oplabspbc.result_op_stack_chains_l_1_data_with_op_chains_from_gs --https://dune.com/queries/3397786\n",
    "#         WHERE dt >= DATE_TRUNC('day',NOW() - interval '{{total_days}}' day)\n",
    "#         AND dt < DATE_TRUNC('day',NOW())\n",
    "#         '''\n",
    "\n",
    "#         dotenv.load_dotenv()\n",
    "#         dune = DuneClient(os.environ[\"DUNE_API_KEY\"])\n",
    "\n",
    "#         query = dune.create_query(\n",
    "#                 name=\"aggregate L1 data\",\n",
    "#                 query_sql=dune_query,\n",
    "#                 params = [QueryParameter.number_type(name=\"total_days\", value=days_of_data)]\n",
    "#                 )\n",
    "#         query_id = query.base.query_id\n",
    "#         print(f\"Created query with id {query.base.query_id}\")\n",
    "# else:\n",
    "        \n",
    "#         query_id = query_id\n",
    "\n",
    "param_dt = du.generate_query_parameter(days_of_data, 'total_days','text')\n",
    "if end_date != -1: #end date config\n",
    "        param_end = du.generate_query_parameter(end_date + ' 00:00:00', 'date_end','date')\n",
    "        param_list = [param_dt,param_end]\n",
    "else:\n",
    "        param_list = [param_dt]\n",
    "\n",
    "dune_df = du.get_dune_data(query_id, params=param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>display_name</th>\n",
       "      <th>block_date</th>\n",
       "      <th>num_l1_submissions</th>\n",
       "      <th>num_l1_txs_inbox</th>\n",
       "      <th>l1_blobgas_purchased_inbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>BOB (Build on Bitcoin)</td>\n",
       "      <td>2024-05-13</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1835008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>CyberConnect</td>\n",
       "      <td>2024-05-17</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>OP Mainnet</td>\n",
       "      <td>2024-05-27</td>\n",
       "      <td>2160</td>\n",
       "      <td>360</td>\n",
       "      <td>283115520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>Ancient8</td>\n",
       "      <td>2024-05-12</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Lyra</td>\n",
       "      <td>2024-05-27</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               display_name block_date  num_l1_submissions  num_l1_txs_inbox  \\\n",
       "99   BOB (Build on Bitcoin) 2024-05-13                  14                 8   \n",
       "408            CyberConnect 2024-05-17                 138               138   \n",
       "197              OP Mainnet 2024-05-27                2160               360   \n",
       "533                Ancient8 2024-05-12                  47                47   \n",
       "195                    Lyra 2024-05-27                  13                13   \n",
       "\n",
       "     l1_blobgas_purchased_inbox  \n",
       "99                    1835008.0  \n",
       "408                         NaN  \n",
       "197                 283115520.0  \n",
       "533                         NaN  \n",
       "195                         NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dune_df = dune_df[['name','dt','num_l1_submissions','num_l1_txs_inbox','l1_blobgas_purchased_inbox']]\n",
    "dune_df['dt'] = pd.to_datetime(dune_df['dt']).dt.tz_localize(None)\n",
    "dune_df = dune_df.rename(columns={'name':'display_name','dt':'block_date'})\n",
    "dune_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>block_date</th>\n",
       "      <th>chain_id</th>\n",
       "      <th>total_len_encoded_transaction</th>\n",
       "      <th>average_len_encoded_transaction</th>\n",
       "      <th>transaction_count</th>\n",
       "      <th>total_len_compressed_transaction</th>\n",
       "      <th>average_len_compressed_transaction</th>\n",
       "      <th>total_estimatedSize</th>\n",
       "      <th>average_estimatedSize</th>\n",
       "      <th>chain_name</th>\n",
       "      <th>display_name</th>\n",
       "      <th>l1_blobgas_purchased_inbox</th>\n",
       "      <th>blobgas_per_l2_tx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2024-05-03</td>\n",
       "      <td>10</td>\n",
       "      <td>586688546</td>\n",
       "      <td>1025.978819</td>\n",
       "      <td>571833</td>\n",
       "      <td>241138071</td>\n",
       "      <td>421.693171</td>\n",
       "      <td>1.865620e+08</td>\n",
       "      <td>326.252614</td>\n",
       "      <td>op</td>\n",
       "      <td>OP Mainnet</td>\n",
       "      <td>191102976.0</td>\n",
       "      <td>334.193682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>10</td>\n",
       "      <td>614298058</td>\n",
       "      <td>1274.448214</td>\n",
       "      <td>482011</td>\n",
       "      <td>274387689</td>\n",
       "      <td>569.256073</td>\n",
       "      <td>2.157649e+08</td>\n",
       "      <td>447.634765</td>\n",
       "      <td>op</td>\n",
       "      <td>OP Mainnet</td>\n",
       "      <td>228851712.0</td>\n",
       "      <td>474.785248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-30</td>\n",
       "      <td>10</td>\n",
       "      <td>519209274</td>\n",
       "      <td>991.033283</td>\n",
       "      <td>523907</td>\n",
       "      <td>227948102</td>\n",
       "      <td>435.092682</td>\n",
       "      <td>1.746110e+08</td>\n",
       "      <td>333.286267</td>\n",
       "      <td>op</td>\n",
       "      <td>OP Mainnet</td>\n",
       "      <td>177733632.0</td>\n",
       "      <td>339.246530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>10</td>\n",
       "      <td>689924791</td>\n",
       "      <td>1318.194967</td>\n",
       "      <td>523386</td>\n",
       "      <td>293814683</td>\n",
       "      <td>561.372836</td>\n",
       "      <td>2.297782e+08</td>\n",
       "      <td>439.022495</td>\n",
       "      <td>op</td>\n",
       "      <td>OP Mainnet</td>\n",
       "      <td>243793920.0</td>\n",
       "      <td>465.801378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2024-05-09</td>\n",
       "      <td>10</td>\n",
       "      <td>744046362</td>\n",
       "      <td>1427.692754</td>\n",
       "      <td>521153</td>\n",
       "      <td>307931954</td>\n",
       "      <td>590.866701</td>\n",
       "      <td>2.424341e+08</td>\n",
       "      <td>465.187878</td>\n",
       "      <td>op</td>\n",
       "      <td>OP Mainnet</td>\n",
       "      <td>257163264.0</td>\n",
       "      <td>493.450607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   block_date  chain_id  total_len_encoded_transaction  \\\n",
       "27 2024-05-03        10                      586688546   \n",
       "16 2024-05-14        10                      614298058   \n",
       "0  2024-05-30        10                      519209274   \n",
       "1  2024-05-29        10                      689924791   \n",
       "21 2024-05-09        10                      744046362   \n",
       "\n",
       "    average_len_encoded_transaction  transaction_count  \\\n",
       "27                      1025.978819             571833   \n",
       "16                      1274.448214             482011   \n",
       "0                        991.033283             523907   \n",
       "1                       1318.194967             523386   \n",
       "21                      1427.692754             521153   \n",
       "\n",
       "    total_len_compressed_transaction  average_len_compressed_transaction  \\\n",
       "27                         241138071                          421.693171   \n",
       "16                         274387689                          569.256073   \n",
       "0                          227948102                          435.092682   \n",
       "1                          293814683                          561.372836   \n",
       "21                         307931954                          590.866701   \n",
       "\n",
       "    total_estimatedSize  average_estimatedSize chain_name display_name  \\\n",
       "27         1.865620e+08             326.252614         op   OP Mainnet   \n",
       "16         2.157649e+08             447.634765         op   OP Mainnet   \n",
       "0          1.746110e+08             333.286267         op   OP Mainnet   \n",
       "1          2.297782e+08             439.022495         op   OP Mainnet   \n",
       "21         2.424341e+08             465.187878         op   OP Mainnet   \n",
       "\n",
       "    l1_blobgas_purchased_inbox  blobgas_per_l2_tx  \n",
       "27                 191102976.0         334.193682  \n",
       "16                 228851712.0         474.785248  \n",
       "0                  177733632.0         339.246530  \n",
       "1                  243793920.0         465.801378  \n",
       "21                 257163264.0         493.450607  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate L2 : L1 ratio metrics\n",
    "combined_df = aggregated_df_map.merge(dune_df[['display_name','block_date','l1_blobgas_purchased_inbox']], on =['display_name','block_date'], how = 'inner')\n",
    "combined_df['blobgas_per_l2_tx'] = combined_df['l1_blobgas_purchased_inbox'] / combined_df['transaction_count']\n",
    "\n",
    "combined_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(aggregated_df['encoded_transaction'][0])\n",
    "# print(len(aggregated_df['encoded_transaction'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted averages and mean\n",
    "def weighted_avg(df, value_column, weight_column):\n",
    "    return (df[value_column] * df[weight_column]).sum() / df[weight_column].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>chain_id</th>\n",
       "      <th>chain_name</th>\n",
       "      <th>display_name</th>\n",
       "      <th>average_len_encoded_transaction</th>\n",
       "      <th>average_len_compressed_transaction</th>\n",
       "      <th>average_estimatedSize</th>\n",
       "      <th>average_blobgas_per_l2_tx</th>\n",
       "      <th>wt_average_blobgas_per_l2_tx</th>\n",
       "      <th>average_daily_l1_blobgas_purchased_inbox</th>\n",
       "      <th>average_daily_transaction_count</th>\n",
       "      <th>start_dt</th>\n",
       "      <th>end_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>op</td>\n",
       "      <td>OP Mainnet</td>\n",
       "      <td>1246.553512</td>\n",
       "      <td>531.635282</td>\n",
       "      <td>415.872157</td>\n",
       "      <td>436.427916</td>\n",
       "      <td>436.427916</td>\n",
       "      <td>2.222279e+08</td>\n",
       "      <td>509197.25</td>\n",
       "      <td>2024-05-03</td>\n",
       "      <td>2024-05-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  chain_id chain_name display_name  average_len_encoded_transaction  \\\n",
       "0      0        10         op   OP Mainnet                      1246.553512   \n",
       "\n",
       "   average_len_compressed_transaction  average_estimatedSize  \\\n",
       "0                          531.635282             415.872157   \n",
       "\n",
       "   average_blobgas_per_l2_tx  wt_average_blobgas_per_l2_tx  \\\n",
       "0                 436.427916                    436.427916   \n",
       "\n",
       "   average_daily_l1_blobgas_purchased_inbox  average_daily_transaction_count  \\\n",
       "0                              2.222279e+08                        509197.25   \n",
       "\n",
       "    start_dt     end_dt  \n",
       "0 2024-05-03 2024-05-30  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agg_cols = ['average_len_encoded_transaction','average_estimatedSize','transaction_count','l1_blobgas_purchased_inbox','blobgas_per_l2_tx']\n",
    "grouped_df = combined_df.groupby(['chain_id','chain_name','display_name'])\n",
    "total_aggregated_df = grouped_df.apply(\n",
    "    lambda x: pd.Series({\n",
    "        'average_len_encoded_transaction': weighted_avg(x, 'average_len_encoded_transaction', 'transaction_count'),\n",
    "        'average_len_compressed_transaction': weighted_avg(x, 'average_len_compressed_transaction', 'transaction_count'),\n",
    "        'average_estimatedSize': weighted_avg(x, 'average_estimatedSize', 'transaction_count'),\n",
    "        'average_blobgas_per_l2_tx': x['l1_blobgas_purchased_inbox'].sum() / x['transaction_count'].sum(),\n",
    "        'wt_average_blobgas_per_l2_tx': weighted_avg(x, 'blobgas_per_l2_tx', 'transaction_count'),\n",
    "        'average_daily_l1_blobgas_purchased_inbox': x['l1_blobgas_purchased_inbox'].mean(),\n",
    "        'average_daily_transaction_count': x['transaction_count'].mean(),\n",
    "        'start_dt': x['block_date'].min(),\n",
    "        'end_dt': x['block_date'].max()\n",
    "    })\n",
    ").reset_index()\n",
    "total_aggregated_df\n",
    "total_aggregated_df =total_aggregated_df.reset_index()\n",
    "total_aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to: outputs/l2_output_20240608_0914.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# Generate current timestamp\n",
    "current_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "# Define the file path\n",
    "file_path = f\"outputs/l2_output_{current_timestamp}.csv\"\n",
    "total_file_path = f\"outputs/total_l2_output_{current_timestamp}.csv\"\n",
    "# Save the DataFrame to CSV\n",
    "aggregated_df_map.to_csv(file_path, index=False)\n",
    "total_aggregated_df.to_csv(total_file_path, index=False)\n",
    "print(f\"DataFrame saved to: {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
