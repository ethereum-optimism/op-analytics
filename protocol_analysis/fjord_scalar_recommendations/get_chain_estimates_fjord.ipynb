{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas web3 hexbytes rlp fastlz clickhouse-connect\n",
    "# ! pip install python-dotenv\n",
    "# ! pip install dune-client\n",
    "# ! pip install loguru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readme\n",
    "FastLZ needs Python version 3.9x or lower, make sure your environment is using a later python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from web3 import Web3\n",
    "from hexbytes import HexBytes\n",
    "import ast\n",
    "import rlp\n",
    "from rlp.sedes import Binary, big_endian_int, binary, List\n",
    "from eth_utils import to_bytes, to_hex, int_to_big_endian\n",
    "import fastlz\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "import time\n",
    "from dune_client.client import DuneClient\n",
    "from dune_client.types import QueryParameter\n",
    "dotenv.load_dotenv()\n",
    "sys.path.append(\"../../helper_functions\")\n",
    "import clickhouse_utils as ch\n",
    "import duneapi_utils as du\n",
    "sys.path.pop()\n",
    "\n",
    "client = ch.connect_to_clickhouse_db() #Default is OPLabs DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) #Supress internal fastlz warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run configs\n",
    "schemas_to_select = [\n",
    "        # 'op', \n",
    "        # 'base',\n",
    "        'mode',\n",
    "        # 'fraxtal',\n",
    "        'zora'\n",
    "        ]  # Add more schemas as needed\n",
    "days_of_data = 28\n",
    "\n",
    "#FastLZ Regression Metrics\n",
    "# Specs - https://specs.optimism.io/fjord/exec-engine.html?search=#fjord-l1-cost-fee-changes-fastlz-estimator\n",
    "intercept = -42_585_600\n",
    "fastlzCoef = 836_500\n",
    "minTransactionSize = 100\n",
    "scaled_by = 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "csv_path = '../../op_chains_tracking/outputs/chain_metadata.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Filter the DataFrame based on the schemas_to_select list\n",
    "filtered_df = df[df['oplabs_db_schema'].isin(schemas_to_select)]\n",
    "\n",
    "# Select the required columns and convert to a list of dictionaries\n",
    "chain_mappings_list = filtered_df[['oplabs_db_schema', 'display_name', 'mainnet_chain_id']].rename(\n",
    "    columns={'oplabs_db_schema': 'schema_name', 'mainnet_chain_id': 'chain_id'}\n",
    ").to_dict(orient='records')\n",
    "\n",
    "# Print the resulting list of dictionaries\n",
    "# print(chain_mappings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test transaction receipt\n",
    "# from web3 import Web3\n",
    "# op_rpc = os.getenv(\"OP_PUBLIC_RPC\")\n",
    "# w3 = Web3(Web3.HTTPProvider(op_rpc))\n",
    "\n",
    "# tx_test = '0xcea81f2e836a37b38ba82afd37e6f66c02e348e7b89538aa232013d91edcb926'\n",
    "# tx = w3.eth.get_transaction(tx_test)\n",
    "# txr = w3.eth.get_transaction_receipt(tx_test)\n",
    "# # # txraw = w3.eth.get_raw_transaction(tx_test)\n",
    "# print(tx)\n",
    "# # print(txr)\n",
    "# # # print(txraw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may not sufficent due to missing transaction signature fields\n",
    "\n",
    "# Get L2 Txs from Clickhouse / Goldsky\n",
    "query_by_day = '''\n",
    "        SELECT distinct @chain_id@ as chain_id, nonce, gas, gas_price,\n",
    "                to_address, value, input, block_timestamp, hash\n",
    "        FROM @chain_db_name@_transactions\n",
    "        WHERE gas_price > 0\n",
    "        # 1 day chunk\n",
    "        AND block_timestamp < DATE_TRUNC('day',NOW()) - interval '@day_num@ days'\n",
    "        AND block_timestamp >= DATE_TRUNC('day',NOW()) - (interval '@day_num@ days') - (interval '1 day')\n",
    "\n",
    "        SETTINGS max_execution_time = 7000\n",
    "'''\n",
    "# AND hash = '0xcea81f2e836a37b38ba82afd37e6f66c02e348e7b89538aa232013d91edcb926'\n",
    "# AND block_number = 120731426\n",
    "\n",
    "# txs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process transactions and RLP encode\n",
    "#https://ethereum.org/en/developers/docs/transactions/\n",
    "\n",
    "# NOTE THE RLP ENCODING IS NOT 1:1 WITH ETHERSCAN YET (but it's ~close-ish)\n",
    "def process_and_encode_transaction(row):\n",
    "    try:\n",
    "        # Process \"to\" field\n",
    "        to_field = row['to_address']\n",
    "        if isinstance(to_field, str):\n",
    "            if to_field:\n",
    "                to_field = to_field.decode('utf-8')\n",
    "                to_bytes = bytes.fromhex(to_field[2:])\n",
    "            else:\n",
    "                to_bytes = b''  # Set to an empty bytes object if \"to\" address is null\n",
    "        elif isinstance(to_field, bytes):\n",
    "            if to_field.startswith(b'0x'):\n",
    "                to_field = to_field.decode('utf-8')\n",
    "                to_bytes = bytes.fromhex(to_field[2:])\n",
    "            else:\n",
    "                to_bytes = to_field\n",
    "        else:\n",
    "            raise ValueError(\"Invalid 'to_address' field type\")\n",
    "\n",
    "        # Prepare transaction parameters\n",
    "        try:\n",
    "            tx_params = {\n",
    "                'nonce': int_to_big_endian(int(row['nonce'])),\n",
    "                'gasPrice': int_to_big_endian(int(row['gas_price'])),\n",
    "                'gas': int_to_big_endian(int(row['gas'])),\n",
    "                'to': to_bytes,\n",
    "                'value': int_to_big_endian(int(row['value'])) if row['value'] != 0 else b'',  # Encode value as byte array if 0\n",
    "                'input': bytes.fromhex(row['input'][2:]),\n",
    "                'v': int_to_big_endian(int(row['v'])),  # Convert v to a bytes object\n",
    "                'r': bytes.fromhex(row['r'][2:]),\n",
    "                's': bytes.fromhex(row['s'][2:])\n",
    "            }\n",
    "        except:\n",
    "            print(row)\n",
    "\n",
    "        # # Print transaction parameters for debugging\n",
    "        # for key, value in tx_params.items():\n",
    "        #     print(f\"{key}: {value}, {type(value)}\")\n",
    "\n",
    "        # Prepare the transaction fields for RLP encoding\n",
    "        transaction = [\n",
    "            tx_params['nonce'],\n",
    "            tx_params['gasPrice'],\n",
    "            tx_params['gas'],\n",
    "            tx_params['to'],\n",
    "            tx_params['value'],\n",
    "            tx_params['input'],\n",
    "            tx_params['v'],\n",
    "            tx_params['r'],\n",
    "            tx_params['s']\n",
    "        ]\n",
    "\n",
    "        # Encode the entire transaction\n",
    "        encoded_tx = rlp.encode(transaction)\n",
    "        encoded_tx_hex = \"0x\" + encoded_tx.hex()\n",
    "        return encoded_tx_hex, len(encoded_tx)\n",
    "\n",
    "    except (ValueError, TypeError, UnicodeDecodeError) as e:\n",
    "        print(\"Error:\", e)\n",
    "        print(\"Failed Transaction Info:\")\n",
    "        print(row)\n",
    "        return None, None\n",
    "\n",
    "# Function to compress transaction data\n",
    "def compress_transaction(encoded_transaction):\n",
    "\n",
    "    hex_string = encoded_transaction[2:]\n",
    "    # Convert the hexadecimal string to bytes\n",
    "    byte_string = bytes.fromhex(hex_string)\n",
    "    compressed_data = fastlz.compress(byte_string)\n",
    "\n",
    "    return compressed_data.hex(), len(compressed_data)\n",
    "# Define a function to apply to each row of the DataFrame\n",
    "def process_and_compress_transaction(row):\n",
    "    encoded_tx = row['encoded_transaction']\n",
    "    compressed_tx, len_tx = compress_transaction(encoded_tx)\n",
    "    return compressed_tx, len_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zora : day 0\n",
      "        Query Done: Completed in 2.83 seconds\n",
      "        Encoding Done: Completed in 6.19 seconds\n",
      "        Compression Done: Completed in 6.19 seconds\n",
      "        Estimation Done: Completed in 0.92 seconds\n",
      "zora : day 1\n",
      "        Query Done: Completed in 3.01 seconds\n",
      "        Encoding Done: Completed in 5.99 seconds\n",
      "        Compression Done: Completed in 5.99 seconds\n",
      "        Estimation Done: Completed in 0.83 seconds\n",
      "zora : day 2\n",
      "        Query Done: Completed in 4.12 seconds\n",
      "        Encoding Done: Completed in 5.75 seconds\n",
      "        Compression Done: Completed in 5.75 seconds\n",
      "        Estimation Done: Completed in 0.90 seconds\n",
      "zora : day 3\n",
      "        Query Done: Completed in 3.47 seconds\n",
      "        Encoding Done: Completed in 6.46 seconds\n",
      "        Compression Done: Completed in 6.46 seconds\n",
      "        Estimation Done: Completed in 0.84 seconds\n",
      "zora : day 4\n",
      "        Query Done: Completed in 3.76 seconds\n",
      "        Encoding Done: Completed in 5.98 seconds\n",
      "        Compression Done: Completed in 5.98 seconds\n",
      "        Estimation Done: Completed in 0.92 seconds\n",
      "zora : day 5\n",
      "        Query Done: Completed in 3.05 seconds\n",
      "        Encoding Done: Completed in 5.62 seconds\n",
      "        Compression Done: Completed in 5.62 seconds\n",
      "        Estimation Done: Completed in 0.88 seconds\n",
      "zora : day 6\n",
      "        Query Done: Completed in 3.45 seconds\n",
      "        Encoding Done: Completed in 6.81 seconds\n",
      "        Compression Done: Completed in 6.81 seconds\n",
      "        Estimation Done: Completed in 1.02 seconds\n",
      "zora : day 7\n",
      "        Query Done: Completed in 4.30 seconds\n",
      "        Encoding Done: Completed in 7.43 seconds\n",
      "        Compression Done: Completed in 7.43 seconds\n",
      "        Estimation Done: Completed in 1.10 seconds\n",
      "zora : day 8\n",
      "        Query Done: Completed in 3.38 seconds\n",
      "        Encoding Done: Completed in 7.16 seconds\n",
      "        Compression Done: Completed in 7.16 seconds\n",
      "        Estimation Done: Completed in 0.99 seconds\n",
      "zora : day 9\n",
      "        Query Done: Completed in 4.26 seconds\n",
      "        Encoding Done: Completed in 6.10 seconds\n",
      "        Compression Done: Completed in 6.10 seconds\n",
      "        Estimation Done: Completed in 0.94 seconds\n",
      "zora : day 10\n",
      "        Query Done: Completed in 3.04 seconds\n",
      "        Encoding Done: Completed in 4.63 seconds\n",
      "        Compression Done: Completed in 4.63 seconds\n",
      "        Estimation Done: Completed in 0.69 seconds\n",
      "zora : day 11\n",
      "        Query Done: Completed in 3.11 seconds\n",
      "        Encoding Done: Completed in 5.36 seconds\n",
      "        Compression Done: Completed in 5.36 seconds\n",
      "        Estimation Done: Completed in 0.85 seconds\n",
      "zora : day 12\n",
      "        Query Done: Completed in 3.12 seconds\n",
      "        Encoding Done: Completed in 5.80 seconds\n",
      "        Compression Done: Completed in 5.80 seconds\n",
      "        Estimation Done: Completed in 0.87 seconds\n",
      "zora : day 13\n",
      "        Query Done: Completed in 3.53 seconds\n",
      "        Encoding Done: Completed in 6.47 seconds\n",
      "        Compression Done: Completed in 6.47 seconds\n",
      "        Estimation Done: Completed in 0.91 seconds\n",
      "zora : day 14\n",
      "        Query Done: Completed in 4.65 seconds\n",
      "        Encoding Done: Completed in 4.86 seconds\n",
      "        Compression Done: Completed in 4.86 seconds\n",
      "        Estimation Done: Completed in 0.73 seconds\n",
      "zora : day 15\n",
      "        Query Done: Completed in 3.59 seconds\n",
      "        Encoding Done: Completed in 5.75 seconds\n",
      "        Compression Done: Completed in 5.75 seconds\n",
      "        Estimation Done: Completed in 0.87 seconds\n",
      "zora : day 16\n",
      "        Query Done: Completed in 11.53 seconds\n",
      "        Encoding Done: Completed in 4.80 seconds\n",
      "        Compression Done: Completed in 4.80 seconds\n",
      "        Estimation Done: Completed in 0.73 seconds\n",
      "zora : day 17\n",
      "        Query Done: Completed in 7.01 seconds\n",
      "        Encoding Done: Completed in 5.59 seconds\n",
      "        Compression Done: Completed in 5.59 seconds\n",
      "        Estimation Done: Completed in 0.76 seconds\n",
      "zora : day 18\n",
      "        Query Done: Completed in 7.01 seconds\n",
      "        Encoding Done: Completed in 4.43 seconds\n",
      "        Compression Done: Completed in 4.43 seconds\n",
      "        Estimation Done: Completed in 0.69 seconds\n",
      "zora : day 19\n",
      "        Query Done: Completed in 6.90 seconds\n",
      "        Encoding Done: Completed in 6.31 seconds\n",
      "        Compression Done: Completed in 6.31 seconds\n",
      "        Estimation Done: Completed in 0.81 seconds\n",
      "zora : day 20\n",
      "        Query Done: Completed in 7.37 seconds\n",
      "        Encoding Done: Completed in 6.72 seconds\n",
      "        Compression Done: Completed in 6.72 seconds\n",
      "        Estimation Done: Completed in 1.05 seconds\n",
      "zora : day 21\n",
      "        Query Done: Completed in 7.10 seconds\n",
      "        Encoding Done: Completed in 5.94 seconds\n",
      "        Compression Done: Completed in 5.94 seconds\n",
      "        Estimation Done: Completed in 0.92 seconds\n",
      "zora : day 22\n",
      "        Query Done: Completed in 12.49 seconds\n",
      "        Encoding Done: Completed in 6.84 seconds\n",
      "        Compression Done: Completed in 6.84 seconds\n",
      "        Estimation Done: Completed in 1.08 seconds\n",
      "zora : day 23\n",
      "        Query Done: Completed in 6.98 seconds\n",
      "        Encoding Done: Completed in 7.34 seconds\n",
      "        Compression Done: Completed in 7.34 seconds\n",
      "        Estimation Done: Completed in 1.42 seconds\n",
      "zora : day 24\n",
      "        Query Done: Completed in 7.37 seconds\n",
      "        Encoding Done: Completed in 7.11 seconds\n",
      "        Compression Done: Completed in 7.11 seconds\n",
      "        Estimation Done: Completed in 1.12 seconds\n",
      "zora : day 25\n",
      "        Query Done: Completed in 7.23 seconds\n",
      "        Encoding Done: Completed in 6.54 seconds\n",
      "        Compression Done: Completed in 6.54 seconds\n",
      "        Estimation Done: Completed in 0.94 seconds\n",
      "zora : day 26\n",
      "        Query Done: Completed in 21.26 seconds\n",
      "        Encoding Done: Completed in 6.26 seconds\n",
      "        Compression Done: Completed in 6.26 seconds\n",
      "        Estimation Done: Completed in 1.18 seconds\n",
      "zora : day 27\n",
      "        Query Done: Completed in 18.27 seconds\n",
      "        Encoding Done: Completed in 7.41 seconds\n",
      "        Compression Done: Completed in 7.41 seconds\n",
      "        Estimation Done: Completed in 1.12 seconds\n",
      "mode : day 0\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for chain in chain_mappings_list:\n",
    "        for day_num in range(0,days_of_data):\n",
    "                result_df = None #Kill so we don't rerun\n",
    "                print(chain['schema_name'] + ' : day ' + str(day_num))\n",
    "                query_map = query_by_day\n",
    "\n",
    "                query_map = query_map.replace(\"@chain_db_name@\", chain['schema_name'])\n",
    "                query_map = query_map.replace(\"@chain_id@\", str(chain['chain_id']))\n",
    "                query_map = query_map.replace(\"@day_num@\", str(day_num))\n",
    "                \n",
    "                query_start_time = time.time()\n",
    "                try:\n",
    "                        result_df = client.query_df(query_map)\n",
    "                except UnicodeDecodeError as e:\n",
    "                        print(f\"UnicodeDecodeError: {e}\")\n",
    "                        print(f\"Problematic byte sequence: {e.object[e.start:e.end]}\")\n",
    "\n",
    "                query_end_time = time.time()  # Record the start time\n",
    "                query_elapsed_time = query_end_time - query_start_time\n",
    "                print (f\"        Query Done: Completed in {query_elapsed_time:.2f} seconds\")\n",
    "                # try:\n",
    "                # Add Dummy Signature and fields\n",
    "                result_df['access_list'] = '[]'\n",
    "                result_df['access_list'] = result_df['access_list'].apply(ast.literal_eval)\n",
    "                result_df['r'] = '0x6727a53c0972c55923242cea052dc4e1105d7b65c91c442e2741440965eac357'\n",
    "                result_df['s'] = '0x0a8e71aea623adb7b5562fb9a779634f3b84dad7be1e1f22caaa640db352a6ff'\n",
    "                result_df['v'] = '55'\n",
    "\n",
    "                # Assuming `txs_df` is your DataFrame\n",
    "                result_df[['encoded_transaction', 'len_encoded_transaction']] = result_df.apply(process_and_encode_transaction, axis=1, result_type='expand')\n",
    "                enc_end_time = time.time()  # Record the start time\n",
    "                enc_elapsed_time = enc_end_time - query_end_time\n",
    "                print (f\"        Encoding Done: Completed in {enc_elapsed_time:.2f} seconds\")\n",
    "\n",
    "                # Apply compression to each transaction in the DataFrame\n",
    "                result_df[['compressed_transaction', 'compressed_transaction_length']] = result_df.apply(process_and_compress_transaction, axis=1, result_type='expand')\n",
    "                comp_end_time = time.time()\n",
    "                comp_elapsed_time = comp_end_time - enc_end_time\n",
    "                print (f\"        Compression Done: Completed in {comp_elapsed_time:.2f} seconds\")\n",
    "                \n",
    "                # Calculate estimated size for each row\n",
    "                result_df['estimatedSize_raw'] = result_df.apply(lambda row: (intercept + (row['compressed_transaction_length'] * fastlzCoef)) / scaled_by, axis=1)\n",
    "                # Calculate minimum value for 'estimatedSize' column\n",
    "                result_df['estimatedSize'] = result_df.apply(lambda row: max(minTransactionSize, row['estimatedSize_raw']), axis=1)\n",
    "                est_end_time = time.time()\n",
    "                est_elapsed_time = est_end_time - comp_end_time\n",
    "                print (f\"        Estimation Done: Completed in {est_elapsed_time:.2f} seconds\")\n",
    "\n",
    "                # Agg L2\n",
    "                # Convert block_timestamp to date (truncate to day)\n",
    "                result_df['block_date'] = pd.to_datetime(result_df['block_timestamp']).dt.date\n",
    "                result_df['block_date'] = pd.to_datetime(result_df['block_date']).dt.tz_localize(None)\n",
    "                grouped_df = result_df.groupby(['block_date', 'chain_id'])\n",
    "                # Define aggregation functions\n",
    "                agg_functions = {\n",
    "                        'len_encoded_transaction': ['sum', 'mean', 'count'],\n",
    "                        'compressed_transaction_length': ['sum', 'mean'],\n",
    "                        'estimatedSize': ['sum', 'mean']\n",
    "                }\n",
    "                # Perform aggregation\n",
    "                aggregated_df = grouped_df.agg(agg_functions).reset_index()\n",
    "                # Rename columns for clarity\n",
    "                aggregated_df.columns = ['block_date', 'chain_id', \n",
    "                                        'total_len_encoded_transaction', 'average_len_encoded_transaction', 'transaction_count',\n",
    "                                        'total_len_compressed_transaction','average_len_compressed_transaction',\n",
    "                                        'total_estimatedSize', 'average_estimatedSize']\n",
    "                try:\n",
    "                        aggregated_df['chain_name'] = chain['schema_name']\n",
    "                        dfs.append(aggregated_df)\n",
    "                except:\n",
    "                        print('nothing to append')\n",
    "                        continue\n",
    "\n",
    "aggregated_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opstack_metadata = pd.read_csv('../../op_chains_tracking/outputs/chain_metadata.csv')\n",
    "meta_columns = ['alignment', 'display_name', 'mainnet_chain_id','op_based_version','is_op_chain','oplabs_db_schema']\n",
    "opstack_metadata = opstack_metadata[meta_columns][~opstack_metadata['oplabs_db_schema'].isna()]\n",
    "\n",
    "opstack_metadata = opstack_metadata.rename(columns={'mainnet_chain_id':'chain_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_map = aggregated_df.merge(opstack_metadata[['chain_id','display_name']], on = 'chain_id', how = 'left')\n",
    "\n",
    "# aggregated_df_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull aggregate L1 data\n",
    "query_id = 3807789\n",
    "\n",
    "if query_id == None:\n",
    "        dune_query = '''\n",
    "        SELECT *\n",
    "        FROM dune.oplabspbc.result_op_stack_chains_l_1_data_with_op_chains_from_gs --https://dune.com/queries/3397786\n",
    "        WHERE dt >= DATE_TRUNC('day',NOW() - interval '{{total_days}}' day)\n",
    "        AND dt < DATE_TRUNC('day',NOW())\n",
    "        '''\n",
    "\n",
    "        dotenv.load_dotenv()\n",
    "        dune = DuneClient(os.environ[\"DUNE_API_KEY\"])\n",
    "\n",
    "        query = dune.create_query(\n",
    "                name=\"aggregate L1 data\",\n",
    "                query_sql=dune_query,\n",
    "                params = [QueryParameter.number_type(name=\"total_days\", value=days_of_data)]\n",
    "                )\n",
    "        query_id = query.base.query_id\n",
    "        print(f\"Created query with id {query.base.query_id}\")\n",
    "else:\n",
    "        query_id = query_id\n",
    "\n",
    "param_dt = du.generate_query_parameter(days_of_data, 'total_days','text')\n",
    "\n",
    "dune_df = du.get_dune_data(query_id, params=[param_dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dune_df = dune_df[['name','dt','num_l1_submissions','num_l1_txs_inbox','l1_blobgas_purchased_inbox']]\n",
    "dune_df['dt'] = pd.to_datetime(dune_df['dt']).dt.tz_localize(None)\n",
    "dune_df = dune_df.rename(columns={'name':'display_name','dt':'block_date'})\n",
    "dune_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate L2 : L1 ratio metrics\n",
    "combined_df = aggregated_df_map.merge(dune_df[['display_name','block_date','l1_blobgas_purchased_inbox']], on =['display_name','block_date'], how = 'inner')\n",
    "combined_df['blobgas_per_l2_tx'] = combined_df['l1_blobgas_purchased_inbox'] / combined_df['transaction_count']\n",
    "\n",
    "combined_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(aggregated_df['encoded_transaction'][0])\n",
    "# print(len(aggregated_df['encoded_transaction'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted averages and mean\n",
    "def weighted_avg(df, value_column, weight_column):\n",
    "    return (df[value_column] * df[weight_column]).sum() / df[weight_column].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_cols = ['average_len_encoded_transaction','average_estimatedSize','transaction_count','l1_blobgas_purchased_inbox','blobgas_per_l2_tx']\n",
    "grouped_df = combined_df.groupby(['chain_id','chain_name','display_name'])\n",
    "total_aggregated_df = grouped_df.apply(\n",
    "    lambda x: pd.Series({\n",
    "        'average_len_encoded_transaction': weighted_avg(x, 'average_len_encoded_transaction', 'transaction_count'),\n",
    "        'average_estimatedSize': weighted_avg(x, 'average_estimatedSize', 'transaction_count'),\n",
    "        'average_blobgas_per_l2_tx': x['l1_blobgas_purchased_inbox'].sum() / x['transaction_count'].sum(),\n",
    "        'average_daily_l1_blobgas_purchased_inbox': x['l1_blobgas_purchased_inbox'].mean(),\n",
    "        'average_daily_transaction_count': x['transaction_count'].mean(),\n",
    "        'start_dt': x['dt'].min(),\n",
    "        'end_dt': x['dt'].max()\n",
    "    })\n",
    ").reset_index()\n",
    "total_aggregated_df\n",
    "total_aggregated_df =total_aggregated_df.reset_index()\n",
    "total_aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Generate current timestamp\n",
    "current_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "# Define the file path\n",
    "file_path = f\"outputs/l2_output_{current_timestamp}.csv\"\n",
    "total_file_path = f\"outputs/total_l2_output_{current_timestamp}.csv\"\n",
    "# Save the DataFrame to CSV\n",
    "aggregated_df_map.to_csv(file_path, index=False)\n",
    "total_aggregated_df.to_csv(total_file_path, index=False)\n",
    "print(f\"DataFrame saved to: {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
