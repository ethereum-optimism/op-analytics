{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of materialized view names\n",
    "mv_names = [\n",
    "    \"erc20_transfers\",\n",
    "    \"native_eth_transfers\",\n",
    "    \"daily_aggregate_transactions\",\n",
    "]\n",
    "set_days_batch_size = 7\n",
    "\n",
    "optimize_all = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "sys.path.append(\"../../helper_functions\")\n",
    "import clickhouse_utils as ch\n",
    "import opstack_metadata_utils as ops\n",
    "import goldsky_db_utils as gsb\n",
    "\n",
    "sys.path.pop()\n",
    "client = ch.connect_to_clickhouse_db()\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Chain List\n",
    "chain_configs = ops.get_superchain_metadata_by_data_source(\"oplabs\")  # OPLabs db\n",
    "# chain_configs = chain_configs[chain_configs['chain_name'].isin('op','zora')]\n",
    "\n",
    "if client is None:\n",
    "    client = ch.connect_to_clickhouse_db()\n",
    "\n",
    "\n",
    "# Function to create ClickHouse view\n",
    "def get_chain_names_from_df(df):\n",
    "    return df[\"blockchain\"].dropna().unique().tolist()\n",
    "\n",
    "\n",
    "# chain_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of chains\n",
    "chains = get_chain_names_from_df(chain_configs)\n",
    "\n",
    "chains.append(\"ethereum\")\n",
    "\n",
    "# Start date for backfilling\n",
    "start_date = datetime.date(2015, 8, 1)\n",
    "end_date = datetime.date.today() + datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_from_file(mv_name):\n",
    "    try:\n",
    "        # Try to get the directory of the current script\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # If __file__ is not defined (e.g., in Jupyter), use the current working directory\n",
    "        script_dir = os.getcwd()\n",
    "\n",
    "    query_file_path = os.path.join(script_dir, \"mv_inputs\", f\"{mv_name}.sql\")\n",
    "    print(f\"Attempting to read query from: {query_file_path}\")\n",
    "\n",
    "    try:\n",
    "        with open(query_file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Query file not found: {query_file_path}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_optimize_on_insert(option_int=1):\n",
    "    client.command(\n",
    "        f\"\"\"\n",
    "        SET optimize_on_insert = {option_int};\n",
    "        \"\"\"\n",
    "    )\n",
    "    print(f\"Set optimize_on_insert = {option_int}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_materialized_view(client, chain, mv_name):\n",
    "    table_view_name = f\"{chain}_{mv_name}\"\n",
    "    full_view_name = f\"{chain}_{mv_name}_mv\"\n",
    "    create_file_name = f\"{mv_name}_create\"\n",
    "    print(full_view_name)\n",
    "\n",
    "    # Check if create file exists\n",
    "    if not os.path.exists(f\"mv_inputs/{create_file_name}.sql\"):\n",
    "        print(f\"Create file {create_file_name}.sql does not exist. Skipping creation.\")\n",
    "    else:\n",
    "        # Check if table already exists\n",
    "        result = client.query(f\"SHOW TABLES LIKE '{table_view_name}'\")\n",
    "        if result.result_rows:\n",
    "            print(f\"Table {table_view_name} already exists. Skipping creation.\")\n",
    "        else:\n",
    "            # Create the table\n",
    "            create_query = get_query_from_file(create_file_name)\n",
    "            create_query = create_query.format(chain=chain, view_name=table_view_name)\n",
    "            client.command(create_query)\n",
    "            print(f\"Created table {table_view_name}\")\n",
    "\n",
    "    # Check if view already exists\n",
    "    result = client.query(f\"SHOW TABLES LIKE '{full_view_name}'\")\n",
    "    if result.result_rows:\n",
    "        print(f\"Materialized view {full_view_name} already exists. Skipping creation.\")\n",
    "        return\n",
    "\n",
    "    query_template = get_query_from_file(f\"{mv_name}_mv\")\n",
    "    query = query_template.format(\n",
    "        chain=chain, view_name=full_view_name, table_name=table_view_name\n",
    "    )\n",
    "    query = gsb.process_goldsky_sql(query)\n",
    "\n",
    "    # print(query)\n",
    "\n",
    "    client.command(query)\n",
    "    print(f\"Created materialized view {full_view_name}\")\n",
    "\n",
    "\n",
    "def ensure_backfill_tracking_table_exists(client):\n",
    "    check_table_query = \"\"\"\n",
    "    SELECT 1 FROM system.tables \n",
    "    WHERE database = currentDatabase() AND name = 'backfill_tracking'\n",
    "    \"\"\"\n",
    "    result = client.query(check_table_query)\n",
    "\n",
    "    if not result.result_rows:\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE backfill_tracking (\n",
    "            chain String,\n",
    "            mv_name String,\n",
    "            start_date Date,\n",
    "            end_date Date\n",
    "        ) ENGINE = MergeTree()\n",
    "        ORDER BY (chain, mv_name, start_date)\n",
    "        \"\"\"\n",
    "        client.command(create_table_query)\n",
    "        print(\"Created backfill_tracking table.\")\n",
    "    else:\n",
    "        print(\"backfill_tracking table already exists.\")\n",
    "\n",
    "\n",
    "def backfill_data(client, chain, mv_name):\n",
    "    full_view_name = f\"{chain}_{mv_name}_mv\"\n",
    "    full_table_name = f\"{chain}_{mv_name}\"\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date < end_date:\n",
    "        attempts = 1\n",
    "        is_success = 0\n",
    "        days_batch_size = set_days_batch_size\n",
    "\n",
    "        while is_success == 0 & attempts < 3:\n",
    "            batch_size = datetime.timedelta(days=days_batch_size)\n",
    "\n",
    "            batch_end = min(current_date + batch_size, end_date)\n",
    "\n",
    "            # Check if this range has been backfilled\n",
    "            check_query = f\"\"\"\n",
    "            SELECT 1\n",
    "            FROM backfill_tracking\n",
    "            WHERE chain = '{chain}'\n",
    "            AND mv_name = '{mv_name}'\n",
    "            HAVING \n",
    "                MIN(start_date) <= toDate('{current_date}')\n",
    "            AND MAX(end_date) >= toDate('{batch_end}')\n",
    "            LIMIT 1\n",
    "            \"\"\"\n",
    "            result = client.query(check_query)\n",
    "            # print(check_query)\n",
    "            # print(result.result_rows)\n",
    "            # Check if data already exists\n",
    "\n",
    "            if not result.result_rows:\n",
    "                # No record of backfill, proceed\n",
    "                query_template = get_query_from_file(f\"{mv_name}_backfill\")\n",
    "                query = query_template.format(\n",
    "                    view_name=full_view_name,\n",
    "                    chain=chain,\n",
    "                    start_date=current_date,\n",
    "                    end_date=batch_end,\n",
    "                    table_name=full_table_name,\n",
    "                )\n",
    "                query = gsb.process_goldsky_sql(query)\n",
    "\n",
    "                # print(query)\n",
    "                try:\n",
    "                    # print(query)\n",
    "                    set_optimize_on_insert()\n",
    "                    client.command(query)\n",
    "                    # Record the backfill\n",
    "                    track_query = f\"\"\"\n",
    "                    INSERT INTO backfill_tracking (chain, mv_name, start_date, end_date)\n",
    "                    VALUES ('{chain}', '{mv_name}', toDate('{current_date}'), toDate('{batch_end}'))\n",
    "                    \"\"\"\n",
    "                    client.command(track_query)\n",
    "\n",
    "                    print(\n",
    "                        f\"Backfilled data for {full_view_name} from {current_date} to {batch_end}\"\n",
    "                    )\n",
    "\n",
    "                    # Optimize the newly backfilled partition\n",
    "                    # optimize_partition(client, full_view_name, current_date, batch_end)\n",
    "                    is_success = 1\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Error during backfill for {full_view_name} from {current_date} to {batch_end}: {str(e)}\"\n",
    "                    )\n",
    "                    days_batch_size = 1\n",
    "                    attempts += 1\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Data already backfilled for {full_view_name} from {current_date} to {batch_end}. Skipping.\"\n",
    "                )\n",
    "                is_success = 1\n",
    "                # if optimize_all:\n",
    "                #     optimize_partition(client, full_view_name, current_date, batch_end)\n",
    "\n",
    "            current_date = batch_end + datetime.timedelta(days=1)\n",
    "\n",
    "\n",
    "# def optimize_partition(client, full_view_name, start_date, end_date):\n",
    "#     # First, let's get the actual partition names\n",
    "#     partition_query = f\"\"\"\n",
    "#     SELECT DISTINCT partition\n",
    "#     FROM system.parts\n",
    "#     WHERE table = '{full_view_name.split('.')[-1]}'\n",
    "#       AND database = '{full_view_name.split('.')[0]}'\n",
    "#       AND active\n",
    "#     ORDER BY partition\n",
    "#     \"\"\"\n",
    "#     print(partition_query)\n",
    "#     partitions = [row[0] for row in client.query(partition_query).result_rows]\n",
    "\n",
    "#     print(f\"Available partitions for {full_view_name}: {partitions}\")\n",
    "\n",
    "#     # Filter partitions within our date range\n",
    "#     start_partition = start_date.strftime('%Y%m')\n",
    "#     end_partition = end_date.strftime('%Y%m')\n",
    "#     partitions_to_optimize = [p for p in partitions if start_partition <= p <= end_partition]\n",
    "\n",
    "#     if not partitions_to_optimize:\n",
    "#         print(f\"No partitions found for {full_view_name} between {start_date} and {end_date}\")\n",
    "#         return\n",
    "\n",
    "#     try:\n",
    "#         for partition in partitions_to_optimize:\n",
    "#             optimize_query = f\"\"\"\n",
    "#             OPTIMIZE TABLE {full_view_name}\n",
    "#             PARTITION '{partition}'\n",
    "#             FINAL SETTINGS max_execution_time = 3000\n",
    "#             \"\"\"\n",
    "#             print(f\"Attempting to optimize {full_view_name} for partition {partition}\")\n",
    "#             client.command(optimize_query)\n",
    "#             print(f\"Successfully optimized partition {partition} for {full_view_name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error executing OPTIMIZE TABLE for {full_view_name}\")\n",
    "#         print(f\"  Partition: {partition}\")\n",
    "#         print(f\"  Date range: {start_date} to {end_date}\")\n",
    "#         print(f\"  Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_materialized_view(client, chain, mv_name):\n",
    "    full_view_name = f\"{chain}_{mv_name}_mv\"\n",
    "    table_name = f\"{chain}_{mv_name}\"\n",
    "\n",
    "    try:\n",
    "        # Drop the existing materialized view\n",
    "        client.command(f\"DROP TABLE IF EXISTS {full_view_name}\")\n",
    "        # client.command(f\"DROP MATERIALIZED VIEW IF EXISTS {full_view_name}\")\n",
    "        print(f\"Dropped materialized view {full_view_name}\")\n",
    "\n",
    "        # Drop the existing materialized view\n",
    "        client.command(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "        print(f\"Dropped table {table_name}\")\n",
    "\n",
    "        # Recreate the materialized view using the existing function\n",
    "        create_materialized_view(client, chain, mv_name)\n",
    "        print(f\"Recreated materialized view {full_view_name}\")\n",
    "\n",
    "        # Clear the backfill tracking for this view\n",
    "        client.command(\n",
    "            f\"\"\"\n",
    "        ALTER TABLE backfill_tracking \n",
    "        DELETE WHERE chain = '{chain}' AND mv_name = '{mv_name}'\n",
    "        \"\"\"\n",
    "        )\n",
    "        print(f\"Cleared backfill tracking for {full_view_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error resetting materialized view {full_view_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To reset a view\n",
    "# reset_materialized_view(client, 'op', 'daily_aggregate_transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "ensure_backfill_tracking_table_exists(client)\n",
    "\n",
    "for chain in chains:\n",
    "    print(f\"Processing chain: {chain}\")\n",
    "    for mv_name in mv_names:\n",
    "        create_materialized_view(client, chain, mv_name)\n",
    "        backfill_data(client, chain, mv_name)\n",
    "\n",
    "    print(f\"Completed processing for {chain}\")\n",
    "\n",
    "print(\"All chains and views processed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
