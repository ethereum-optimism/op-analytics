{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of materialized view names\n",
    "mvs = [\n",
    "    {'mv_name': 'across_bridging_txs_v3', 'start_date': '2024-07-01', 'chains': ''},\n",
    "    # # {'mv_name': 'across_bridging_txs_v3_logs_only', 'start_date': '2024-07-01'},\n",
    "    # # {'mv_name': 'filtered_logs_l2s', 'start_date': ''},\n",
    "    # ### {'mv_name': 'erc20_transfers', 'start_date': ''},\n",
    "    # ### {'mv_name': 'native_eth_transfers', 'start_date': ''},\n",
    "    # ### {'mv_name': 'transactions_unique', 'start_date': ''},\n",
    "    # # {'mv_name': 'daily_aggregate_transactions_to', 'start_date': ''},\n",
    "    # {'mv_name': 'event_emitting_transactions_l2s', 'start_date': ''},\n",
    "    {'mv_name': 'weekly_retention_rate_temp', 'start_date': '2024-01-01', 'chains': 'superchain'},\n",
    "    {'mv_name': 'event_emitting_transactions_l2s_nofilter', 'start_date': '2024-01-01', 'chains': ''},\n",
    "    # {'mv_name': 'event_emitting_transactions_l2s_nofilter', 'start_date': '2024-01-01'},\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "set_days_batch_size = 30# 7 #30\n",
    "\n",
    "optimize_all = False #True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic0_maps = [\n",
    "        {'table_type': 'materialized', 'topic0_func': \"topic0\"},\n",
    "        {'table_type': 'raw', 'topic0_func': \"arrayElement(splitByString(',', topics), 1)\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import traceback\n",
    "sys.path.append(\"../../helper_functions\")\n",
    "import clickhouse_utils as ch\n",
    "import opstack_metadata_utils as ops\n",
    "import goldsky_db_utils as gsb\n",
    "sys.path.pop()\n",
    "client = ch.connect_to_clickhouse_db()\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_names = [item['mv_name'] for item in mvs]\n",
    "\n",
    "# Get Chain List\n",
    "chain_configs = ops.get_superchain_metadata_by_data_source('oplabs') # OPLabs db\n",
    "\n",
    "if client is None:\n",
    "        client = ch.connect_to_clickhouse_db()\n",
    "\n",
    "# Function to create ClickHouse view\n",
    "def get_chain_names_from_df(df):\n",
    "    return df['blockchain'].dropna().unique().tolist()\n",
    "\n",
    "# chain_configs = chain_configs[chain_configs['chain_name'] == 'xterio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of chains\n",
    "# chains = get_chain_names_from_df(chain_configs)\n",
    "\n",
    "# Start date for backfilling\n",
    "start_date = datetime.date(2021, 11, 1)\n",
    "# start_date = datetime.date(2024, 5, 1)\n",
    "end_date = datetime.date.today() #+ datetime.timedelta(days=1)\n",
    "\n",
    "print(end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_start_date = start_date\n",
    "og_end_date = end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_from_file(mv_name):\n",
    "    try:\n",
    "        # Try to get the directory of the current script\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # If __file__ is not defined (e.g., in Jupyter), use the current working directory\n",
    "        script_dir = os.getcwd()\n",
    "    \n",
    "    query_file_path = os.path.join(script_dir, 'mv_inputs', f'{mv_name}.sql')\n",
    "    # print(f\"Attempting to read query from: {query_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(query_file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Query file not found: {query_file_path}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_optimize_on_insert(option_int = 1):\n",
    "    client.command(f\"\"\"\n",
    "        SET optimize_on_insert = {option_int};\n",
    "        \"\"\")\n",
    "    print(f\"Set optimize_on_insert = {option_int}\")\n",
    "\n",
    "def do_optimize_final(client, chain_name, mv_name):\n",
    "    # print(f'Optimizing: {chain_name}_{mv_name}')\n",
    "    opcmd = f'OPTIMIZE TABLE {chain_name}_{mv_name} FINAL;'\n",
    "    print(opcmd)\n",
    "    client.command(opcmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import clickhouse_connect\n",
    "from clickhouse_connect.driver.exceptions import ClickHouseError\n",
    "\n",
    "def create_materialized_view(client, chain, mv_name, block_time = 2):\n",
    "    table_view_name = f'{chain}_{mv_name}'\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    create_file_name = f'{mv_name}_create'\n",
    "    print(full_view_name)\n",
    "    \n",
    "    # Check if create file exists\n",
    "    if not os.path.exists(f'mv_inputs/{create_file_name}.sql'):\n",
    "        print(f\"Table create file {create_file_name}.sql does not exist. Skipping table creation.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Check if table already exists\n",
    "            result = client.query(f\"SHOW TABLES LIKE '{table_view_name}'\")\n",
    "            result_rows = list(result.result_rows)\n",
    "            if result_rows:\n",
    "                print(f\"Table {table_view_name} already exists. Skipping creation.\")\n",
    "            else:\n",
    "                # Create the table\n",
    "                create_query = get_query_from_file(create_file_name)\n",
    "                create_query = create_query.format(chain=chain, view_name=table_view_name)\n",
    "                client.command(create_query)\n",
    "                print(f\"Created table {table_view_name}\")\n",
    "        except ClickHouseError as e:\n",
    "            print(f\"Error creating table {table_view_name}: {str(e)}\")\n",
    "            return  # Exit the function if table creation fails\n",
    "\n",
    "    # Comment out matview since we're backfilling daily now\n",
    "    # try:\n",
    "    #     # Check if view already exists\n",
    "    #     result = client.query(f\"SHOW TABLES LIKE '{full_view_name}'\")\n",
    "    #     result_rows = list(result.result_rows)\n",
    "    #     if result_rows:\n",
    "    #         print(f\"Materialized view {full_view_name} already exists. Skipping creation.\")\n",
    "    #         return\n",
    "\n",
    "    #     query_template = get_query_from_file(f'{mv_name}_mv')\n",
    "    #     query = query_template.format(chain=chain, view_name=full_view_name, table_name=table_view_name, block_time_sec=block_time)\n",
    "    #     query = gsb.process_goldsky_sql(query)\n",
    "    #     # Save the query\n",
    "    #     output_folder = os.path.join(\"mv_outputs\", \"sql\")\n",
    "    #     os.makedirs(output_folder, exist_ok=True)\n",
    "    #     filename = f\"{mv_name}_mv.sql\"\n",
    "    #     file_path = os.path.join(output_folder, filename)\n",
    "    #     with open(file_path, 'w') as file:\n",
    "    #         file.write(query)\n",
    "    #     # print(query)\n",
    "    #     client.command(query)\n",
    "    #     print(f\"Created materialized view {full_view_name}\")\n",
    "    # except ClickHouseError as e:\n",
    "    #     print(f\"Error creating materialized view {full_view_name}: {str(e)}\")\n",
    "\n",
    "\n",
    "def ensure_backfill_tracking_table_exists(client):\n",
    "    check_table_query = \"\"\"\n",
    "    SELECT 1 FROM system.tables \n",
    "    WHERE database = currentDatabase() AND name = 'backfill_tracking'\n",
    "    \"\"\"\n",
    "    result = client.query(check_table_query)\n",
    "    \n",
    "    if not result.result_rows:\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE backfill_tracking (\n",
    "            chain String,\n",
    "            mv_name String,\n",
    "            start_date Date,\n",
    "            end_date Date\n",
    "        ) ENGINE = MergeTree()\n",
    "        ORDER BY (chain, mv_name, start_date)\n",
    "        \"\"\"\n",
    "        client.command(create_table_query)\n",
    "        print(\"Created backfill_tracking table.\")\n",
    "    else:\n",
    "        print(\"backfill_tracking table already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# LEGACY FUNCTIONS\n",
    "# def backfill_data(client, chain, mv_name, end_date = end_date, block_time = 2, mod_start_date = start_date):\n",
    "#     full_view_name = f'{chain}_{mv_name}_mv'\n",
    "#     full_table_name = f'{chain}_{mv_name}'\n",
    "#     if mod_start_date == '':\n",
    "#         current_date_q = f\"SELECT DATE_TRUNC('day',MIN(timestamp)) AS start_dt FROM {chain}_blocks WHERE number = 1 AND is_deleted = 0\"\n",
    "#         current_date = client.query(current_date_q).result_rows[0][0].date()\n",
    "#     else:\n",
    "#         current_date = pd.to_datetime(mod_start_date).date()\n",
    "\n",
    "#     while current_date <= end_date:\n",
    "#         print(f\"{chain} - {mv_name}: Current date: {current_date} - End Date: {end_date}\")\n",
    "#         attempts = 1\n",
    "#         is_success = 0\n",
    "#         days_batch_size = set_days_batch_size\n",
    "        \n",
    "#         while (is_success == 0) & (attempts <= 3) :#& (current_date + datetime.timedelta(days=days_batch_size) <= end_date):\n",
    "#             if attempts == 1:\n",
    "#                 days_batch_size = set_days_batch_size\n",
    "#             elif attempts == 2:\n",
    "#                 days_batch_size = int( set_days_batch_size / 2 )\n",
    "#                 print(f'reset batch size to {days_batch_size}')\n",
    "#             else:\n",
    "#                 days_batch_size = 1\n",
    "#                 print(f'reset batch size to {days_batch_size}')\n",
    "            \n",
    "#             batch_size = datetime.timedelta(days=days_batch_size)\n",
    "#             print(f\"attempt: {attempts}\")\n",
    "#             batch_end = min(current_date + batch_size, end_date)\n",
    "#             # print(f'init batch end: {batch_end}')\n",
    "#             # print('checking backfill tracking')\n",
    "#             # Check if this range has been backfilled\n",
    "#             check_query_temp = get_query_from_file('backfill_check_query')\n",
    "#             check_query = check_query_temp.format(\n",
    "#                     mv_name=mv_name,\n",
    "#                     chain=chain,\n",
    "#                     start_date=current_date,\n",
    "#                     end_date=batch_end\n",
    "#                 )\n",
    "#             result = client.query(check_query)\n",
    "\n",
    "#             if result.result_rows: # Get date to start backfilling\n",
    "#                 latest_fill_start = result.result_rows[0][0]\n",
    "#                 # print(f\"Latest Fill Result: {latest_fill_start}\")\n",
    "#                 current_date = max(latest_fill_start, current_date)\n",
    "#                 batch_end = min(current_date + batch_size, end_date + datetime.timedelta(days = 1))\n",
    "#             else:\n",
    "#                 print(\"no backfill exists\")\n",
    "            \n",
    "#             # print(f'backfill check batch end: {batch_end}')\n",
    "#             # print(f\"Fill start: {current_date}\")\n",
    "\n",
    "#             # print(check_query)\n",
    "#             # print(result.result_rows)\n",
    "#             #Check if data already exists\n",
    "\n",
    "#             # Start 1 day back\n",
    "#             query_start_date = current_date - datetime.timedelta(days = 1)\n",
    "#             query_end_date = batch_end #+ datetime.timedelta(days = 1)\n",
    "\n",
    "\n",
    "#             if not result.result_rows:\n",
    "#                 # No record of backfill, proceed\n",
    "#                 query_template = get_query_from_file(f'{mv_name}_backfill')\n",
    "#                 query = query_template.format(\n",
    "#                     view_name=full_view_name,\n",
    "#                     chain=chain,\n",
    "#                     start_date=query_start_date,\n",
    "#                     end_date=query_end_date,\n",
    "#                     table_name = full_table_name,\n",
    "#                     block_time_sec = block_time\n",
    "#                 )\n",
    "#                 query = gsb.process_goldsky_sql(query)\n",
    "                \n",
    "#                 # print(query)\n",
    "#                 try:\n",
    "#                     # print(query)\n",
    "#                     # set_optimize_on_insert(0) # for runtime\n",
    "#                     print(f\"Starting backfill for {full_view_name} from {query_start_date} to {batch_end}\")\n",
    "\n",
    "#                     # # Save the query\n",
    "#                     # output_folder = os.path.join(\"mv_outputs\", \"sql\")\n",
    "#                     # os.makedirs(output_folder, exist_ok=True)\n",
    "#                     # filename = f\"{chain}_{mv_name}_backfill.sql\"\n",
    "#                     # file_path = os.path.join(output_folder, filename)\n",
    "#                     # with open(file_path, 'w') as file:\n",
    "#                     #     file.write(query)\n",
    "\n",
    "#                     client.command(query)\n",
    "#                     # Record the backfill\n",
    "#                     track_query = f\"\"\"\n",
    "#                     INSERT INTO backfill_tracking (chain, mv_name, start_date, end_date)\n",
    "#                     VALUES ('{chain}', '{mv_name}', toDate('{current_date}'), toDate('{batch_end}'))\n",
    "#                     \"\"\"\n",
    "#                     client.command(track_query)\n",
    "                    \n",
    "#                     print(f\"Backfilled data for {full_view_name} from {query_start_date} to {batch_end}\")\n",
    "\n",
    "#                     # Optimize the newly backfilled partition\n",
    "#                     # optimize_partition(client, full_view_name, current_date, batch_end)\n",
    "#                     is_success = 1\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error during backfill for {full_view_name} from {current_date} to {batch_end}: {str(e)}\")\n",
    "#                     attempts += 1\n",
    "#                 time.sleep(1)\n",
    "#             else:\n",
    "#                 print(f\"Data already backfilled for {full_view_name} from {current_date} to {batch_end}. Skipping.\")\n",
    "#                 is_success = 1\n",
    "#                 # if optimize_all:\n",
    "#                 #     optimize_partition(client, full_view_name, current_date, batch_end)\n",
    "#         # print(f\"Current Date: {current_date}, Batch End: {batch_end}\")\n",
    "#         current_date = max(batch_end,current_date + datetime.timedelta(days=1))\n",
    "\n",
    "        # print(f\"New Current Date: {current_date}\")\n",
    "\n",
    "# def optimize_partition(client, full_view_name, start_date, end_date):\n",
    "#     # First, let's get the actual partition names\n",
    "#     partition_query = f\"\"\"\n",
    "#     SELECT DISTINCT partition\n",
    "#     FROM system.parts\n",
    "#     WHERE table = '{full_view_name.split('.')[-1]}'\n",
    "#       AND database = '{full_view_name.split('.')[0]}'\n",
    "#       AND active\n",
    "#     ORDER BY partition\n",
    "#     \"\"\"\n",
    "#     print(partition_query)\n",
    "#     partitions = [row[0] for row in client.query(partition_query).result_rows]\n",
    "    \n",
    "#     print(f\"Available partitions for {full_view_name}: {partitions}\")\n",
    "\n",
    "#     # Filter partitions within our date range\n",
    "#     start_partition = start_date.strftime('%Y%m')\n",
    "#     end_partition = end_date.strftime('%Y%m')\n",
    "#     partitions_to_optimize = [p for p in partitions if start_partition <= p <= end_partition]\n",
    "\n",
    "#     if not partitions_to_optimize:\n",
    "#         print(f\"No partitions found for {full_view_name} between {start_date} and {end_date}\")\n",
    "#         return\n",
    "\n",
    "#     try:\n",
    "#         for partition in partitions_to_optimize:\n",
    "#             optimize_query = f\"\"\"\n",
    "#             OPTIMIZE TABLE {full_view_name} \n",
    "#             PARTITION '{partition}'\n",
    "#             FINAL SETTINGS max_execution_time = 3000\n",
    "#             \"\"\"\n",
    "#             print(f\"Attempting to optimize {full_view_name} for partition {partition}\")\n",
    "#             client.command(optimize_query)\n",
    "#             print(f\"Successfully optimized partition {partition} for {full_view_name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error executing OPTIMIZE TABLE for {full_view_name}\")\n",
    "#         print(f\"  Partition: {partition}\")\n",
    "#         print(f\"  Date range: {start_date} to {end_date}\")\n",
    "#         print(f\"  Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backfill_data(client, chain, mv_name, end_date, block_time=2, mod_start_date='', set_days_batch_size=7):\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    full_table_name = f'{chain}_{mv_name}'\n",
    "\n",
    "    if chain in ['op', 'base']:\n",
    "        topic0_func = next(item['topic0_func'] for item in topic0_maps if item['table_type'] == 'materialized')\n",
    "    else:\n",
    "        topic0_func = next(item['topic0_func'] for item in topic0_maps if item['table_type'] == 'raw')\n",
    "    \n",
    "    # Check on Date Ranges\n",
    "    current_date_q = f\"\"\"\n",
    "    SELECT \n",
    "        DATE_TRUNC('day', MIN(timestamp)) AS start_dt, \n",
    "        DATE_TRUNC('day', MAX(timestamp)) AS end_dt \n",
    "    FROM {chain}_blocks \n",
    "    WHERE number >= 1 AND is_deleted = 0\n",
    "    \"\"\"\n",
    "    result = client.query(current_date_q).result_rows\n",
    "    \n",
    "    if result and result[0][0] is not None and result[0][1] is not None:\n",
    "        start_date_result = max(result[0][0].date() , og_start_date) # handle for corrupt timestamp\n",
    "        end_date_result = min( result[0][1].date(), og_end_date) # handle for corrupt timestamp\n",
    "        print(f'start_date_result {start_date_result} - end_date_result {end_date_result}')\n",
    "    else:\n",
    "        start_date_result = start_date\n",
    "        end_date_result = end_date\n",
    "\n",
    "    # Determine start date\n",
    "    if mod_start_date == '':\n",
    "        start_date = start_date_result\n",
    "    else:\n",
    "        start_date = pd.to_datetime(mod_start_date).date()\n",
    "\n",
    "    start_date = start_date - datetime.timedelta(days=1) #ensure we fill in prior day\n",
    "\n",
    "    # Determine end date\n",
    "    end_date = end_date_result\n",
    "\n",
    "    # Print out resulting range\n",
    "    print(f\"Checking Backfill for {full_view_name} from {start_date} to {end_date}\")\n",
    "\n",
    "    # Fetch all existing backfill ranges\n",
    "    backfill_query = f\"\"\"\n",
    "    SELECT start_date, end_date\n",
    "    FROM backfill_tracking\n",
    "    WHERE chain = '{chain}' AND mv_name = '{mv_name}'\n",
    "    AND start_date >= toDate('{start_date}')\n",
    "    ORDER BY start_date\n",
    "    \"\"\"\n",
    "    backfill_ranges = client.query(backfill_query).result_rows\n",
    "\n",
    "    # Convert to list of tuples and sort\n",
    "    backfill_ranges = sorted([(row[0], row[1]) for row in backfill_ranges])\n",
    "\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        # Find the next date that needs backfilling\n",
    "        while current_date <= end_date:\n",
    "            # print(f'check: start {start_date}, current {current_date}, end {end_date}')\n",
    "            is_backfilled = any(bf_start <= current_date <= bf_end for bf_start, bf_end in backfill_ranges)\n",
    "            if not is_backfilled:\n",
    "                break\n",
    "            current_date += datetime.timedelta(days=1)\n",
    "        \n",
    "        if current_date > end_date:\n",
    "            print(f\"All dates up to {end_date} have been backfilled.\")\n",
    "            break\n",
    "\n",
    "        # Determine the batch end date\n",
    "        batch_end = min(current_date + datetime.timedelta(days=set_days_batch_size - 1), end_date)\n",
    "        \n",
    "        # Adjust batch_end if it overlaps with an existing backfill range\n",
    "        for bf_start, bf_end in backfill_ranges:\n",
    "            if current_date < bf_start <= batch_end:\n",
    "                batch_end = bf_start - datetime.timedelta(days=1)\n",
    "                break\n",
    "\n",
    "        # Perform the backfill\n",
    "        attempts = 1\n",
    "        query_start_date = current_date - datetime.timedelta(days=1)\n",
    "\n",
    "        while attempts <= 3:\n",
    "            try:\n",
    "                query_template = get_query_from_file(f'{mv_name}_backfill')\n",
    "                query = query_template.format(\n",
    "                    view_name=full_view_name,\n",
    "                    chain=chain,\n",
    "                    start_date=query_start_date,\n",
    "                    end_date=batch_end,\n",
    "                    table_name=full_table_name,\n",
    "                    block_time_sec=block_time,\n",
    "                    topic0_func=topic0_func\n",
    "                )\n",
    "                query = gsb.process_goldsky_sql(query)\n",
    "                \n",
    "                print(f\"Starting backfill for {full_view_name} from {query_start_date} to {batch_end}\")\n",
    "                client.command(query)\n",
    "                \n",
    "                # Record the backfill\n",
    "                track_query = f\"\"\"\n",
    "                INSERT INTO backfill_tracking (chain, mv_name, start_date, end_date)\n",
    "                VALUES ('{chain}', '{mv_name}', toDate('{query_start_date}'), toDate('{batch_end}'))\n",
    "                \"\"\"\n",
    "                client.command(track_query)\n",
    "                \n",
    "                print(f\"Backfilled data for {full_view_name} from {query_start_date} to {batch_end}\")\n",
    "                \n",
    "                # Update backfill_ranges with the new range\n",
    "                backfill_ranges.append((current_date, batch_end))\n",
    "                backfill_ranges.sort()\n",
    "                \n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error during backfill for {full_view_name} from {current_date} to {batch_end}: {str(e)}\")\n",
    "                attempts += 1\n",
    "                batch_end = min(current_date + datetime.timedelta(days=set_days_batch_size // (2 ** (attempts - 1))), end_date)\n",
    "            time.sleep(1)\n",
    "\n",
    "        current_date = batch_end + datetime.timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_reset_materialized_view(client, chain, mv_name):\n",
    "    table_view_name = f'{chain}_{mv_name}'\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    create_file_name = f'{mv_name}_create'\n",
    "    print(full_view_name)\n",
    "    query_template = get_query_from_file(f'{mv_name}_mv')\n",
    "    query = query_template.format(chain=chain, view_name=full_view_name, table_name=table_view_name, block_time_sec=block_time)\n",
    "    query = gsb.process_goldsky_sql(query)\n",
    "    client.command(query)\n",
    "    print(f\"Updated materialized view {full_view_name}\")\n",
    "\n",
    "    # dt_cmd = f\"DETACH TABLE PERMANENTLY {full_view_name}\"\n",
    "    # dt_cmd = f\"ALTER TABLE {full_view_name} MODIFY QUERY SELECT 1 WHERE 0\"\n",
    "\n",
    "    # print(dt_cmd)\n",
    "    client.command(dt_cmd)\n",
    "    print(f\"Detached table {full_view_name}\")\n",
    "\n",
    "def reset_materialized_view(client, chain, mv_name,):\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    table_name = f'{chain}_{mv_name}'\n",
    "\n",
    "    try:\n",
    "        # Drop the existing materialized view\n",
    "        client.command(f\"DROP TABLE IF EXISTS {full_view_name}\")\n",
    "        # client.command(f\"DROP MATERIALIZED VIEW IF EXISTS {full_view_name}\")\n",
    "        print(f\"Dropped materialized view {full_view_name}\")\n",
    "\n",
    "        # Drop the existing materialized view\n",
    "        client.command(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "        client.command(f\"DROP TABLE IF EXISTS {full_view_name}\")\n",
    "        print(f\"Dropped table {table_name}\")\n",
    "\n",
    "        # Clear the backfill tracking for this view\n",
    "        bf_delete = f\"\"\"\n",
    "        ALTER TABLE backfill_tracking \n",
    "        DELETE WHERE chain = '{chain}' AND mv_name = '{mv_name}'\n",
    "        \"\"\"\n",
    "        # print(bf_delete)\n",
    "        client.command(bf_delete)\n",
    "\n",
    "        print(f\"Cleared backfill tracking for {full_view_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error resetting materialized view {full_view_name}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_backfill_gaps(client):\n",
    "    query = get_query_from_file('find_backfill_gaps')\n",
    "    result = client.query(query)\n",
    "\n",
    "    if result.result_rows:\n",
    "        print(\"Backfill gaps found:\")\n",
    "        print(\"Chain | Table/View Name | Gap Start | Gap End\")\n",
    "        print(\"-\" * 50)\n",
    "        for row in result.result_rows:\n",
    "            print(f\"{row[0]} | {row[1]} | {row[2]} | {row[3]}\")\n",
    "    else:\n",
    "        print(\"No backfill gaps found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # To reset a view\n",
    "# for row in chain_configs.itertuples(index=False):\n",
    "#         chain = row.chain_name\n",
    "#         reset_materialized_view(client, chain, 'weekly_retention_rate_temp')\n",
    "\n",
    "# # # # # # # reset a single chain\n",
    "# # # # # reset_materialized_view(client, 'xterio', 'daily_aggregate_transactions_to')\n",
    "\n",
    "# # # # # # # # # # # # for mv in mv_names:\n",
    "# # # # # # # # # # # #         # print(row)\n",
    "# # # # # # # # # # # #         reset_materialized_view(client, 'bob', mv)\n",
    "\n",
    "\n",
    "# # # # # # # # Clear all\n",
    "# # # # # # # # mv_names\n",
    "# # # # # # # # for row in chain_configs.itertuples(index=False):\n",
    "# # # # # # # #         for mv in mv_names:\n",
    "# # # # # # # #                 chain = row.chain_name\n",
    "# # # # # # # #                 reset_materialized_view(client, chain, mv)\n",
    "\n",
    "# # # Detach all\n",
    "# # detach_reset_materialized_view\n",
    "# # for row in chain_configs.itertuples(index=False):\n",
    "# #         for mv in mv_names:\n",
    "# #                 chain = row.chain_name\n",
    "# #                 detach_reset_materialized_view(client, chain, mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimize\n",
    "# for row in chain_configs.itertuples(index=False):\n",
    "#         for mv in mv_names:\n",
    "#                 chain = row.chain_name\n",
    "#                 do_optimize_final(client, chain, mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_chain_configs = chain_configs\n",
    "chain_configs_if_agg = pd.DataFrame({\n",
    "    'chain_name': ['superchain'],\n",
    "    'block_time_sec': [2]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain_configs_if_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "ensure_backfill_tracking_table_exists(client)\n",
    "\n",
    "for mv_row in mvs:\n",
    "\n",
    "    if mv_row['chains'] == 'superchain':\n",
    "        chain_configs = chain_configs_if_agg\n",
    "    else:\n",
    "        chain_configs = og_chain_configs\n",
    "        \n",
    "    for chain_row in chain_configs.itertuples(index=False):\n",
    "        chain = chain_row.chain_name\n",
    "        block_time = chain_row.block_time_sec\n",
    "\n",
    "        mv_name = mv_row['mv_name']\n",
    "        print(f\"Processing chain: {chain} - {mv_name}\")\n",
    "\n",
    "        if mv_row['start_date'] != '':\n",
    "            mod_start_date = mv_row['start_date']\n",
    "        else:\n",
    "            mod_start_date = ''\n",
    "        \n",
    "        try:\n",
    "            print('create matview')\n",
    "            create_materialized_view(client, chain, mv_name, block_time = block_time)\n",
    "        except:\n",
    "            print('error')\n",
    "        try:\n",
    "            print('create backfill')\n",
    "            backfill_data(client, chain, mv_name, end_date = end_date, block_time = block_time, mod_start_date = mod_start_date, set_days_batch_size = set_days_batch_size)\n",
    "        except Exception as e:\n",
    "            print('An error occurred:')\n",
    "            print(str(e))\n",
    "            print('Traceback:')\n",
    "            print(traceback.format_exc())\n",
    "        \n",
    "    print(f\"Completed processing for {chain}\")\n",
    "\n",
    "print(\"All chains and views processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_backfill_gaps(client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
