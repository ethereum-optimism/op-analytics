{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of materialized view names\n",
    "mvs = [\n",
    "    {'mv_name': 'across_bridging_txs_v3', 'start_date': '2024-07-01', 'chains': ''},\n",
    "    # # {'mv_name': 'across_bridging_txs_v3_logs_only', 'start_date': '2024-07-01'},\n",
    "    # # {'mv_name': 'filtered_logs_l2s', 'start_date': ''},\n",
    "    # ### {'mv_name': 'erc20_transfers', 'start_date': ''},\n",
    "    # ### {'mv_name': 'native_eth_transfers', 'start_date': ''},\n",
    "    # ### {'mv_name': 'transactions_unique', 'start_date': ''},\n",
    "    # # {'mv_name': 'daily_aggregate_transactions_to', 'start_date': ''},\n",
    "    # {'mv_name': 'event_emitting_transactions_l2s', 'start_date': ''},\n",
    "    {'mv_name': 'weekly_retention_rate_temp', 'start_date': '2024-01-01', 'chains': 'superchain'},\n",
    "    {'mv_name': 'event_emitting_transactions_l2s_nofilter', 'start_date': '2024-01-01', 'chains': ''},\n",
    "    # {'mv_name': 'event_emitting_transactions_l2s_nofilter', 'start_date': '2024-01-01'},\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "set_days_batch_size = 30# 7 #30\n",
    "\n",
    "optimize_all = False #True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import traceback\n",
    "sys.path.append(\"../../helper_functions\")\n",
    "import clickhouse_utils as ch\n",
    "import opstack_metadata_utils as ops\n",
    "import goldsky_db_utils as gsb\n",
    "sys.path.pop()\n",
    "client = ch.connect_to_clickhouse_db()\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic0_maps = [\n",
    "        {'table_type': 'materialized', 'topic0_func': \"topic0\"},\n",
    "        {'table_type': 'raw', 'topic0_func': \"arrayElement(splitByString(',', topics), 1)\"}\n",
    "]\n",
    "    \n",
    "def check_topic0(client,chain):\n",
    "        sql = f\"SELECT topic0 FROM {chain}_logs limit 1\"\n",
    "        try:\n",
    "                client.command(sql)\n",
    "                topic0_func = next(item['topic0_func'] for item in topic0_maps if item['table_type'] == 'materialized')\n",
    "        except:\n",
    "                topic0_func = next(item['topic0_func'] for item in topic0_maps if item['table_type'] == 'raw')\n",
    "\n",
    "        return topic0_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_names = [item['mv_name'] for item in mvs]\n",
    "\n",
    "# Get Chain List\n",
    "chain_configs = ops.get_superchain_metadata_by_data_source('oplabs') # OPLabs db\n",
    "\n",
    "if client is None:\n",
    "        client = ch.connect_to_clickhouse_db()\n",
    "\n",
    "# Function to create ClickHouse view\n",
    "def get_chain_names_from_df(df):\n",
    "    return df['blockchain'].dropna().unique().tolist()\n",
    "\n",
    "# chain_configs = chain_configs[chain_configs['chain_name'] == 'xterio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-25\n"
     ]
    }
   ],
   "source": [
    "# List of chains\n",
    "# chains = get_chain_names_from_df(chain_configs)\n",
    "\n",
    "# Start date for backfilling\n",
    "start_date = datetime.date(2021, 11, 1)\n",
    "# start_date = datetime.date(2024, 5, 1)\n",
    "end_date = datetime.date.today() #+ datetime.timedelta(days=1)\n",
    "\n",
    "print(end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_start_date = start_date\n",
    "og_end_date = end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_from_file(mv_name):\n",
    "    try:\n",
    "        # Try to get the directory of the current script\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # If __file__ is not defined (e.g., in Jupyter), use the current working directory\n",
    "        script_dir = os.getcwd()\n",
    "    \n",
    "    query_file_path = os.path.join(script_dir, 'mv_inputs', f'{mv_name}.sql')\n",
    "    # print(f\"Attempting to read query from: {query_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(query_file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Query file not found: {query_file_path}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_optimize_on_insert(option_int = 1):\n",
    "    client.command(f\"\"\"\n",
    "        SET optimize_on_insert = {option_int};\n",
    "        \"\"\")\n",
    "    print(f\"Set optimize_on_insert = {option_int}\")\n",
    "\n",
    "def do_optimize_final(client, chain_name, mv_name):\n",
    "    # print(f'Optimizing: {chain_name}_{mv_name}')\n",
    "    opcmd = f'OPTIMIZE TABLE {chain_name}_{mv_name} FINAL;'\n",
    "    print(opcmd)\n",
    "    client.command(opcmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import clickhouse_connect\n",
    "from clickhouse_connect.driver.exceptions import ClickHouseError\n",
    "\n",
    "def create_materialized_view(client, chain, mv_name, block_time = 2):\n",
    "    table_view_name = f'{chain}_{mv_name}'\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    create_file_name = f'{mv_name}_create'\n",
    "    print(full_view_name)\n",
    "    \n",
    "    # Check if create file exists\n",
    "    if not os.path.exists(f'mv_inputs/{create_file_name}.sql'):\n",
    "        print(f\"Table create file {create_file_name}.sql does not exist. Skipping table creation.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Check if table already exists\n",
    "            result = client.query(f\"SHOW TABLES LIKE '{table_view_name}'\")\n",
    "            result_rows = list(result.result_rows)\n",
    "            if result_rows:\n",
    "                print(f\"Table {table_view_name} already exists. Skipping creation.\")\n",
    "            else:\n",
    "                # Create the table\n",
    "                create_query = get_query_from_file(create_file_name)\n",
    "                create_query = create_query.format(chain=chain, view_name=table_view_name)\n",
    "                client.command(create_query)\n",
    "                print(f\"Created table {table_view_name}\")\n",
    "        except ClickHouseError as e:\n",
    "            print(f\"Error creating table {table_view_name}: {str(e)}\")\n",
    "            return  # Exit the function if table creation fails\n",
    "\n",
    "    # Comment out matview since we're backfilling daily now\n",
    "    # try:\n",
    "    #     # Check if view already exists\n",
    "    #     result = client.query(f\"SHOW TABLES LIKE '{full_view_name}'\")\n",
    "    #     result_rows = list(result.result_rows)\n",
    "    #     if result_rows:\n",
    "    #         print(f\"Materialized view {full_view_name} already exists. Skipping creation.\")\n",
    "    #         return\n",
    "\n",
    "    #     query_template = get_query_from_file(f'{mv_name}_mv')\n",
    "    #     query = query_template.format(chain=chain, view_name=full_view_name, table_name=table_view_name, block_time_sec=block_time)\n",
    "    #     query = gsb.process_goldsky_sql(query)\n",
    "    #     # Save the query\n",
    "    #     output_folder = os.path.join(\"mv_outputs\", \"sql\")\n",
    "    #     os.makedirs(output_folder, exist_ok=True)\n",
    "    #     filename = f\"{mv_name}_mv.sql\"\n",
    "    #     file_path = os.path.join(output_folder, filename)\n",
    "    #     with open(file_path, 'w') as file:\n",
    "    #         file.write(query)\n",
    "    #     # print(query)\n",
    "    #     client.command(query)\n",
    "    #     print(f\"Created materialized view {full_view_name}\")\n",
    "    # except ClickHouseError as e:\n",
    "    #     print(f\"Error creating materialized view {full_view_name}: {str(e)}\")\n",
    "\n",
    "\n",
    "def ensure_backfill_tracking_table_exists(client):\n",
    "    check_table_query = \"\"\"\n",
    "    SELECT 1 FROM system.tables \n",
    "    WHERE database = currentDatabase() AND name = 'backfill_tracking'\n",
    "    \"\"\"\n",
    "    result = client.query(check_table_query)\n",
    "    \n",
    "    if not result.result_rows:\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE backfill_tracking (\n",
    "            chain String,\n",
    "            mv_name String,\n",
    "            start_date Date,\n",
    "            end_date Date\n",
    "        ) ENGINE = MergeTree()\n",
    "        ORDER BY (chain, mv_name, start_date)\n",
    "        \"\"\"\n",
    "        client.command(create_table_query)\n",
    "        print(\"Created backfill_tracking table.\")\n",
    "    else:\n",
    "        print(\"backfill_tracking table already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# LEGACY FUNCTIONS\n",
    "# def backfill_data(client, chain, mv_name, end_date = end_date, block_time = 2, mod_start_date = start_date):\n",
    "#     full_view_name = f'{chain}_{mv_name}_mv'\n",
    "#     full_table_name = f'{chain}_{mv_name}'\n",
    "#     if mod_start_date == '':\n",
    "#         current_date_q = f\"SELECT DATE_TRUNC('day',MIN(timestamp)) AS start_dt FROM {chain}_blocks WHERE number = 1 AND is_deleted = 0\"\n",
    "#         current_date = client.query(current_date_q).result_rows[0][0].date()\n",
    "#     else:\n",
    "#         current_date = pd.to_datetime(mod_start_date).date()\n",
    "\n",
    "#     while current_date <= end_date:\n",
    "#         print(f\"{chain} - {mv_name}: Current date: {current_date} - End Date: {end_date}\")\n",
    "#         attempts = 1\n",
    "#         is_success = 0\n",
    "#         days_batch_size = set_days_batch_size\n",
    "        \n",
    "#         while (is_success == 0) & (attempts <= 3) :#& (current_date + datetime.timedelta(days=days_batch_size) <= end_date):\n",
    "#             if attempts == 1:\n",
    "#                 days_batch_size = set_days_batch_size\n",
    "#             elif attempts == 2:\n",
    "#                 days_batch_size = int( set_days_batch_size / 2 )\n",
    "#                 print(f'reset batch size to {days_batch_size}')\n",
    "#             else:\n",
    "#                 days_batch_size = 1\n",
    "#                 print(f'reset batch size to {days_batch_size}')\n",
    "            \n",
    "#             batch_size = datetime.timedelta(days=days_batch_size)\n",
    "#             print(f\"attempt: {attempts}\")\n",
    "#             batch_end = min(current_date + batch_size, end_date)\n",
    "#             # print(f'init batch end: {batch_end}')\n",
    "#             # print('checking backfill tracking')\n",
    "#             # Check if this range has been backfilled\n",
    "#             check_query_temp = get_query_from_file('backfill_check_query')\n",
    "#             check_query = check_query_temp.format(\n",
    "#                     mv_name=mv_name,\n",
    "#                     chain=chain,\n",
    "#                     start_date=current_date,\n",
    "#                     end_date=batch_end\n",
    "#                 )\n",
    "#             result = client.query(check_query)\n",
    "\n",
    "#             if result.result_rows: # Get date to start backfilling\n",
    "#                 latest_fill_start = result.result_rows[0][0]\n",
    "#                 # print(f\"Latest Fill Result: {latest_fill_start}\")\n",
    "#                 current_date = max(latest_fill_start, current_date)\n",
    "#                 batch_end = min(current_date + batch_size, end_date + datetime.timedelta(days = 1))\n",
    "#             else:\n",
    "#                 print(\"no backfill exists\")\n",
    "            \n",
    "#             # print(f'backfill check batch end: {batch_end}')\n",
    "#             # print(f\"Fill start: {current_date}\")\n",
    "\n",
    "#             # print(check_query)\n",
    "#             # print(result.result_rows)\n",
    "#             #Check if data already exists\n",
    "\n",
    "#             # Start 1 day back\n",
    "#             query_start_date = current_date - datetime.timedelta(days = 1)\n",
    "#             query_end_date = batch_end #+ datetime.timedelta(days = 1)\n",
    "\n",
    "\n",
    "#             if not result.result_rows:\n",
    "#                 # No record of backfill, proceed\n",
    "#                 query_template = get_query_from_file(f'{mv_name}_backfill')\n",
    "#                 query = query_template.format(\n",
    "#                     view_name=full_view_name,\n",
    "#                     chain=chain,\n",
    "#                     start_date=query_start_date,\n",
    "#                     end_date=query_end_date,\n",
    "#                     table_name = full_table_name,\n",
    "#                     block_time_sec = block_time\n",
    "#                 )\n",
    "#                 query = gsb.process_goldsky_sql(query)\n",
    "                \n",
    "#                 # print(query)\n",
    "#                 try:\n",
    "#                     # print(query)\n",
    "#                     # set_optimize_on_insert(0) # for runtime\n",
    "#                     print(f\"Starting backfill for {full_view_name} from {query_start_date} to {batch_end}\")\n",
    "\n",
    "#                     # # Save the query\n",
    "#                     # output_folder = os.path.join(\"mv_outputs\", \"sql\")\n",
    "#                     # os.makedirs(output_folder, exist_ok=True)\n",
    "#                     # filename = f\"{chain}_{mv_name}_backfill.sql\"\n",
    "#                     # file_path = os.path.join(output_folder, filename)\n",
    "#                     # with open(file_path, 'w') as file:\n",
    "#                     #     file.write(query)\n",
    "\n",
    "#                     client.command(query)\n",
    "#                     # Record the backfill\n",
    "#                     track_query = f\"\"\"\n",
    "#                     INSERT INTO backfill_tracking (chain, mv_name, start_date, end_date)\n",
    "#                     VALUES ('{chain}', '{mv_name}', toDate('{current_date}'), toDate('{batch_end}'))\n",
    "#                     \"\"\"\n",
    "#                     client.command(track_query)\n",
    "                    \n",
    "#                     print(f\"Backfilled data for {full_view_name} from {query_start_date} to {batch_end}\")\n",
    "\n",
    "#                     # Optimize the newly backfilled partition\n",
    "#                     # optimize_partition(client, full_view_name, current_date, batch_end)\n",
    "#                     is_success = 1\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error during backfill for {full_view_name} from {current_date} to {batch_end}: {str(e)}\")\n",
    "#                     attempts += 1\n",
    "#                 time.sleep(1)\n",
    "#             else:\n",
    "#                 print(f\"Data already backfilled for {full_view_name} from {current_date} to {batch_end}. Skipping.\")\n",
    "#                 is_success = 1\n",
    "#                 # if optimize_all:\n",
    "#                 #     optimize_partition(client, full_view_name, current_date, batch_end)\n",
    "#         # print(f\"Current Date: {current_date}, Batch End: {batch_end}\")\n",
    "#         current_date = max(batch_end,current_date + datetime.timedelta(days=1))\n",
    "\n",
    "        # print(f\"New Current Date: {current_date}\")\n",
    "\n",
    "# def optimize_partition(client, full_view_name, start_date, end_date):\n",
    "#     # First, let's get the actual partition names\n",
    "#     partition_query = f\"\"\"\n",
    "#     SELECT DISTINCT partition\n",
    "#     FROM system.parts\n",
    "#     WHERE table = '{full_view_name.split('.')[-1]}'\n",
    "#       AND database = '{full_view_name.split('.')[0]}'\n",
    "#       AND active\n",
    "#     ORDER BY partition\n",
    "#     \"\"\"\n",
    "#     print(partition_query)\n",
    "#     partitions = [row[0] for row in client.query(partition_query).result_rows]\n",
    "    \n",
    "#     print(f\"Available partitions for {full_view_name}: {partitions}\")\n",
    "\n",
    "#     # Filter partitions within our date range\n",
    "#     start_partition = start_date.strftime('%Y%m')\n",
    "#     end_partition = end_date.strftime('%Y%m')\n",
    "#     partitions_to_optimize = [p for p in partitions if start_partition <= p <= end_partition]\n",
    "\n",
    "#     if not partitions_to_optimize:\n",
    "#         print(f\"No partitions found for {full_view_name} between {start_date} and {end_date}\")\n",
    "#         return\n",
    "\n",
    "#     try:\n",
    "#         for partition in partitions_to_optimize:\n",
    "#             optimize_query = f\"\"\"\n",
    "#             OPTIMIZE TABLE {full_view_name} \n",
    "#             PARTITION '{partition}'\n",
    "#             FINAL SETTINGS max_execution_time = 3000\n",
    "#             \"\"\"\n",
    "#             print(f\"Attempting to optimize {full_view_name} for partition {partition}\")\n",
    "#             client.command(optimize_query)\n",
    "#             print(f\"Successfully optimized partition {partition} for {full_view_name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error executing OPTIMIZE TABLE for {full_view_name}\")\n",
    "#         print(f\"  Partition: {partition}\")\n",
    "#         print(f\"  Date range: {start_date} to {end_date}\")\n",
    "#         print(f\"  Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backfill_data(client, chain, mv_name, end_date, block_time=2, mod_start_date='', set_days_batch_size=7):\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    full_table_name = f'{chain}_{mv_name}'\n",
    "\n",
    "    topic0_func = check_topic0(client, chain)\n",
    "    \n",
    "    # Check on Date Ranges\n",
    "    current_date_q = f\"\"\"\n",
    "    SELECT \n",
    "        DATE_TRUNC('day', MIN(timestamp)) AS start_dt, \n",
    "        DATE_TRUNC('day', MAX(timestamp)) AS end_dt \n",
    "    FROM {chain}_blocks \n",
    "    WHERE number >= 1 AND is_deleted = 0\n",
    "    \"\"\"\n",
    "    result = client.query(current_date_q).result_rows\n",
    "    \n",
    "    if result and result[0][0] is not None and result[0][1] is not None:\n",
    "        start_date_result = max(result[0][0].date() , og_start_date) # handle for corrupt timestamp\n",
    "        end_date_result = min( result[0][1].date(), og_end_date) # handle for corrupt timestamp\n",
    "        print(f'start_date_result {start_date_result} - end_date_result {end_date_result}')\n",
    "    else:\n",
    "        start_date_result = start_date\n",
    "        end_date_result = end_date\n",
    "\n",
    "    # Determine start date\n",
    "    if mod_start_date == '':\n",
    "        start_date = start_date_result\n",
    "    else:\n",
    "        start_date = pd.to_datetime(mod_start_date).date()\n",
    "\n",
    "    start_date = start_date - datetime.timedelta(days=1) #ensure we fill in prior day\n",
    "\n",
    "    # Determine end date\n",
    "    end_date = end_date_result\n",
    "\n",
    "    # Print out resulting range\n",
    "    print(f\"Checking Backfill for {full_view_name} from {start_date} to {end_date}\")\n",
    "\n",
    "    # Fetch all existing backfill ranges\n",
    "    backfill_query = f\"\"\"\n",
    "    SELECT start_date, end_date\n",
    "    FROM backfill_tracking\n",
    "    WHERE chain = '{chain}' AND mv_name = '{mv_name}'\n",
    "    AND start_date >= toDate('{start_date}')\n",
    "    ORDER BY start_date\n",
    "    \"\"\"\n",
    "    backfill_ranges = client.query(backfill_query).result_rows\n",
    "\n",
    "    # Convert to list of tuples and sort\n",
    "    backfill_ranges = sorted([(row[0], row[1]) for row in backfill_ranges])\n",
    "\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        # Find the next date that needs backfilling\n",
    "        while current_date <= end_date:\n",
    "            # print(f'check: start {start_date}, current {current_date}, end {end_date}')\n",
    "            is_backfilled = any(bf_start <= current_date <= bf_end for bf_start, bf_end in backfill_ranges)\n",
    "            if not is_backfilled:\n",
    "                break\n",
    "            current_date += datetime.timedelta(days=1)\n",
    "        \n",
    "        if current_date > end_date:\n",
    "            print(f\"All dates up to {end_date} have been backfilled.\")\n",
    "            break\n",
    "\n",
    "        # Determine the batch end date\n",
    "        batch_end = min(current_date + datetime.timedelta(days=set_days_batch_size - 1), end_date)\n",
    "        \n",
    "        # Adjust batch_end if it overlaps with an existing backfill range\n",
    "        for bf_start, bf_end in backfill_ranges:\n",
    "            if current_date < bf_start <= batch_end:\n",
    "                batch_end = bf_start - datetime.timedelta(days=1)\n",
    "                break\n",
    "\n",
    "        # Perform the backfill\n",
    "        attempts = 1\n",
    "        query_start_date = current_date - datetime.timedelta(days=1)\n",
    "\n",
    "        while attempts <= 3:\n",
    "            try:\n",
    "                query_template = get_query_from_file(f'{mv_name}_backfill')\n",
    "                query = query_template.format(\n",
    "                    view_name=full_view_name,\n",
    "                    chain=chain,\n",
    "                    start_date=query_start_date,\n",
    "                    end_date=batch_end,\n",
    "                    table_name=full_table_name,\n",
    "                    block_time_sec=block_time,\n",
    "                    topic0_func=topic0_func\n",
    "                )\n",
    "                query = gsb.process_goldsky_sql(query)\n",
    "                \n",
    "                print(f\"Starting backfill for {full_view_name} from {query_start_date} to {batch_end}\")\n",
    "                client.command(query)\n",
    "                \n",
    "                # Record the backfill\n",
    "                track_query = f\"\"\"\n",
    "                INSERT INTO backfill_tracking (chain, mv_name, start_date, end_date)\n",
    "                VALUES ('{chain}', '{mv_name}', toDate('{query_start_date}'), toDate('{batch_end}'))\n",
    "                \"\"\"\n",
    "                client.command(track_query)\n",
    "                \n",
    "                print(f\"Backfilled data for {full_view_name} from {query_start_date} to {batch_end}\")\n",
    "                \n",
    "                # Update backfill_ranges with the new range\n",
    "                backfill_ranges.append((current_date, batch_end))\n",
    "                backfill_ranges.sort()\n",
    "                \n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error during backfill for {full_view_name} from {current_date} to {batch_end}: {str(e)}\")\n",
    "                attempts += 1\n",
    "                batch_end = min(current_date + datetime.timedelta(days=set_days_batch_size // (2 ** (attempts - 1))), end_date)\n",
    "            time.sleep(1)\n",
    "\n",
    "        current_date = batch_end + datetime.timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_reset_materialized_view(client, chain, mv_name):\n",
    "    table_view_name = f'{chain}_{mv_name}'\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    create_file_name = f'{mv_name}_create'\n",
    "    print(full_view_name)\n",
    "    query_template = get_query_from_file(f'{mv_name}_mv')\n",
    "    query = query_template.format(chain=chain, view_name=full_view_name, table_name=table_view_name, block_time_sec=block_time)\n",
    "    query = gsb.process_goldsky_sql(query)\n",
    "    client.command(query)\n",
    "    print(f\"Updated materialized view {full_view_name}\")\n",
    "\n",
    "    # dt_cmd = f\"DETACH TABLE PERMANENTLY {full_view_name}\"\n",
    "    # dt_cmd = f\"ALTER TABLE {full_view_name} MODIFY QUERY SELECT 1 WHERE 0\"\n",
    "\n",
    "    # print(dt_cmd)\n",
    "    client.command(dt_cmd)\n",
    "    print(f\"Detached table {full_view_name}\")\n",
    "\n",
    "def reset_materialized_view(client, chain, mv_name,):\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    table_name = f'{chain}_{mv_name}'\n",
    "\n",
    "    try:\n",
    "        # Drop the existing materialized view\n",
    "        client.command(f\"DROP TABLE IF EXISTS {full_view_name}\")\n",
    "        # client.command(f\"DROP MATERIALIZED VIEW IF EXISTS {full_view_name}\")\n",
    "        print(f\"Dropped materialized view {full_view_name}\")\n",
    "\n",
    "        # Drop the existing materialized view\n",
    "        client.command(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "        client.command(f\"DROP TABLE IF EXISTS {full_view_name}\")\n",
    "        print(f\"Dropped table {table_name}\")\n",
    "\n",
    "        # Clear the backfill tracking for this view\n",
    "        bf_delete = f\"\"\"\n",
    "        ALTER TABLE backfill_tracking \n",
    "        DELETE WHERE chain = '{chain}' AND mv_name = '{mv_name}'\n",
    "        \"\"\"\n",
    "        # print(bf_delete)\n",
    "        client.command(bf_delete)\n",
    "\n",
    "        print(f\"Cleared backfill tracking for {full_view_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error resetting materialized view {full_view_name}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_backfill_gaps(client):\n",
    "    query = get_query_from_file('find_backfill_gaps')\n",
    "    result = client.query(query)\n",
    "\n",
    "    if result.result_rows:\n",
    "        print(\"Backfill gaps found:\")\n",
    "        print(\"Chain | Table/View Name | Gap Start | Gap End\")\n",
    "        print(\"-\" * 50)\n",
    "        for row in result.result_rows:\n",
    "            print(f\"{row[0]} | {row[1]} | {row[2]} | {row[3]}\")\n",
    "    else:\n",
    "        print(\"No backfill gaps found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # To reset a view\n",
    "# for row in chain_configs.itertuples(index=False):\n",
    "#         chain = row.chain_name\n",
    "#         reset_materialized_view(client, chain, 'across_bridging_txs_v3')\n",
    "\n",
    "# # # # # # # reset a single chain\n",
    "# # # # # reset_materialized_view(client, 'xterio', 'daily_aggregate_transactions_to')\n",
    "\n",
    "# # # # # # # # # # # # for mv in mv_names:\n",
    "# # # # # # # # # # # #         # print(row)\n",
    "# # # # # # # # # # # #         reset_materialized_view(client, 'bob', mv)\n",
    "\n",
    "\n",
    "# # # # # # # # Clear all\n",
    "# # # # # # # # mv_names\n",
    "# # # # # # # # for row in chain_configs.itertuples(index=False):\n",
    "# # # # # # # #         for mv in mv_names:\n",
    "# # # # # # # #                 chain = row.chain_name\n",
    "# # # # # # # #                 reset_materialized_view(client, chain, mv)\n",
    "\n",
    "# # # Detach all\n",
    "# # detach_reset_materialized_view\n",
    "# # for row in chain_configs.itertuples(index=False):\n",
    "# #         for mv in mv_names:\n",
    "# #                 chain = row.chain_name\n",
    "# #                 detach_reset_materialized_view(client, chain, mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimize\n",
    "# for row in chain_configs.itertuples(index=False):\n",
    "#         for mv in mv_names:\n",
    "#                 chain = row.chain_name\n",
    "#                 do_optimize_final(client, chain, mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_chain_configs = chain_configs\n",
    "chain_configs_if_agg = pd.DataFrame({\n",
    "    'chain_name': ['superchain'],\n",
    "    'block_time_sec': [2]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain_configs_if_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backfill_tracking table already exists.\n",
      "Processing chain: zora - across_bridging_txs_v3\n",
      "create matview\n",
      "zora_across_bridging_txs_v3_mv\n",
      "Created table zora_across_bridging_txs_v3\n",
      "create backfill\n",
      "start_date_result 2023-06-13 - end_date_result 2024-10-25\n",
      "Checking Backfill for zora_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for zora_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-29\n",
      "Backfilled data for zora_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-29\n",
      "Starting backfill for zora_across_bridging_txs_v3_mv from 2024-07-29 to 2024-08-28\n",
      "Backfilled data for zora_across_bridging_txs_v3_mv from 2024-07-29 to 2024-08-28\n",
      "Starting backfill for zora_across_bridging_txs_v3_mv from 2024-08-28 to 2024-09-27\n",
      "Backfilled data for zora_across_bridging_txs_v3_mv from 2024-08-28 to 2024-09-27\n",
      "Starting backfill for zora_across_bridging_txs_v3_mv from 2024-09-27 to 2024-10-25\n",
      "Backfilled data for zora_across_bridging_txs_v3_mv from 2024-09-27 to 2024-10-25\n",
      "Processing chain: base - across_bridging_txs_v3\n",
      "create matview\n",
      "base_across_bridging_txs_v3_mv\n",
      "Created table base_across_bridging_txs_v3\n",
      "create backfill\n",
      "start_date_result 2023-06-15 - end_date_result 2024-10-25\n",
      "Checking Backfill for base_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for base_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-29\n",
      "Backfilled data for base_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-29\n",
      "Starting backfill for base_across_bridging_txs_v3_mv from 2024-07-29 to 2024-08-28\n",
      "Backfilled data for base_across_bridging_txs_v3_mv from 2024-07-29 to 2024-08-28\n",
      "Starting backfill for base_across_bridging_txs_v3_mv from 2024-08-28 to 2024-09-27\n",
      "Backfilled data for base_across_bridging_txs_v3_mv from 2024-08-28 to 2024-09-27\n",
      "Starting backfill for base_across_bridging_txs_v3_mv from 2024-09-27 to 2024-10-25\n",
      "Backfilled data for base_across_bridging_txs_v3_mv from 2024-09-27 to 2024-10-25\n",
      "Processing chain: mode - across_bridging_txs_v3\n",
      "create matview\n",
      "mode_across_bridging_txs_v3_mv\n",
      "Table mode_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2023-11-16 - end_date_result 2024-10-25\n",
      "Checking Backfill for mode_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for mode_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for mode_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: lisk - across_bridging_txs_v3\n",
      "create matview\n",
      "lisk_across_bridging_txs_v3_mv\n",
      "Table lisk_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-05-03 - end_date_result 2024-10-25\n",
      "Checking Backfill for lisk_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for lisk_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for lisk_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: metal - across_bridging_txs_v3\n",
      "create matview\n",
      "metal_across_bridging_txs_v3_mv\n",
      "Table metal_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-03-27 - end_date_result 2024-10-25\n",
      "Checking Backfill for metal_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for metal_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for metal_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: mint - across_bridging_txs_v3\n",
      "create matview\n",
      "mint_across_bridging_txs_v3_mv\n",
      "Table mint_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-05-13 - end_date_result 2024-10-25\n",
      "Checking Backfill for mint_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for mint_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for mint_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: xterio - across_bridging_txs_v3\n",
      "create matview\n",
      "xterio_across_bridging_txs_v3_mv\n",
      "Table xterio_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-05-24 - end_date_result 2024-10-25\n",
      "Checking Backfill for xterio_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for xterio_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for xterio_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: polynomial - across_bridging_txs_v3\n",
      "create matview\n",
      "polynomial_across_bridging_txs_v3_mv\n",
      "Table polynomial_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-06-10 - end_date_result 2024-10-25\n",
      "Checking Backfill for polynomial_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for polynomial_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for polynomial_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: race - across_bridging_txs_v3\n",
      "create matview\n",
      "race_across_bridging_txs_v3_mv\n",
      "Table race_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-07-08 - end_date_result 2024-10-25\n",
      "Checking Backfill for race_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for race_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for race_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: worldchain - across_bridging_txs_v3\n",
      "create matview\n",
      "worldchain_across_bridging_txs_v3_mv\n",
      "Table worldchain_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-06-25 - end_date_result 2024-10-25\n",
      "Checking Backfill for worldchain_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for worldchain_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for worldchain_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: shape - across_bridging_txs_v3\n",
      "create matview\n",
      "shape_across_bridging_txs_v3_mv\n",
      "Table shape_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-07-23 - end_date_result 2024-10-25\n",
      "Checking Backfill for shape_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for shape_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for shape_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: op - across_bridging_txs_v3\n",
      "create matview\n",
      "op_across_bridging_txs_v3_mv\n",
      "Table op_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2021-11-11 - end_date_result 2024-10-25\n",
      "Checking Backfill for op_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for op_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for op_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: fraxtal - across_bridging_txs_v3\n",
      "create matview\n",
      "fraxtal_across_bridging_txs_v3_mv\n",
      "Table fraxtal_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-02-01 - end_date_result 2024-10-25\n",
      "Checking Backfill for fraxtal_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for fraxtal_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for fraxtal_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: redstone - across_bridging_txs_v3\n",
      "create matview\n",
      "redstone_across_bridging_txs_v3_mv\n",
      "Table redstone_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-04-03 - end_date_result 2024-10-25\n",
      "Checking Backfill for redstone_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for redstone_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for redstone_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: cyber - across_bridging_txs_v3\n",
      "create matview\n",
      "cyber_across_bridging_txs_v3_mv\n",
      "Table cyber_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-04-18 - end_date_result 2024-10-25\n",
      "Checking Backfill for cyber_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for cyber_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for cyber_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: kroma - across_bridging_txs_v3\n",
      "create matview\n",
      "kroma_across_bridging_txs_v3_mv\n",
      "Table kroma_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2023-09-05 - end_date_result 2024-10-25\n",
      "Checking Backfill for kroma_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for kroma_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for kroma_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: ham - across_bridging_txs_v3\n",
      "create matview\n",
      "ham_across_bridging_txs_v3_mv\n",
      "Table ham_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-05-24 - end_date_result 2024-10-25\n",
      "Checking Backfill for ham_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for ham_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for ham_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: swan - across_bridging_txs_v3\n",
      "create matview\n",
      "swan_across_bridging_txs_v3_mv\n",
      "Table swan_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-06-17 - end_date_result 2024-10-25\n",
      "Checking Backfill for swan_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for swan_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for swan_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: lyra - across_bridging_txs_v3\n",
      "create matview\n",
      "lyra_across_bridging_txs_v3_mv\n",
      "Table lyra_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2023-11-15 - end_date_result 2024-10-25\n",
      "Checking Backfill for lyra_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for lyra_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for lyra_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: orderly - across_bridging_txs_v3\n",
      "create matview\n",
      "orderly_across_bridging_txs_v3_mv\n",
      "Table orderly_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2023-10-06 - end_date_result 2024-10-25\n",
      "Checking Backfill for orderly_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for orderly_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for orderly_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: bob - across_bridging_txs_v3\n",
      "create matview\n",
      "bob_across_bridging_txs_v3_mv\n",
      "Table bob_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-04-11 - end_date_result 2024-10-25\n",
      "Checking Backfill for bob_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for bob_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "Backfilled data for bob_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: automata - across_bridging_txs_v3\n",
      "create matview\n",
      "automata_across_bridging_txs_v3_mv\n",
      "Table automata_across_bridging_txs_v3 already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-07-17 - end_date_result 2024-10-25\n",
      "Checking Backfill for automata_across_bridging_txs_v3_mv from 2024-06-30 to 2024-10-25\n",
      "Starting backfill for automata_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-29\n",
      "Backfilled data for automata_across_bridging_txs_v3_mv from 2024-06-29 to 2024-07-29\n",
      "Starting backfill for automata_across_bridging_txs_v3_mv from 2024-07-29 to 2024-08-28\n",
      "Backfilled data for automata_across_bridging_txs_v3_mv from 2024-07-29 to 2024-08-28\n",
      "Starting backfill for automata_across_bridging_txs_v3_mv from 2024-08-28 to 2024-09-27\n",
      "Backfilled data for automata_across_bridging_txs_v3_mv from 2024-08-28 to 2024-09-27\n",
      "Starting backfill for automata_across_bridging_txs_v3_mv from 2024-09-27 to 2024-10-25\n",
      "Backfilled data for automata_across_bridging_txs_v3_mv from 2024-09-27 to 2024-10-25\n",
      "Completed processing for automata\n",
      "Processing chain: superchain - weekly_retention_rate_temp\n",
      "create matview\n",
      "superchain_weekly_retention_rate_temp_mv\n",
      "Table superchain_weekly_retention_rate_temp already exists. Skipping creation.\n",
      "create backfill\n",
      "An error occurred:\n",
      ":HTTPDriver for https://pdmv9lhojy.us-west-2.aws.clickhouse.cloud:8443 returned response code 404)\n",
      " Code: 47. DB::Exception: Missing columns: 'is_deleted' while processing query: 'SELECT dateTrunc('day', min(timestamp)) AS start_dt, dateTrunc('day', max(timestamp)) AS end_dt FROM (SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.zora_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.base_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.mode_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.lisk_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.metal_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.mint_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.xterio_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.polynomial_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.race_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.worldchain_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.shape_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.op_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.fraxtal_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.redstone_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.cyber_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.kroma_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.ham_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.swan_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.lyra_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.orderly_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.bob_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.automata_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1)) AS superchain_blocks WHERE (number >= 1) AND (is_deleted = 0)', required columns: 'number' 'is_deleted' 'timestamp', maybe you meant: 'number' or 'timestamp'. (UNKNOWN_IDENTIFIER) (version 24.6.1.4609 (official build))\n",
      "\n",
      "Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/by/kltjc8yd0yz_7_wrtrzhrm9m0000gn/T/ipykernel_77237/2857348472.py\", line 30, in <module>\n",
      "    backfill_data(client, chain, mv_name, end_date = end_date, block_time = block_time, mod_start_date = mod_start_date, set_days_batch_size = set_days_batch_size)\n",
      "  File \"/var/folders/by/kltjc8yd0yz_7_wrtrzhrm9m0000gn/T/ipykernel_77237/1823657065.py\", line 15, in backfill_data\n",
      "    result = client.query(current_date_q).result_rows\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/michaelsilberling/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/clickhouse_connect/driver/client.py\", line 205, in query\n",
      "    return self._query_with_context(query_context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/michaelsilberling/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/clickhouse_connect/driver/httpclient.py\", line 215, in _query_with_context\n",
      "    response = self._raw_request(body,\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/michaelsilberling/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/clickhouse_connect/driver/httpclient.py\", line 442, in _raw_request\n",
      "    self._error_handler(response)\n",
      "  File \"/Users/michaelsilberling/opt/anaconda3/envs/gcp-env/lib/python3.12/site-packages/clickhouse_connect/driver/httpclient.py\", line 366, in _error_handler\n",
      "    raise OperationalError(err_str) if retried else DatabaseError(err_str) from None\n",
      "clickhouse_connect.driver.exceptions.DatabaseError: :HTTPDriver for https://pdmv9lhojy.us-west-2.aws.clickhouse.cloud:8443 returned response code 404)\n",
      " Code: 47. DB::Exception: Missing columns: 'is_deleted' while processing query: 'SELECT dateTrunc('day', min(timestamp)) AS start_dt, dateTrunc('day', max(timestamp)) AS end_dt FROM (SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.zora_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.base_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.mode_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.lisk_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.metal_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.mint_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.xterio_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.polynomial_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.race_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.worldchain_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.shape_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.op_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.fraxtal_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.redstone_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.cyber_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.kroma_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.ham_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.swan_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.lyra_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.orderly_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.bob_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1) UNION ALL SELECT id, number, hash, parent_hash, nonce, sha3_uncles, logs_bloom, transactions_root, state_root, receipts_root, miner, CAST(difficulty, 'Float64') AS difficulty, CAST(total_difficulty, 'Float64') AS total_difficulty, size, extra_data, gas_limit, gas_used, timestamp, transaction_count, base_fee_per_gas, withdrawals_root, chain, network, chain_id, insert_time FROM default.automata_blocks WHERE 1 = 1 HAVING (is_deleted = 0) AND (number >= 1)) AS superchain_blocks WHERE (number >= 1) AND (is_deleted = 0)', required columns: 'number' 'is_deleted' 'timestamp', maybe you meant: 'number' or 'timestamp'. (UNKNOWN_IDENTIFIER) (version 24.6.1.4609 (official build))\n",
      "\n",
      "\n",
      "Completed processing for superchain\n",
      "Processing chain: zora - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "zora_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table zora_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2023-06-13 - end_date_result 2024-10-25\n",
      "Checking Backfill for zora_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for zora_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for zora_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: base - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "base_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table base_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2023-06-15 - end_date_result 2024-10-25\n",
      "Checking Backfill for base_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for base_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for base_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: mode - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "mode_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table mode_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2023-11-16 - end_date_result 2024-10-25\n",
      "Checking Backfill for mode_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for mode_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for mode_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: lisk - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "lisk_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table lisk_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-05-03 - end_date_result 2024-10-25\n",
      "Checking Backfill for lisk_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for lisk_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for lisk_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: metal - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "metal_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table metal_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-03-27 - end_date_result 2024-10-25\n",
      "Checking Backfill for metal_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for metal_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for metal_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: mint - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "mint_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table mint_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-05-13 - end_date_result 2024-10-25\n",
      "Checking Backfill for mint_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for mint_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for mint_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: xterio - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "xterio_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table xterio_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-05-24 - end_date_result 2024-10-25\n",
      "Checking Backfill for xterio_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for xterio_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for xterio_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: polynomial - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "polynomial_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table polynomial_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-06-10 - end_date_result 2024-10-25\n",
      "Checking Backfill for polynomial_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for polynomial_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for polynomial_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: race - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "race_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table race_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-07-08 - end_date_result 2024-10-25\n",
      "Checking Backfill for race_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for race_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for race_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: worldchain - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "worldchain_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table worldchain_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-06-25 - end_date_result 2024-10-25\n",
      "Checking Backfill for worldchain_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for worldchain_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for worldchain_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: shape - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "shape_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table shape_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-07-23 - end_date_result 2024-10-25\n",
      "Checking Backfill for shape_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for shape_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for shape_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: op - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "op_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table op_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2021-11-11 - end_date_result 2024-10-25\n",
      "Checking Backfill for op_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for op_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for op_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: fraxtal - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "fraxtal_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table fraxtal_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-02-01 - end_date_result 2024-10-25\n",
      "Checking Backfill for fraxtal_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for fraxtal_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for fraxtal_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: redstone - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "redstone_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table redstone_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-04-03 - end_date_result 2024-10-25\n",
      "Checking Backfill for redstone_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for redstone_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for redstone_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: cyber - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "cyber_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table cyber_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-04-18 - end_date_result 2024-10-25\n",
      "Checking Backfill for cyber_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for cyber_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for cyber_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: kroma - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "kroma_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table kroma_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2023-09-05 - end_date_result 2024-10-25\n",
      "Checking Backfill for kroma_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for kroma_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for kroma_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: ham - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "ham_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table ham_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-05-24 - end_date_result 2024-10-25\n",
      "Checking Backfill for ham_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for ham_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for ham_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: swan - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "swan_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table swan_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-06-17 - end_date_result 2024-10-25\n",
      "Checking Backfill for swan_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for swan_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for swan_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: lyra - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "lyra_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table lyra_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2023-11-15 - end_date_result 2024-10-25\n",
      "Checking Backfill for lyra_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for lyra_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for lyra_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: orderly - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "orderly_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table orderly_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2023-10-06 - end_date_result 2024-10-25\n",
      "Checking Backfill for orderly_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for orderly_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for orderly_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: bob - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "bob_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table bob_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-04-11 - end_date_result 2024-10-25\n",
      "Checking Backfill for bob_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for bob_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for bob_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Processing chain: automata - event_emitting_transactions_l2s_nofilter\n",
      "create matview\n",
      "automata_event_emitting_transactions_l2s_nofilter_mv\n",
      "Table automata_event_emitting_transactions_l2s_nofilter already exists. Skipping creation.\n",
      "create backfill\n",
      "start_date_result 2024-07-17 - end_date_result 2024-10-25\n",
      "Checking Backfill for automata_event_emitting_transactions_l2s_nofilter_mv from 2023-12-31 to 2024-10-25\n",
      "Starting backfill for automata_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "Backfilled data for automata_event_emitting_transactions_l2s_nofilter_mv from 2023-12-30 to 2024-01-28\n",
      "All dates up to 2024-10-25 have been backfilled.\n",
      "Completed processing for automata\n",
      "All chains and views processed successfully\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "ensure_backfill_tracking_table_exists(client)\n",
    "\n",
    "for mv_row in mvs:\n",
    "\n",
    "    if mv_row['chains'] == 'superchain':\n",
    "        chain_configs = chain_configs_if_agg\n",
    "    else:\n",
    "        chain_configs = og_chain_configs\n",
    "        \n",
    "    for chain_row in chain_configs.itertuples(index=False):\n",
    "        chain = chain_row.chain_name\n",
    "        block_time = chain_row.block_time_sec\n",
    "\n",
    "        mv_name = mv_row['mv_name']\n",
    "        print(f\"Processing chain: {chain} - {mv_name}\")\n",
    "\n",
    "        if mv_row['start_date'] != '':\n",
    "            mod_start_date = mv_row['start_date']\n",
    "        else:\n",
    "            mod_start_date = ''\n",
    "        \n",
    "        try:\n",
    "            print('create matview')\n",
    "            create_materialized_view(client, chain, mv_name, block_time = block_time)\n",
    "        except:\n",
    "            print('error')\n",
    "        try:\n",
    "            print('create backfill')\n",
    "            backfill_data(client, chain, mv_name, end_date = end_date, block_time = block_time, mod_start_date = mod_start_date, set_days_batch_size = set_days_batch_size)\n",
    "        except Exception as e:\n",
    "            print('An error occurred:')\n",
    "            print(str(e))\n",
    "            print('Traceback:')\n",
    "            print(traceback.format_exc())\n",
    "        \n",
    "    print(f\"Completed processing for {chain}\")\n",
    "\n",
    "print(\"All chains and views processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No backfill gaps found.\n"
     ]
    }
   ],
   "source": [
    "print_backfill_gaps(client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
