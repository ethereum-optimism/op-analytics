{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of materialized view names\n",
    "mvs = [\n",
    "    {'mv_name': 'across_bridging_txs_v3', 'start_date': '2024-07-01'},\n",
    "    {'mv_name': 'across_bridging_txs_v3_logs_only', 'start_date': '2024-07-01'},\n",
    "    {'mv_name': 'filtered_logs_l2s', 'start_date': ''},\n",
    "    ### {'mv_name': 'erc20_transfers', 'start_date': ''},\n",
    "    ### {'mv_name': 'native_eth_transfers', 'start_date': ''},\n",
    "    ### {'mv_name': 'transactions_unique', 'start_date': ''},\n",
    "    {'mv_name': 'daily_aggregate_transactions_to', 'start_date': ''},\n",
    "    {'mv_name': 'event_emitting_transactions_l2s', 'start_date': ''},\n",
    "]\n",
    "\n",
    "set_days_batch_size = 2 #7 #30\n",
    "\n",
    "optimize_all = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import traceback\n",
    "sys.path.append(\"../../helper_functions\")\n",
    "import clickhouse_utils as ch\n",
    "import opstack_metadata_utils as ops\n",
    "import goldsky_db_utils as gsb\n",
    "sys.path.pop()\n",
    "client = ch.connect_to_clickhouse_db()\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_names = [item['mv_name'] for item in mvs]\n",
    "\n",
    "# Get Chain List\n",
    "chain_configs = ops.get_superchain_metadata_by_data_source('oplabs') # OPLabs db\n",
    "\n",
    "if client is None:\n",
    "        client = ch.connect_to_clickhouse_db()\n",
    "\n",
    "# Function to create ClickHouse view\n",
    "def get_chain_names_from_df(df):\n",
    "    return df['blockchain'].dropna().unique().tolist()\n",
    "\n",
    "# chain_configs = chain_configs[chain_configs['chain_name'] == 'xterio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of chains\n",
    "# chains = get_chain_names_from_df(chain_configs)\n",
    "\n",
    "# Start date for backfilling\n",
    "start_date = datetime.date(2021, 11, 1)\n",
    "# start_date = datetime.date(2024, 5, 1)\n",
    "end_date = datetime.date.today() + datetime.timedelta(days=1)\n",
    "\n",
    "print(end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_from_file(mv_name):\n",
    "    try:\n",
    "        # Try to get the directory of the current script\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # If __file__ is not defined (e.g., in Jupyter), use the current working directory\n",
    "        script_dir = os.getcwd()\n",
    "    \n",
    "    query_file_path = os.path.join(script_dir, 'mv_inputs', f'{mv_name}.sql')\n",
    "    # print(f\"Attempting to read query from: {query_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(query_file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Query file not found: {query_file_path}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_optimize_on_insert(option_int = 1):\n",
    "    client.command(f\"\"\"\n",
    "        SET optimize_on_insert = {option_int};\n",
    "        \"\"\")\n",
    "    print(f\"Set optimize_on_insert = {option_int}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import clickhouse_connect\n",
    "from clickhouse_connect.driver.exceptions import ClickHouseError\n",
    "\n",
    "def create_materialized_view(client, chain, mv_name, block_time = 2):\n",
    "    table_view_name = f'{chain}_{mv_name}'\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    create_file_name = f'{mv_name}_create'\n",
    "    print(full_view_name)\n",
    "    \n",
    "    # Check if create file exists\n",
    "    if not os.path.exists(f'mv_inputs/{create_file_name}.sql'):\n",
    "        print(f\"Table create file {create_file_name}.sql does not exist. Skipping table creation.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Check if table already exists\n",
    "            result = client.query(f\"SHOW TABLES LIKE '{table_view_name}'\")\n",
    "            result_rows = list(result.result_rows)\n",
    "            if result_rows:\n",
    "                print(f\"Table {table_view_name} already exists. Skipping creation.\")\n",
    "            else:\n",
    "                # Create the table\n",
    "                create_query = get_query_from_file(create_file_name)\n",
    "                create_query = create_query.format(chain=chain, view_name=table_view_name)\n",
    "                client.command(create_query)\n",
    "                print(f\"Created table {table_view_name}\")\n",
    "        except ClickHouseError as e:\n",
    "            print(f\"Error creating table {table_view_name}: {str(e)}\")\n",
    "            return  # Exit the function if table creation fails\n",
    "\n",
    "    # Comment out matview since we're backfilling daily now\n",
    "    # try:\n",
    "    #     # Check if view already exists\n",
    "    #     result = client.query(f\"SHOW TABLES LIKE '{full_view_name}'\")\n",
    "    #     result_rows = list(result.result_rows)\n",
    "    #     if result_rows:\n",
    "    #         print(f\"Materialized view {full_view_name} already exists. Skipping creation.\")\n",
    "    #         return\n",
    "\n",
    "    #     query_template = get_query_from_file(f'{mv_name}_mv')\n",
    "    #     query = query_template.format(chain=chain, view_name=full_view_name, table_name=table_view_name, block_time_sec=block_time)\n",
    "    #     query = gsb.process_goldsky_sql(query)\n",
    "    #     # Save the query\n",
    "    #     output_folder = os.path.join(\"mv_outputs\", \"sql\")\n",
    "    #     os.makedirs(output_folder, exist_ok=True)\n",
    "    #     filename = f\"{mv_name}_mv.sql\"\n",
    "    #     file_path = os.path.join(output_folder, filename)\n",
    "    #     with open(file_path, 'w') as file:\n",
    "    #         file.write(query)\n",
    "    #     # print(query)\n",
    "    #     client.command(query)\n",
    "    #     print(f\"Created materialized view {full_view_name}\")\n",
    "    # except ClickHouseError as e:\n",
    "    #     print(f\"Error creating materialized view {full_view_name}: {str(e)}\")\n",
    "\n",
    "\n",
    "def ensure_backfill_tracking_table_exists(client):\n",
    "    check_table_query = \"\"\"\n",
    "    SELECT 1 FROM system.tables \n",
    "    WHERE database = currentDatabase() AND name = 'backfill_tracking'\n",
    "    \"\"\"\n",
    "    result = client.query(check_table_query)\n",
    "    \n",
    "    if not result.result_rows:\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE backfill_tracking (\n",
    "            chain String,\n",
    "            mv_name String,\n",
    "            start_date Date,\n",
    "            end_date Date\n",
    "        ) ENGINE = MergeTree()\n",
    "        ORDER BY (chain, mv_name, start_date)\n",
    "        \"\"\"\n",
    "        client.command(create_table_query)\n",
    "        print(\"Created backfill_tracking table.\")\n",
    "    else:\n",
    "        print(\"backfill_tracking table already exists.\")\n",
    "\n",
    "def backfill_data(client, chain, mv_name, end_date = end_date, block_time = 2, mod_start_date = start_date):\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    full_table_name = f'{chain}_{mv_name}'\n",
    "    if mod_start_date == '':\n",
    "        current_date_q = f\"SELECT DATE_TRUNC('day',MIN(timestamp)) AS start_dt FROM {chain}_blocks WHERE number = 1 AND is_deleted = 0\"\n",
    "        current_date = client.query(current_date_q).result_rows[0][0].date()\n",
    "    else:\n",
    "        current_date = pd.to_datetime(mod_start_date).date()\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        print(f\"{chain} - {mv_name}: Current date: {current_date} - End Date: {end_date}\")\n",
    "        attempts = 1\n",
    "        is_success = 0\n",
    "        days_batch_size = set_days_batch_size\n",
    "        \n",
    "        while (is_success == 0) & (attempts < 3) :#& (current_date + datetime.timedelta(days=days_batch_size) <= end_date):\n",
    "            batch_size = datetime.timedelta(days=days_batch_size)\n",
    "            print(f\"attempt: {attempts}\")\n",
    "            batch_end = min(current_date + batch_size, end_date)\n",
    "            # print(f'init batch end: {batch_end}')\n",
    "            # print('checking backfill tracking')\n",
    "            # Check if this range has been backfilled\n",
    "            check_query = f\"\"\"\n",
    "            SELECT MAX(start_date) AS latest_fill_start\n",
    "            FROM backfill_tracking\n",
    "            WHERE chain = '{chain}'\n",
    "            AND mv_name = '{mv_name}'\n",
    "            HAVING \n",
    "                MIN(start_date) <= toDate('{current_date}')\n",
    "            AND MAX(end_date) > toDate('{batch_end}')\n",
    "            LIMIT 1\n",
    "            \"\"\"\n",
    "            result = client.query(check_query)\n",
    "\n",
    "            if result.result_rows: # Get date to start backfilling\n",
    "                latest_fill_start = result.result_rows[0][0]\n",
    "                # print(f\"Latest Fill Result: {latest_fill_start}\")\n",
    "                current_date = max(latest_fill_start, current_date)\n",
    "                batch_end = min(current_date + batch_size, end_date)\n",
    "            else:\n",
    "                print(\"no backfill exists\")\n",
    "            \n",
    "            # print(f'backfill check batch end: {batch_end}')\n",
    "            # print(f\"Fill start: {current_date}\")\n",
    "\n",
    "            # print(check_query)\n",
    "            # print(result.result_rows)\n",
    "            #Check if data already exists\n",
    "\n",
    "            # Start 1 day back\n",
    "            query_start_date = current_date - datetime.timedelta(days = 1)\n",
    "            query_end_date = batch_end + datetime.timedelta(days = 1)\n",
    "\n",
    "\n",
    "            if not result.result_rows:\n",
    "                # No record of backfill, proceed\n",
    "                query_template = get_query_from_file(f'{mv_name}_backfill')\n",
    "                query = query_template.format(\n",
    "                    view_name=full_view_name,\n",
    "                    chain=chain,\n",
    "                    start_date=query_start_date,\n",
    "                    end_date=query_end_date,\n",
    "                    table_name = full_table_name,\n",
    "                    block_time_sec = block_time\n",
    "                )\n",
    "                query = gsb.process_goldsky_sql(query)\n",
    "                \n",
    "                # print(query)\n",
    "                try:\n",
    "                    # print(query)\n",
    "                    # set_optimize_on_insert(0) # for runtime\n",
    "                    print(f\"Starting backfill for {full_view_name} from {query_start_date} to {batch_end}\")\n",
    "\n",
    "                    # # Save the query\n",
    "                    # output_folder = os.path.join(\"mv_outputs\", \"sql\")\n",
    "                    # os.makedirs(output_folder, exist_ok=True)\n",
    "                    # filename = f\"{chain}_{mv_name}_backfill.sql\"\n",
    "                    # file_path = os.path.join(output_folder, filename)\n",
    "                    # with open(file_path, 'w') as file:\n",
    "                    #     file.write(query)\n",
    "\n",
    "                    client.command(query)\n",
    "                    # Record the backfill\n",
    "                    track_query = f\"\"\"\n",
    "                    INSERT INTO backfill_tracking (chain, mv_name, start_date, end_date)\n",
    "                    VALUES ('{chain}', '{mv_name}', toDate('{current_date}'), toDate('{batch_end}'))\n",
    "                    \"\"\"\n",
    "                    client.command(track_query)\n",
    "                    \n",
    "                    print(f\"Backfilled data for {full_view_name} from {current_date} to {batch_end}\")\n",
    "\n",
    "                    # Optimize the newly backfilled partition\n",
    "                    # optimize_partition(client, full_view_name, current_date, batch_end)\n",
    "                    is_success = 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during backfill for {full_view_name} from {current_date} to {batch_end}: {str(e)}\")\n",
    "                    days_batch_size = 1\n",
    "                    attempts += 1\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                print(f\"Data already backfilled for {full_view_name} from {current_date} to {batch_end}. Skipping.\")\n",
    "                is_success = 1\n",
    "                # if optimize_all:\n",
    "                #     optimize_partition(client, full_view_name, current_date, batch_end)\n",
    "        # print(f\"Current Date: {current_date}, Batch End: {batch_end}\")\n",
    "        current_date = max(batch_end,current_date) + datetime.timedelta(days=1)\n",
    "\n",
    "        # print(f\"New Current Date: {current_date}\")\n",
    "\n",
    "# def optimize_partition(client, full_view_name, start_date, end_date):\n",
    "#     # First, let's get the actual partition names\n",
    "#     partition_query = f\"\"\"\n",
    "#     SELECT DISTINCT partition\n",
    "#     FROM system.parts\n",
    "#     WHERE table = '{full_view_name.split('.')[-1]}'\n",
    "#       AND database = '{full_view_name.split('.')[0]}'\n",
    "#       AND active\n",
    "#     ORDER BY partition\n",
    "#     \"\"\"\n",
    "#     print(partition_query)\n",
    "#     partitions = [row[0] for row in client.query(partition_query).result_rows]\n",
    "    \n",
    "#     print(f\"Available partitions for {full_view_name}: {partitions}\")\n",
    "\n",
    "#     # Filter partitions within our date range\n",
    "#     start_partition = start_date.strftime('%Y%m')\n",
    "#     end_partition = end_date.strftime('%Y%m')\n",
    "#     partitions_to_optimize = [p for p in partitions if start_partition <= p <= end_partition]\n",
    "\n",
    "#     if not partitions_to_optimize:\n",
    "#         print(f\"No partitions found for {full_view_name} between {start_date} and {end_date}\")\n",
    "#         return\n",
    "\n",
    "#     try:\n",
    "#         for partition in partitions_to_optimize:\n",
    "#             optimize_query = f\"\"\"\n",
    "#             OPTIMIZE TABLE {full_view_name} \n",
    "#             PARTITION '{partition}'\n",
    "#             FINAL SETTINGS max_execution_time = 3000\n",
    "#             \"\"\"\n",
    "#             print(f\"Attempting to optimize {full_view_name} for partition {partition}\")\n",
    "#             client.command(optimize_query)\n",
    "#             print(f\"Successfully optimized partition {partition} for {full_view_name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error executing OPTIMIZE TABLE for {full_view_name}\")\n",
    "#         print(f\"  Partition: {partition}\")\n",
    "#         print(f\"  Date range: {start_date} to {end_date}\")\n",
    "#         print(f\"  Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_reset_materialized_view(client, chain, mv_name):\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    dt_cmd = f\"DETACH TABLE PERMANENTLY {full_view_name}\"\n",
    "    print(dt_cmd)\n",
    "    client.command(dt_cmd)\n",
    "    print(f\"Detached table {full_view_name}\")\n",
    "\n",
    "def reset_materialized_view(client, chain, mv_name):\n",
    "    full_view_name = f'{chain}_{mv_name}_mv'\n",
    "    table_name = f'{chain}_{mv_name}'\n",
    "\n",
    "    try:\n",
    "        # Drop the existing materialized view\n",
    "        client.command(f\"DROP TABLE IF EXISTS {full_view_name}\")\n",
    "        # client.command(f\"DROP MATERIALIZED VIEW IF EXISTS {full_view_name}\")\n",
    "        print(f\"Dropped materialized view {full_view_name}\")\n",
    "\n",
    "        # Drop the existing materialized view\n",
    "        client.command(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "        print(f\"Dropped table {table_name}\")\n",
    "\n",
    "        # Clear the backfill tracking for this view\n",
    "        bf_delete = f\"\"\"\n",
    "        ALTER TABLE backfill_tracking \n",
    "        DELETE WHERE chain = '{chain}' AND mv_name = '{mv_name}'\n",
    "        \"\"\"\n",
    "        # print(bf_delete)\n",
    "        client.command(bf_delete)\n",
    "\n",
    "        print(f\"Cleared backfill tracking for {full_view_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error resetting materialized view {full_view_name}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # To reset a view\n",
    "# for row in chain_configs.itertuples(index=False):\n",
    "#         chain = row.chain_name\n",
    "#         reset_materialized_view(client, chain, 'filtered_logs_l2s')\n",
    "\n",
    "# # # # # reset a single chain\n",
    "# # # # reset_materialized_view(client, 'xterio', 'daily_aggregate_transactions_to')\n",
    "\n",
    "# # # # # # # # # # # for mv in mv_names:\n",
    "# # # # # # # # # # #         # print(row)\n",
    "# # # # # # # # # # #         reset_materialized_view(client, 'bob', mv)\n",
    "\n",
    "\n",
    "# # # # # # # Clear all\n",
    "# # # # # # # mv_names\n",
    "# # # # # # # for row in chain_configs.itertuples(index=False):\n",
    "# # # # # # #         for mv in mv_names:\n",
    "# # # # # # #                 chain = row.chain_name\n",
    "# # # # # # #                 reset_materialized_view(client, chain, mv)\n",
    "\n",
    "# # Detach all\n",
    "# detach_reset_materialized_view\n",
    "# for row in chain_configs.itertuples(index=False):\n",
    "#         for mv in mv_names:\n",
    "#                 chain = row.chain_name\n",
    "#                 detach_reset_materialized_view(client, chain, mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "ensure_backfill_tracking_table_exists(client)\n",
    "\n",
    "for row in chain_configs.itertuples(index=False):\n",
    "\n",
    "    chain = row.chain_name\n",
    "    block_time = row.block_time_sec\n",
    "    print(f\"Processing chain: {chain}\")\n",
    "    for row in mvs:\n",
    "        mv_name = row['mv_name']\n",
    "\n",
    "        if row['start_date'] != '':\n",
    "            mod_start_date = row['start_date']\n",
    "        else:\n",
    "            mod_start_date = start_date\n",
    "        \n",
    "        try:\n",
    "            print('create matview')\n",
    "            create_materialized_view(client, chain, mv_name, block_time = block_time)\n",
    "        except:\n",
    "            print('error')\n",
    "        try:\n",
    "            print('create backfill')\n",
    "            backfill_data(client, chain, mv_name, end_date = end_date, block_time = block_time, mod_start_date = mod_start_date)\n",
    "        except Exception as e:\n",
    "            print('An error occurred:')\n",
    "            print(str(e))\n",
    "            print('Traceback:')\n",
    "            print(traceback.format_exc())\n",
    "        \n",
    "    print(f\"Completed processing for {chain}\")\n",
    "\n",
    "print(\"All chains and views processed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
