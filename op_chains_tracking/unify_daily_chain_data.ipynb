{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unify CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unify start\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "print('unify start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opstack_metadata = pd.read_csv('../op_chains_tracking/outputs/chain_metadata.csv')\n",
    "meta_columns = ['alignment', 'display_name', 'mainnet_chain_id','op_based_version','is_op_chain']\n",
    "# opstack_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode_chain_activity_by_day_gs.csv\n",
      "pgn_chain_activity_by_day_gs.csv\n",
      "lyra_chain_activity_by_day_gs.csv\n",
      "base_chain_activity_by_day_gs.csv\n",
      "zora_chain_activity_by_day_gs.csv\n",
      "All CSVs have been combined and saved.\n"
     ]
    }
   ],
   "source": [
    "# Directory containing your CSV files\n",
    "directory = 'outputs/chain_data'\n",
    "output_file = 'outputs/all_chain_activity_by_day_gs.csv'\n",
    "\n",
    "# Create an empty DataFrame to store combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through the directory and append each matching CSV to the DataFrame\n",
    "for filename in os.listdir(directory):\n",
    "    if ('chain_activity_by_day_gs' in filename and filename.endswith('.csv')) & (filename != 'opchain_activity_by_day_gs.csv'):\n",
    "        print(filename)\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        temp_df = pd.read_csv(filepath)\n",
    "        combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "        temp_df = None # Free up Memory\n",
    "\n",
    "# Save the combined DataFrame to a single CSV file\n",
    "combined_df['dt'] = pd.to_datetime(combined_df['dt'])\n",
    "combined_df = combined_df.rename(columns={'chain':'chain_gs'})\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "print('All CSVs have been combined and saved.')\n",
    "# print(combined_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start dfl merge\n"
     ]
    }
   ],
   "source": [
    "# Join to Defillama\n",
    "print('start dfl merge')\n",
    "# Load the TVL data\n",
    "tvl_data_path = '../other_chains_tracking/outputs/dfl_chain_tvl.csv'\n",
    "tvl_df = pd.read_csv(tvl_data_path)\n",
    "tvl_df = tvl_df.rename(columns={'date':'dt','chain':'chain_dfl'})\n",
    "tvl_df = tvl_df[tvl_df['defillama_slug'].isin(opstack_metadata['defillama_slug'])]\n",
    "# Remove Partial Days\n",
    "tvl_df['dt'] = pd.to_datetime(tvl_df['dt'])\n",
    "# tvl_df = tvl_df[tvl_df['dt'].dt.time == pd.Timestamp('00:00:00').time()]\n",
    "# # Cast to date format\n",
    "# tvl_df['dt'] = tvl_df['dt'].dt.date\n",
    "# print(tvl_df.columns)\n",
    "\n",
    "# Drop Dupe metadata columns\n",
    "tvl_df = tvl_df.drop(columns=meta_columns, errors='ignore')\n",
    "\n",
    "# tvl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    chain_gs chain_name         dt num_raw_txs\n",
      "508     lyra       lyra 2024-02-17      43,959\n",
      "     chain_dfl       chain_name         dt           tvl\n",
      "5731     Metis  metis andromeda 2022-05-14  5.554761e+07\n"
     ]
    }
   ],
   "source": [
    "print(combined_df[['chain_gs','chain_name','dt','num_raw_txs']].sample(3))\n",
    "print(tvl_df[['chain_dfl','chain_name','dt','tvl']].sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a left join on 'chain_name' and 'dt'\n",
    "merged_df = pd.merge(combined_df, tvl_df, on=['chain_name','dt'], how='outer')\n",
    "merged_df = merged_df.merge(opstack_metadata[meta_columns + ['chain_name']], on=['chain_name'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data has been saved to: outputs/all_chain_data_by_day_combined.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the merged DataFrame to a new CSV file\n",
    "output_path = 'outputs/all_chain_data_by_day_combined.csv'\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "print('Merged data has been saved to:', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import to GSheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gspread\n",
    "# from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Google Sheets credentials and scope\n",
    "# scope = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "# credentials = ServiceAccountCredentials.from_json_keyfile_name('path_to_your_credentials.json', scope)\n",
    "# client = gspread.authorize(credentials)\n",
    "\n",
    "# # Open the Google Sheet and select the tab\n",
    "# sheet = client.open('Name_of_Your_Sheet').worksheet('Transaction Data')\n",
    "\n",
    "# # Clear existing data in the sheet\n",
    "# sheet.clear()\n",
    "\n",
    "# # Read the combined CSV file\n",
    "# data_df = pd.read_csv(output_file)\n",
    "\n",
    "# # Convert DataFrame to a list of lists, where each sublist is a row in the DataFrame\n",
    "# data_list = data_df.values.tolist()\n",
    "\n",
    "# # Insert column headers as the first row\n",
    "# data_list.insert(0, data_df.columns.tolist())\n",
    "\n",
    "# # Update the sheet with the new data\n",
    "# sheet.update('A1', data_list)\n",
    "\n",
    "# print('Google Sheet has been updated with new data.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
