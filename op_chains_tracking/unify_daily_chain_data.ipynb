{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unify CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "print('unify start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opstack_metadata = pd.read_csv('../op_chains_tracking/outputs/chain_metadata.csv')\n",
    "meta_columns = ['alignment', 'display_name', 'mainnet_chain_id','op_based_version','is_op_chain']\n",
    "# opstack_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing your CSV files\n",
    "directory = 'outputs/chain_data'\n",
    "output_file = 'outputs/all_chain_activity_by_day_gs.csv'\n",
    "\n",
    "# Create an empty DataFrame to store combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through the directory and append each matching CSV to the DataFrame\n",
    "for filename in os.listdir(directory):\n",
    "    if ('chain_activity_by_day_gs' in filename and filename.endswith('.csv')) & (filename != 'opchain_activity_by_day_gs.csv'):\n",
    "        print(filename)\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        temp_df = pd.read_csv(filepath)\n",
    "        combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "        temp_df = None # Free up Memory\n",
    "\n",
    "# Save the combined DataFrame to a single CSV file\n",
    "combined_df['dt'] = pd.to_datetime(combined_df['dt']).dt.strftime('%Y-%m-%d')\n",
    "combined_df = combined_df.rename(columns={'chain':'chain_gs'})\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "print('All CSVs have been combined and saved.')\n",
    "# print(combined_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to Defillama\n",
    "print('start dfl merge')\n",
    "# Load the TVL data\n",
    "tvl_data_path = '../other_chains_tracking/outputs/dfl_chain_tvl.csv'\n",
    "tvl_df = pd.read_csv(tvl_data_path)\n",
    "tvl_df = tvl_df.rename(columns={'date':'dt','chain':'chain_dfl'})\n",
    "tvl_df = tvl_df[tvl_df['defillama_slug'].isin(opstack_metadata['defillama_slug'])]\n",
    "# Remove Partial Days\n",
    "tvl_df['dt'] = pd.to_datetime(tvl_df['dt']).dt.strftime('%Y-%m-%d')\n",
    "# tvl_df = tvl_df[tvl_df['dt'].dt.time == pd.Timestamp('00:00:00').time()]\n",
    "# # Cast to date format\n",
    "# tvl_df['dt'] = tvl_df['dt'].dt.date\n",
    "# print(tvl_df.columns)\n",
    "\n",
    "# Drop Dupe metadata columns\n",
    "tvl_df = tvl_df.drop(columns=meta_columns, errors='ignore')\n",
    "\n",
    "tvl_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('start l2b merge')\n",
    "l2b_df = pd.read_csv('../other_chains_tracking/outputs/l2beat_l2_activity.csv')\n",
    "l2b_df = l2b_df.rename(columns={'timestamp':'dt','chain':'chain_l2b'})\n",
    "l2b_df['l2beat_slug'] = l2b_df['chain_l2b']\n",
    "l2b_df = l2b_df.merge(opstack_metadata[['l2beat_slug','chain_name']], on=['l2beat_slug'], how='inner')\n",
    "l2b_df['dt'] = pd.to_datetime(l2b_df['dt']).dt.strftime('%Y-%m-%d')\n",
    "l2b_df = l2b_df.drop(columns=meta_columns, errors='ignore')\n",
    "#Drop NaN\n",
    "l2b_df.dropna(subset=['valueUsd'], inplace=True)\n",
    "\n",
    "value_cols = ['valueUsd','cbvUsd','ebvUsd','nmvUsd','valueEth', 'cbvEth', 'ebvEth', 'nmvEth','transactions']\n",
    "\n",
    "# Update column names and value_cols list\n",
    "updated_value_cols = []\n",
    "for col in value_cols:\n",
    "        new_col_name = col + '_l2b'\n",
    "        l2b_df.rename(columns={col: new_col_name}, inplace=True)\n",
    "        updated_value_cols.append(new_col_name)\n",
    "\n",
    "value_cols = updated_value_cols\n",
    "\n",
    "l2b_df = l2b_df[['dt','chain_l2b','chain_name'] + value_cols ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('start dune merge')\n",
    "dune_df = pd.read_csv('../op_chains_tracking/outputs/l2_transaction_qualification.csv')\n",
    "#filters\n",
    "dune_df = dune_df[(dune_df['tx_success'] == True) & (dune_df['is_contract_tx'] == True) & (dune_df['has_event_log'] == True) & (dune_df['has_rev_dev'] == True)]\n",
    "dune_df = dune_df.rename(columns={'num_raw_transactions':'qualified_transactions','sum_gas_used':'qualified_sum_gas_used'})\n",
    "dune_df['dt'] = pd.to_datetime(dune_df['dt']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "dune_df = dune_df.groupby(['dune_schema','dt',]).agg({\n",
    "                                'qualified_transactions': 'sum'\n",
    "                                , 'qualified_sum_gas_used': 'sum'\n",
    "                                })\n",
    "dune_df.reset_index(inplace=True)\n",
    "dune_df = dune_df.merge(opstack_metadata[['dune_schema','chain_name']], on=['dune_schema'], how='inner')\n",
    "\n",
    "dune_df = dune_df[['dune_schema','chain_name','dt','qualified_transactions','qualified_sum_gas_used']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df[['chain_gs','chain_name','dt','num_raw_txs']].sample(3))\n",
    "print(tvl_df[['chain_dfl','chain_name','dt','tvl']].sample(3))\n",
    "print(l2b_df[['chain_l2b','chain_name','dt','valueUsd_l2b']].sample(3))\n",
    "print(dune_df[['dune_schema','chain_name','dt','qualified_transactions']].sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a left join on 'chain_name' and 'dt'\n",
    "merged_df = pd.merge(combined_df, tvl_df, on=['chain_name','dt'], how='outer')\n",
    "merged_df = pd.merge(merged_df, l2b_df, on=['chain_name','dt'], how='outer')\n",
    "merged_df = pd.merge(merged_df, dune_df, on=['chain_name','dt'], how='outer')\n",
    "\n",
    "meta_df_columns = meta_columns + ['chain_name']\n",
    "\n",
    "merged_df = merged_df.merge(opstack_metadata[meta_df_columns], on=['chain_name'], how='left')\n",
    "\n",
    "# merged_df[merged_df['chain_name']=='zora'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged DataFrame to a new CSV file\n",
    "output_path = 'outputs/all_chain_data_by_day_combined.csv'\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "print('Merged data has been saved to:', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import to GSheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gspread\n",
    "# from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Google Sheets credentials and scope\n",
    "# scope = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "# credentials = ServiceAccountCredentials.from_json_keyfile_name('path_to_your_credentials.json', scope)\n",
    "# client = gspread.authorize(credentials)\n",
    "\n",
    "# # Open the Google Sheet and select the tab\n",
    "# sheet = client.open('Name_of_Your_Sheet').worksheet('Transaction Data')\n",
    "\n",
    "# # Clear existing data in the sheet\n",
    "# sheet.clear()\n",
    "\n",
    "# # Read the combined CSV file\n",
    "# data_df = pd.read_csv(output_file)\n",
    "\n",
    "# # Convert DataFrame to a list of lists, where each sublist is a row in the DataFrame\n",
    "# data_list = data_df.values.tolist()\n",
    "\n",
    "# # Insert column headers as the first row\n",
    "# data_list.insert(0, data_df.columns.tolist())\n",
    "\n",
    "# # Update the sheet with the new data\n",
    "# sheet.update('A1', data_list)\n",
    "\n",
    "# print('Google Sheet has been updated with new data.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
