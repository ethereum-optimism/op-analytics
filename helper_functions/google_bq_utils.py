import os
import dotenv
import json
from google.cloud import bigquery
from google.oauth2 import service_account
from google.api_core.exceptions import NotFound
from google.auth.exceptions import OAuthError, RefreshError
from google.api_core.exceptions import (
    Unauthorized,
    Forbidden,
    PermissionDenied,
    GoogleAPICallError,
)
import pandas_utils as pu
import pandas as pd
import math
import subprocess
import time
import logging

dotenv.load_dotenv()

# Setup logging configuration
logging.basicConfig(level=logging.ERROR)  # Set logging level to ERROR
logger = logging.getLogger(__name__)  # Create logger instance for this module


def _get_gcloud_access_token():
    """
    Get a fresh access token using gcloud CLI.

    This bypasses stale OIDC tokens by using gcloud's internal token management.

    Returns:
        str or None: Access token if successful, None otherwise
    """
    try:
        result = subprocess.run(
            ['gcloud', 'auth', 'print-access-token'],
            capture_output=True,
            text=True,
            timeout=30
        )
        if result.returncode == 0:
            return result.stdout.strip()
        else:
            logging.warning(f"gcloud auth print-access-token failed: {result.stderr}")
            return None
    except FileNotFoundError:
        logging.warning("gcloud CLI not found")
        return None
    except subprocess.TimeoutExpired:
        logging.warning("gcloud auth print-access-token timed out")
        return None
    except Exception as e:
        logging.warning(f"Error getting gcloud access token: {e}")
        return None


def _create_client_with_access_token(project_id: str, access_token: str):
    """
    Create a BigQuery client using an explicit access token.

    Args:
        project_id: The Google Cloud project ID
        access_token: A valid OAuth2 access token

    Returns:
        BigQuery client
    """
    from google.oauth2.credentials import Credentials

    credentials = Credentials(token=access_token)
    return bigquery.Client(credentials=credentials, project=project_id)


def refresh_oidc_token_file():
    """
    Refresh the OIDC token file in CI environment.

    CircleCI OIDC tokens expire after ~1 hour. For long-running jobs,
    we need to refresh the token file before making BigQuery API calls.

    The OIDC token environment variable may be dynamically regenerated by CircleCI.
    This function re-reads it and writes it to the token file.

    Returns:
        bool: True if refresh was successful or not needed, False on failure
    """
    # Only run in CI environment
    if os.getenv("CI", "").lower() != "true":
        return True

    # Get the OIDC token from CircleCI environment (re-read, may be fresh)
    oidc_token = os.getenv("CIRCLE_OIDC_TOKEN_V2") or os.getenv("CIRCLE_OIDC_TOKEN")
    if not oidc_token:
        logging.warning("No CircleCI OIDC token found in environment")
        return False

    # Find the token file path from GOOGLE_APPLICATION_CREDENTIALS config
    gac_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
    if not gac_path or not os.path.exists(gac_path):
        logging.warning(f"GOOGLE_APPLICATION_CREDENTIALS not set or file doesn't exist: {gac_path}")
        return False

    try:
        with open(gac_path, 'r') as f:
            config = json.load(f)

        # Check if this is a workload identity configuration
        if config.get("type") == "external_account":
            # Get the token file path from the credential source
            cred_source = config.get("credential_source", {})
            token_file = cred_source.get("file")

            if token_file:
                # Write the current OIDC token to the file
                logging.info(f"Writing OIDC token to {token_file}")
                with open(token_file, 'w') as tf:
                    tf.write(oidc_token)
                logging.info("OIDC token file updated")
                return True
            else:
                logging.warning("No token file path found in workload identity config")
                return False
        else:
            logging.info(f"Not using workload identity (type={config.get('type')})")
            return True

    except Exception as e:
        logging.error(f"Error updating OIDC token file: {e}")
        return False

def prepare_bq_credentials():
    """
    Prepare BigQuery credentials before performing BQ operations.

    For long-running CI jobs, call this function right before BQ operations
    to refresh credentials and avoid stale OIDC token errors.

    This function:
    1. Refreshes the OIDC token file if in CI with workload identity
    2. Logs the current credential configuration for debugging

    Returns:
        bool: True if preparation was successful
    """
    is_ci = os.getenv("CI", "").lower() == "true"

    print("Preparing BQ credentials...")
    print(f"  CI environment: {is_ci}")
    print(f"  BQ_APPLICATION_CREDENTIALS set: {bool(os.getenv('BQ_APPLICATION_CREDENTIALS'))}")
    print(f"  GOOGLE_APPLICATION_CREDENTIALS: {os.getenv('GOOGLE_APPLICATION_CREDENTIALS', 'not set')}")

    if is_ci:
        # Check if service account credentials are available
        bq_sa_env = os.getenv("BQ_APPLICATION_CREDENTIALS")
        if bq_sa_env:
            try:
                creds = json.loads(bq_sa_env)
                if creds.get("type") == "service_account":
                    print("  Using service account credentials (no OIDC refresh needed)")
                    return True
                else:
                    print(f"  BQ_APPLICATION_CREDENTIALS has type: {creds.get('type')} (not service_account)")
            except json.JSONDecodeError:
                if os.path.exists(bq_sa_env) and _is_service_account_file(bq_sa_env):
                    print("  Using service account file (no OIDC refresh needed)")
                    return True
                else:
                    print(f"  BQ_APPLICATION_CREDENTIALS is not valid: {bq_sa_env}")

        # Refresh OIDC token file
        print("  Refreshing OIDC token file...")
        success = refresh_oidc_token_file()
        if success:
            print("  OIDC token file refreshed")
        else:
            print("  WARNING: Could not refresh OIDC token file")
            print("  If this job runs >1 hour, consider using a service account key")

        return success

    return True


def _is_auth_error(err: Exception) -> bool:
    """
    Return True if the exception likely indicates an auth/token issue that is
    retriable by recreating credentials.
    """
    if isinstance(err, (OAuthError, RefreshError, Unauthorized, PermissionDenied, Forbidden)):
        return True
    # Some auth errors are wrapped; inspect message text
    msg = str(err).lower()
    indicators = [
        "invalid_grant",
        "unauthorized",
        "401",
        "request had invalid authentication credentials",
        "expired",
        "stale",
        "token",
        "invalid credentials",
    ]
    return any(ind in msg for ind in indicators)

def _is_service_account_file(file_path: str) -> bool:
    """Check if a file is a service account key file (not workload identity config)."""
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
        # Service account keys have type "service_account"
        # Workload identity configs have type "external_account"
        return data.get("type") == "service_account"
    except Exception:
        return False


def _create_service_account_client(project_id: str):
    """Create a BigQuery client using service account credentials from env."""
    credentials = None
    # Prefer explicit JSON in env var; fallback to file path
    bq_sa_env = os.getenv("BQ_APPLICATION_CREDENTIALS")
    if bq_sa_env:
        try:
            # Try parse as JSON string first
            service_account_key = json.loads(bq_sa_env)
            # Verify it's actually a service account key
            if service_account_key.get("type") != "service_account":
                raise ValueError(f"Expected service_account type, got {service_account_key.get('type')}")
            credentials = service_account.Credentials.from_service_account_info(service_account_key)
        except json.JSONDecodeError:
            # Treat as file path - but only if it's actually a service account file
            if os.path.exists(bq_sa_env) and _is_service_account_file(bq_sa_env):
                credentials = service_account.Credentials.from_service_account_file(bq_sa_env)
            else:
                raise ValueError(f"BQ_APPLICATION_CREDENTIALS is not valid JSON and not a service account file: {bq_sa_env}")
    else:
        # Last resort: GOOGLE_APPLICATION_CREDENTIALS path if available
        # But only if it's a service account file (not workload identity config)
        creds_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
        if creds_path and os.path.exists(creds_path) and _is_service_account_file(creds_path):
            credentials = service_account.Credentials.from_service_account_file(creds_path)
    if credentials is None:
        raise Exception("Service account credentials not found in env (BQ_APPLICATION_CREDENTIALS not set or GOOGLE_APPLICATION_CREDENTIALS is not a service account key)")
    return bigquery.Client(credentials=credentials, project=project_id)

def setup_google_cloud_env():
    """
    Set up Google Cloud environment variables if not already set.
    Returns (success, error_message) tuple
    """
    # Default paths
    default_creds_path = os.path.expanduser("~/.config/gcloud/application_default_credentials.json")
    
    # Check if credentials are already set
    if not os.getenv('GOOGLE_APPLICATION_CREDENTIALS'):
        if os.path.exists(default_creds_path):
            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = default_creds_path
        else:
            return False, f"No credentials found at {default_creds_path}"
    
    # Check if project is already set
    if not os.getenv('GOOGLE_CLOUD_PROJECT'):
        try:
            # First try to get project from credentials file
            with open(os.environ['GOOGLE_APPLICATION_CREDENTIALS']) as f:
                creds_data = json.load(f)
                if 'quota_project_id' in creds_data:
                    os.environ['GOOGLE_CLOUD_PROJECT'] = creds_data['quota_project_id']
                elif 'project_id' in creds_data:
                    os.environ['GOOGLE_CLOUD_PROJECT'] = creds_data['project_id']
                else:
                    # If not in credentials, try getting from gcloud config
                    try:
                        result = subprocess.run(
                            ['gcloud', 'config', 'get-value', 'project'],
                            capture_output=True,
                            text=True,
                            check=True
                        )
                        project = result.stdout.strip()
                        if project:
                            os.environ['GOOGLE_CLOUD_PROJECT'] = project
                        else:
                            return False, "No project found in gcloud config"
                    except subprocess.CalledProcessError as e:
                        return False, f"Error getting project from gcloud: {str(e)}"
                    except FileNotFoundError:
                        return False, "gcloud command not found"
        except (json.JSONDecodeError, FileNotFoundError) as e:
            return False, f"Error reading credentials file: {str(e)}"
    
    return True, None

def connect_bq_client(project_id = os.getenv("BQ_PROJECT_ID"), max_retries=3):
    """
    Connect to BigQuery with retry mechanism for token expiration.

    Args:
        project_id: The Google Cloud project ID
        max_retries: Maximum number of retry attempts for token refresh

    Returns:
        BigQuery client or None if all retries fail
    """
    # In CI, refresh the OIDC token file before connecting
    if os.getenv("CI", "").lower() == "true":
        refresh_oidc_token_file()

    for attempt in range(max_retries):
        try:
            # In CI or when explicitly requested, prefer service account creds first
            prefer_sa = (
                os.getenv("CI", "").lower() == "true"
                or os.getenv("PREFER_BQ_SERVICE_ACCOUNT", "").lower() == "true"
                or bool(os.getenv("BQ_APPLICATION_CREDENTIALS"))
            )
            if prefer_sa:
                logging.info(f"Attempt {attempt + 1}: Using service account credentials")
                return _create_service_account_client(project_id)
        except Exception as e:
            logging.warning(f"Service account init failed on attempt {attempt + 1}: {e}")

        # In CI, try to get a fresh access token via gcloud CLI
        # This bypasses stale OIDC tokens by using gcloud's internal token management
        if os.getenv("CI", "").lower() == "true":
            try:
                logging.info(f"Attempt {attempt + 1}: Getting fresh access token via gcloud CLI")
                access_token = _get_gcloud_access_token()
                if access_token:
                    logging.info("Successfully obtained fresh access token")
                    return _create_client_with_access_token(project_id, access_token)
                else:
                    logging.warning("Failed to get access token from gcloud CLI")
            except Exception as e:
                logging.warning(f"gcloud access token method failed on attempt {attempt + 1}: {e}")

        try:
            # ADC / OIDC login via environment (may be short-lived in CI)
            logging.info(f"Attempt {attempt + 1}: Using ADC/OIDC login")
            client = bigquery.Client()
            return client
        except OAuthError as e:
            if "stale" in str(e).lower() or "invalid_grant" in str(e).lower():
                logging.warning(f"OAuth token expired on attempt {attempt + 1}: {e}")
                if attempt < max_retries - 1:
                    logging.info("Waiting before retry...")
                    time.sleep(2 ** attempt)  # Exponential backoff
                    continue
                else:
                    logging.error("All OIDC attempts failed due to token expiration")
            else:
                logging.error(f"OAuth error on attempt {attempt + 1}: {e}")
        except Exception as e:
            logging.error(f"Exception occurred while trying to use OIDC login on attempt {attempt + 1}: {e}")

        # If OIDC fails, try local machine credentials
        try:
            logging.info(f"Attempt {attempt + 1}: Using local machine credentials")
            setup_google_cloud_env()
            client = bigquery.Client(project=project_id)
            return client
        except Exception as e:
            logging.error(f"Exception occurred while trying to get local credentials on attempt {attempt + 1}: {e}")

        try:
            # Final fallback: service account
            logging.info(f"Attempt {attempt + 1}: Fallback to service account credentials")
            return _create_service_account_client(project_id)
        except Exception as e:
            logging.critical(f"Service account fallback failed on attempt {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                continue

    logging.critical("All authentication methods failed")
    logging.critical(
        "NOTE: If this is a long-running CI job (>1 hour), the OIDC token may have expired. "
        "Consider using a service account key (BQ_APPLICATION_CREDENTIALS) instead of OIDC, "
        "or splitting the job into smaller jobs."
    )
    return None

def check_table_exists(client, table_id, dataset_id='api_table_uploads', project_id=os.getenv("BQ_PROJECT_ID")):
    table_ref = f"{project_id}.{dataset_id}.{table_id}"
    try:
        client.get_table(table_ref)
        return True
    except Exception as e:
        if 'Not found' in str(e):
            return False
        else:
            logger.error("An error occurred while checking table existence")
            raise

def validate_schema(df, schema):
    df_columns = set(df.columns)
    schema_columns = set(field.name for field in schema)
    if df_columns != schema_columns:
        print("DataFrame columns:", df_columns)
        print("Schema columns:", schema_columns)
        missing = schema_columns - df_columns
        extra = df_columns - schema_columns
        raise ValueError(f"Schema mismatch. Missing: {missing}, Extra: {extra}")
    
def get_bq_type(column_name, column_type, series=None):
    column_name = str(column_name)
    column_type = str(column_type)

    if series is not None and pu.is_repeated_field(series):
        if series.apply(lambda x: isinstance(x, dict)).any():
            return 'RECORD'
        else:
            return 'STRING'
    
    if (column_name == 'date' or column_name == 'dt' or 
        column_name.endswith('_dt') or column_name.startswith('dt_')):
        return 'DATETIME'
    elif column_type == 'float64':
        return 'FLOAT64'
    elif column_type in ['int64', 'uint64']:
        return 'INTEGER'
    elif column_type in ['Int64', 'UInt64']:
        return 'FLOAT64'  # Or 'INTEGER' if you prefer
    elif column_type == 'datetime64[ns]':
        return 'DATETIME'
    elif column_type == 'bool':
        return 'BOOL'
    elif column_type in('string','python[string]'):
        return 'STRING'
    else:
        return 'STRING'
    
def clean_chain_id(value):
    if pd.isna(value):
        return ''
    
    # Convert to string and remove '.0' if present
    str_value = str(value).strip()
    return str_value[:-2] if str_value.endswith('.0') else str_value

def drop_table_if_exists(client, project_id, dataset_id, table_id):
    table_ref = f"{project_id}.{dataset_id}.{table_id}"
    try:
        client.delete_table(table_ref)
        print(f"Table {table_ref} deleted.")
    except NotFound:
        print(f"Table {table_ref} does not exist.")
        
def write_df_to_bq_table(df, table_id, dataset_id='api_table_uploads', 
                         write_mode='overwrite', project_id=os.getenv("BQ_PROJECT_ID"), 
                         chunk_size=100000, max_retries=3):
    print(f"Start Writing {dataset_id}.{table_id}")
    
    for attempt in range(max_retries):
        try:
            client = connect_bq_client(project_id)
            if client is None:
                raise Exception("Failed to connect to BigQuery")

            # Check if the table exists
            table_ref = f"{project_id}.{dataset_id}.{table_id}"
            try:
                table = client.get_table(table_ref)
                # Use existing schema if table exists
                schema = table.schema
                
                # Check for new columns and update schema if needed
                existing_columns = set(field.name for field in table.schema)
                new_columns = set(df.columns) - existing_columns
                
                if new_columns:
                    print(f"Adding new columns to {table_id}: {', '.join(new_columns)}")
                    new_schema = table.schema[:]
                    for new_col in new_columns:
                        bq_data_type = get_bq_type(new_col, str(df[new_col].dtype))
                        new_schema.append(bigquery.SchemaField(new_col, bq_data_type))
                    
                    table.schema = new_schema
                    client.update_table(table, ['schema'])
                    schema = new_schema  # Update schema for validation
                    
            except NotFound:
                # Create schema based on the first chunk if table doesn't exist
                first_chunk = df.iloc[:chunk_size]
                schema = create_schema(first_chunk)

            first_chunk = df.iloc[:chunk_size]
            
            # Set the write disposition
            write_disposition = (bigquery.WriteDisposition.WRITE_APPEND if write_mode == 'append' 
                                 else bigquery.WriteDisposition.WRITE_TRUNCATE)

            # Create a job configuration
            job_config = bigquery.LoadJobConfig(
                write_disposition=write_disposition,
                schema=schema
            )

            # Calculate the number of chunks
            total_rows = len(df)
            num_chunks = math.ceil(total_rows / chunk_size)

            for i, (_, chunk_df) in enumerate(df.groupby(df.index // chunk_size)):
                # Reset index for each chunk
                chunk_df = chunk_df.reset_index(drop=True)
                # Ensure chain id isn't weird
                try:
                    for col in df.columns:
                        if ('chain_id' in col.lower() or 'chainid' in col.lower()) and (df[col].dtype == 'object' or df[col].dtype == 'string'):
                            chunk_df[col] = chunk_df[col].apply(clean_chain_id)
                except Exception as e:
                    print(f"An error occurred while processing column {col}: {str(e)}")
                        
                # Process the chunk (flatten nested data, etc.)
                # print("Original DataFrame columns:", df.columns.tolist())
                # print("Chunk columns before processing:", chunk_df.columns.tolist())
                chunk_df = process_chunk(chunk_df)
                # print("Chunk columns after processing:", chunk_df.columns.tolist())
                # print("Schema fields:", [field.name for field in schema])

                validate_schema(chunk_df, schema)
                # Load the chunk into BigQuery
                job = client.load_table_from_dataframe(
                    chunk_df, f"{dataset_id}.{table_id}", job_config=job_config
                )

                try:
                    job.result()  # Wait for the job to complete
                    print(f"Chunk {i+1}/{num_chunks} loaded successfully to {dataset_id}.{table_id}")
                except Exception as e:
                    print(f"Error loading chunk {i+1}/{num_chunks} to BigQuery: {e}")
                    raise  # Re-raise the exception for higher-level error handling

                # If it's not the first chunk, change write mode to append
                if i == 0 and write_mode == 'overwrite':
                    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND

            print(f"All data loaded successfully to {dataset_id}.{table_id}")
            return  # Success, exit the retry loop
            
        except Exception as e:
            if _is_auth_error(e):
                print(f"Auth error on attempt {attempt + 1}: {e}")
                if attempt < max_retries - 1:
                    print("Waiting before retry...")
                    time.sleep(2 ** attempt)
                    continue
                else:
                    print("All attempts failed due to authentication errors")
                    raise
            print(f"Error on attempt {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                print("Waiting before retry...")
                time.sleep(2 ** attempt)
                continue
            else:
                print("All attempts failed")
                raise
    
    print("All retry attempts failed")
    raise Exception("Failed to write data to BigQuery after all retry attempts")

def create_schema(df):
    # print('makin schema')
    schema = []
    for column_name, column_type in df.dtypes.items():
        bq_data_type = get_bq_type(column_name, column_type, df[column_name])
        if bq_data_type == 'RECORD':
            schema.append(bigquery.SchemaField(column_name, bq_data_type, mode='REPEATED'))
        else:
            schema.append(bigquery.SchemaField(column_name, bq_data_type))
    # print(schema)
    return schema

def process_chunk(df):
    original_columns = df.columns.tolist()
    processed_df = df.copy()

    for column_name, column_type in df.dtypes.items():
        if pu.is_repeated_field(df[column_name]):
            processed_df[column_name] = df[column_name].apply(lambda x: json.dumps(x) if x is not None else None)
        elif column_type == 'object':
            try:
                processed_df = pu.flatten_nested_data(processed_df, column_name)
            except ValueError:
                continue

    # Ensure all original columns are present
    for col in original_columns:
        if col not in processed_df.columns:
            processed_df[col] = df[col]  # Restore the original column

    # Reorder columns to match original order
    processed_df = processed_df[original_columns]

    return processed_df

def append_and_upsert_df_to_bq_table(df, table_id, dataset_id='api_table_uploads', project_id=os.getenv("BQ_PROJECT_ID"), unique_keys=['chain', 'dt'], max_retries=3):
    for attempt in range(max_retries):
        try:
            # Create a fresh client on each attempt to handle token expiration
            client = connect_bq_client(project_id)
            if client is None:
                raise Exception("Failed to connect to BigQuery")
                
            table_ref = f"{project_id}.{dataset_id}.{table_id}"

            for key in unique_keys:
                if key in df.columns:
                    try:
                        df[key] = df[key].fillna('none')
                    except Exception as e:
                        print(f"Warning: Could not fill NULLs for column {key}. Error: {str(e)}")
            
            try:
                # Check if the table exists
                if check_table_exists(client, table_id, dataset_id, project_id):
                    # Get the existing table schema
                    table = client.get_table(table_ref)
                    existing_columns = set(field.name for field in table.schema)
                    
                    # Identify new columns
                    new_columns = set(df.columns) - existing_columns

                    # After checking for new columns
                    missing_columns = existing_columns - set(df.columns)
                    if missing_columns:
                        for col in missing_columns:
                            df[col] = None  # or an appropriate default value
                    
                    # If there are new columns, alter the table
                    if new_columns:
                        new_schema = table.schema[:]
                        for new_col in new_columns:
                            bq_data_type = get_bq_type(new_col, str(df[new_col].dtype))
                            new_schema.append(bigquery.SchemaField(new_col, bq_data_type))
                        
                        table.schema = new_schema
                        client.update_table(table, ['schema'])
                        logger.info(f"Added new columns to {table_id}: {', '.join(new_columns)}")
                        
                    # Create staging table for upsert
                    staging_table_id = f"{table_id}_staging"
                    staging_table_ref = f"{project_id}.{dataset_id}.{staging_table_id}"
                    
                    # Write data to staging table (overwrite mode)
                    # print(df.head(5))
                    delete_bq_table(table_id = staging_table_id, dataset_id = dataset_id, project_id=project_id)
                    write_df_to_bq_table(df, staging_table_id, dataset_id, write_mode='overwrite', project_id=project_id)
                    
                    # Perform upsert from staging table to main table
                    merge_query = f"""
                    MERGE `{table_ref}` T
                    USING `{staging_table_ref}` S
                    ON {" AND ".join([f"T.{key} = S.{key}" for key in unique_keys])}
                    WHEN MATCHED THEN
                      UPDATE SET {", ".join([f"T.{col} = S.{col}" for col in df.columns if col not in unique_keys])}
                    WHEN NOT MATCHED THEN
                      INSERT ({", ".join(df.columns)}) VALUES ({", ".join([f'S.{col}' for col in df.columns])})
                    """
                    
                    # Execute the merge query
                    query_job = client.query(merge_query)
                    query_job.result()
                    
                    logger.info(f"Append and upsert to {dataset_id}.{table_id} completed successfully")
                    
                    # Clean up staging table
                    client.delete_table(staging_table_ref)
                    delete_bq_table(table_id = staging_table_id, dataset_id = dataset_id, project_id=project_id)
                    logger.info(f"Staging table {staging_table_ref} deleted.")
                    
                else:
                    # If the table doesn't exist, just create it by writing the data (overwrite mode)
                    write_df_to_bq_table(df, table_id, dataset_id, write_mode='overwrite', project_id=project_id)
            
            except Exception as e:
                if _is_auth_error(e):
                    print(f"Auth error during append_and_upsert on attempt {attempt + 1}: {e}")
                    if attempt < max_retries - 1:
                        print("Waiting before retry...")
                        time.sleep(2 ** attempt)
                        continue
                    else:
                        print("All attempts failed due to authentication errors")
                        raise
                logger.error(f"Error during append_and_upsert_df_to_bq_table on attempt {attempt + 1}: {e}")
                if attempt < max_retries - 1:
                    print("Waiting before retry...")
                    time.sleep(2 ** attempt)
                    continue
                else:
                    raise  # Re-raise the exception for higher-level error handling
            
            return  # Success, exit the retry loop
            
        except Exception as e:
            if _is_auth_error(e):
                print(f"Auth error on attempt {attempt + 1}: {e}")
                if attempt < max_retries - 1:
                    print("Waiting before retry...")
                    time.sleep(2 ** attempt)
                    continue
                else:
                    print("All attempts failed due to authentication errors")
                    raise
            print(f"Error on attempt {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                print("Waiting before retry...")
                time.sleep(2 ** attempt)
                continue
            else:
                print("All attempts failed")
                raise
    
    print("All retry attempts failed")
    raise Exception("Failed to upsert data to BigQuery after all retry attempts")

# WARNING THE DELETES TABLES
def delete_bq_table(dataset_id, table_id, project_id=os.getenv("BQ_PROJECT_ID"), max_retries=3):
    """
    Deletes a table from a BigQuery dataset.

    Args:
        dataset_id (str): The ID of the dataset containing the table.
        table_id (str): The ID of the table to be deleted.
        project_id (str, optional): The ID of the Google Cloud project. If not provided,
            the project ID from the environment variable GOOGLE_CLOUD_PROJECT will be used.
        max_retries (int): Maximum number of retry attempts for token expiration.

    Returns:
        bool: True if the table was deleted successfully, False otherwise.
    """
    for attempt in range(max_retries):
        try:
            # Create a fresh client on each attempt to handle token expiration
            client = connect_bq_client(project_id=project_id)
            if client is None:
                raise Exception("Failed to connect to BigQuery")

            # Get the table reference
            table_ref = client.dataset(dataset_id).table(table_id)

            # Delete the table
            client.delete_table(table_ref, not_found_ok=True)
            print(f"Table '{table_id}' deleted successfully from dataset '{dataset_id}'.")
            return True
            
        except Exception as e:
            if _is_auth_error(e):
                print(f"Auth error on attempt {attempt + 1}: {e}")
                if attempt < max_retries - 1:
                    print("Waiting before retry...")
                    time.sleep(2 ** attempt)
                    continue
                else:
                    print("All attempts failed due to authentication errors")
                    return False
            print(f"Error deleting table '{table_id}' from dataset '{dataset_id}' on attempt {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                print("Waiting before retry...")
                time.sleep(2 ** attempt)
                continue
            else:
                return False
    
    print("All retry attempts failed")
    return False

def run_query_to_df(query, project_id=os.getenv("BQ_PROJECT_ID"), max_retries=3):
    for attempt in range(max_retries):
        try:
            client = connect_bq_client(project_id)
            if client is None:
                raise Exception("Failed to connect to BigQuery")
                
            query_job = client.query(query)
            df = query_job.to_dataframe()
            return df
            
        except Exception as e:
            if _is_auth_error(e):
                print(f"Auth error on attempt {attempt + 1}: {e}")
                if attempt < max_retries - 1:
                    print("Waiting before retry...")
                    time.sleep(2 ** attempt)
                    continue
                else:
                    print("All attempts failed due to authentication errors")
                    raise
            print(f"Error on attempt {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                print("Waiting before retry...")
                time.sleep(2 ** attempt)
                continue
            else:
                print("All attempts failed")
                raise
    
    print("All retry attempts failed")
    raise Exception("Failed to run query after all retry attempts")
     
     
def run_query_and_save_csv(query, destination_file, project_id=os.getenv("BQ_PROJECT_ID")):
    """
    Runs a BigQuery SQL query and saves the results to a CSV file.

    Args:
        query (str): The SQL query to execute.
        destination_file (str): The path and filename for the CSV file to be created.
        project_id (str, optional): The BigQuery project ID. Defaults to the value of the BQ_PROJECT_ID environment variable.

    Returns:
        None
    """
    results = run_query_to_df(query, project_id)

    # Save the DataFrame to a CSV file
    results.to_csv(destination_file, index=False)
    print(f"Query results saved to {destination_file}")


def remove_duplicates(table_id, dataset_id='api_table_uploads', project_id=os.getenv("BQ_PROJECT_ID"), unique_keys=['chain', 'date']):
    client = connect_bq_client(project_id)
    table_ref = f"{project_id}.{dataset_id}.{table_id}"

    # Create a temporary table to hold the deduplicated data
    temp_table_id = f"{table_id}_deduped_temp"
    temp_table_ref = f"{project_id}.{dataset_id}.{temp_table_id}"

    # SQL query to remove duplicates
    dedup_query = f"""
    CREATE OR REPLACE TABLE `{temp_table_ref}` AS
    SELECT
        {', '.join([f'{col}' for col in client.get_table(table_ref).schema if col.name != 'row_num_dedup'])}
    FROM (
        SELECT
            *,
            ROW_NUMBER() OVER (PARTITION BY {', '.join(unique_keys)} ORDER BY {unique_keys[0]}) AS row_num_dedup
        FROM
            `{table_ref}`
    )
    WHERE
        row_num_dedup = 1
    """

    # Execute the deduplication query
    dedup_job = client.query(dedup_query)
    dedup_job.result()
    print(f"Temporary deduplicated table {temp_table_ref} created.")

    # Replace the original table with the deduplicated table
    replace_query = f"""
    CREATE OR REPLACE TABLE `{table_ref}` AS
    SELECT
        *
    FROM
        `{temp_table_ref}`
    """

    replace_job = client.query(replace_query)
    replace_job.result()
    print(f"Original table {table_ref} replaced with deduplicated data.")

    # Drop the temporary table
    client.delete_table(temp_table_ref)
    print(f"Temporary table {temp_table_ref} deleted.")
